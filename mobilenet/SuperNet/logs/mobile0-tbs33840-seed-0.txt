[05/08 02:16:06] SuperNet Training INFO: tag                 : mobile0-tbs33840
[05/08 02:16:06] SuperNet Training INFO: seed                : 0
[05/08 02:16:06] SuperNet Training INFO: thresholds          : [38, 40]
[05/08 02:16:06] SuperNet Training INFO: data_path           : ../../../dataset/ILSVRC2012
[05/08 02:16:06] SuperNet Training INFO: save_path           : ./SuperNet
[05/08 02:16:06] SuperNet Training INFO: search_space        : proxyless
[05/08 02:16:06] SuperNet Training INFO: valid_size          : 50000
[05/08 02:16:06] SuperNet Training INFO: num_gpus            : 8
[05/08 02:16:06] SuperNet Training INFO: workers             : 4
[05/08 02:16:06] SuperNet Training INFO: interval_ep_eval    : 8
[05/08 02:16:06] SuperNet Training INFO: train_batch_size    : 1024
[05/08 02:16:06] SuperNet Training INFO: test_batch_size     : 256
[05/08 02:16:06] SuperNet Training INFO: max_epoch           : 120
[05/08 02:16:06] SuperNet Training INFO: learning_rate       : 0.12
[05/08 02:16:06] SuperNet Training INFO: momentum            : 0.9
[05/08 02:16:06] SuperNet Training INFO: weight_decay        : 4e-05
[05/08 02:16:06] SuperNet Training INFO: nesterov            : True
[05/08 02:16:06] SuperNet Training INFO: lr_schedule_type    : cosine
[05/08 02:16:06] SuperNet Training INFO: warmup              : False
[05/08 02:16:06] SuperNet Training INFO: label_smooth        : 0.1
[05/08 02:16:06] SuperNet Training INFO: rank                : 0
[05/08 02:16:06] SuperNet Training INFO: gpu                 : 0
[05/08 02:16:06] SuperNet Training INFO: save_name           : mobile0-tbs33840-seed-0
[05/08 02:16:06] SuperNet Training INFO: log_path            : ./SuperNet/logs/mobile0-tbs33840-seed-0.txt
[05/08 02:16:06] SuperNet Training INFO: ckpt_path           : ./SuperNet/checkpoint/mobile0-tbs33840-seed-0.pt
[05/08 02:16:06] SuperNet Training INFO: dist_url            : tcp://127.0.0.1:23456
[05/08 02:16:06] SuperNet Training INFO: world_size          : 8
[05/08 02:16:06] SuperNet Training INFO: distributed         : True
[05/08 02:16:06] SuperNet Training INFO: ['3x3_MBConv3', '3x3_MBConv6', '5x5_MBConv3', '5x5_MBConv6', '7x7_MBConv3', '7x7_MBConv6', 'Identity']
[05/08 02:16:47] SuperNet Training INFO: DistributedDataParallel(
  (module): SuperNet(
    (first_conv): Sequential(
      (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
      (2): ReLU6(inplace=True)
    )
    (first_block): InvertedResidual(
      (depth_conv): Sequential(
        (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)
        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
        (2): ReLU6(inplace=True)
      )
      (point_linear): Sequential(
        (0): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
      )
    )
    (blocks): ModuleList(
      (0): ModuleList(
        (0): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(16, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(48, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=48, bias=False)
            (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(48, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (1): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)
            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(96, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (2): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(16, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(48, 48, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=48, bias=False)
            (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(48, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (3): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(96, 96, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=96, bias=False)
            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(96, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (4): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(16, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(48, 48, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), groups=48, bias=False)
            (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(48, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (5): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(96, 96, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), groups=96, bias=False)
            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(96, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
      )
      (1): ModuleList(
        (0): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(32, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(96, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (1): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (2): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(32, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96, bias=False)
            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(96, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (3): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(192, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=192, bias=False)
            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (4): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(32, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96, bias=False)
            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(96, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (5): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192, bias=False)
            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (6): Identity()
      )
      (2): ModuleList(
        (0): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(32, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(96, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (1): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (2): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(32, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96, bias=False)
            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(96, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (3): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(192, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=192, bias=False)
            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (4): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(32, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96, bias=False)
            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(96, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (5): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192, bias=False)
            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (6): Identity()
      )
      (3): ModuleList(
        (0): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(32, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(96, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (1): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (2): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(32, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96, bias=False)
            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(96, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (3): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(192, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=192, bias=False)
            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (4): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(32, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96, bias=False)
            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(96, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (5): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192, bias=False)
            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (6): Identity()
      )
      (4): ModuleList(
        (0): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(32, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)
            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(96, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (1): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)
            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(192, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (2): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(32, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(96, 96, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=96, bias=False)
            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(96, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (3): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(192, 192, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=192, bias=False)
            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(192, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (4): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(32, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(96, 96, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), groups=96, bias=False)
            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(96, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (5): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(192, 192, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), groups=192, bias=False)
            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(192, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
      )
      (5): ModuleList(
        (0): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(120, 120, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=120, bias=False)
            (1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(120, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (1): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(240, 240, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=240, bias=False)
            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (2): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(120, 120, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=120, bias=False)
            (1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(120, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (3): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(240, 240, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=240, bias=False)
            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (4): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(120, 120, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=120, bias=False)
            (1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(120, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (5): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(240, 240, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=240, bias=False)
            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (6): Identity()
      )
      (6): ModuleList(
        (0): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(120, 120, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=120, bias=False)
            (1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(120, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (1): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(240, 240, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=240, bias=False)
            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (2): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(120, 120, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=120, bias=False)
            (1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(120, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (3): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(240, 240, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=240, bias=False)
            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (4): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(120, 120, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=120, bias=False)
            (1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(120, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (5): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(240, 240, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=240, bias=False)
            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (6): Identity()
      )
      (7): ModuleList(
        (0): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(120, 120, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=120, bias=False)
            (1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(120, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (1): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(240, 240, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=240, bias=False)
            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (2): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(120, 120, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=120, bias=False)
            (1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(120, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (3): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(240, 240, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=240, bias=False)
            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (4): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(120, 120, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=120, bias=False)
            (1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(120, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (5): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(240, 240, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=240, bias=False)
            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (6): Identity()
      )
      (8): ModuleList(
        (0): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(120, 120, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=120, bias=False)
            (1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(120, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (1): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(240, 240, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=240, bias=False)
            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (2): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(120, 120, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=120, bias=False)
            (1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(120, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (3): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(240, 240, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=240, bias=False)
            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (4): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(120, 120, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), groups=120, bias=False)
            (1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(120, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (5): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(240, 240, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), groups=240, bias=False)
            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
      )
      (9): ModuleList(
        (0): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(80, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(240, 240, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=240, bias=False)
            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (1): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)
            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (2): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(80, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(240, 240, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=240, bias=False)
            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (3): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(480, 480, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=480, bias=False)
            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (4): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(80, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(240, 240, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=240, bias=False)
            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (5): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(480, 480, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=480, bias=False)
            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (6): Identity()
      )
      (10): ModuleList(
        (0): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(80, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(240, 240, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=240, bias=False)
            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (1): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)
            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (2): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(80, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(240, 240, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=240, bias=False)
            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (3): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(480, 480, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=480, bias=False)
            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (4): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(80, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(240, 240, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=240, bias=False)
            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (5): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(480, 480, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=480, bias=False)
            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (6): Identity()
      )
      (11): ModuleList(
        (0): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(80, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(240, 240, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=240, bias=False)
            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (1): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)
            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (2): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(80, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(240, 240, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=240, bias=False)
            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (3): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(480, 480, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=480, bias=False)
            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (4): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(80, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(240, 240, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=240, bias=False)
            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (5): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(480, 480, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=480, bias=False)
            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (6): Identity()
      )
      (12): ModuleList(
        (0): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(80, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(240, 240, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=240, bias=False)
            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(240, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (1): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)
            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(480, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (2): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(80, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(240, 240, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=240, bias=False)
            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(240, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (3): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(480, 480, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=480, bias=False)
            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(480, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (4): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(80, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(240, 240, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=240, bias=False)
            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(240, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (5): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(480, 480, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=480, bias=False)
            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(480, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
      )
      (13): ModuleList(
        (0): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(96, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
            (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(288, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (1): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)
            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (2): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(96, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(288, 288, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=288, bias=False)
            (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(288, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (3): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(576, 576, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=576, bias=False)
            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (4): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(96, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(288, 288, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=288, bias=False)
            (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(288, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (5): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(576, 576, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=576, bias=False)
            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (6): Identity()
      )
      (14): ModuleList(
        (0): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(96, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
            (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(288, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (1): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)
            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (2): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(96, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(288, 288, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=288, bias=False)
            (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(288, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (3): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(576, 576, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=576, bias=False)
            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (4): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(96, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(288, 288, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=288, bias=False)
            (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(288, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (5): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(576, 576, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=576, bias=False)
            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (6): Identity()
      )
      (15): ModuleList(
        (0): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(96, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
            (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(288, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (1): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)
            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (2): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(96, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(288, 288, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=288, bias=False)
            (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(288, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (3): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(576, 576, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=576, bias=False)
            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (4): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(96, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(288, 288, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=288, bias=False)
            (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(288, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (5): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(576, 576, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=576, bias=False)
            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (6): Identity()
      )
      (16): ModuleList(
        (0): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(96, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(288, 288, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=288, bias=False)
            (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(288, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (1): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=576, bias=False)
            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(576, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (2): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(96, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(288, 288, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=288, bias=False)
            (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(288, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (3): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(576, 576, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=576, bias=False)
            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(576, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (4): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(96, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(288, 288, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), groups=288, bias=False)
            (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(288, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (5): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(576, 576, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), groups=576, bias=False)
            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(576, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
      )
      (17): ModuleList(
        (0): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(192, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)
            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(576, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (1): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152, bias=False)
            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (2): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(192, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(576, 576, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=576, bias=False)
            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(576, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (3): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)
            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (4): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(192, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(576, 576, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=576, bias=False)
            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(576, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (5): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(1152, 1152, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1152, bias=False)
            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (6): Identity()
      )
      (18): ModuleList(
        (0): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(192, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)
            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(576, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (1): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152, bias=False)
            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (2): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(192, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(576, 576, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=576, bias=False)
            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(576, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (3): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)
            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (4): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(192, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(576, 576, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=576, bias=False)
            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(576, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (5): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(1152, 1152, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1152, bias=False)
            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (6): Identity()
      )
      (19): ModuleList(
        (0): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(192, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)
            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(576, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (1): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152, bias=False)
            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (2): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(192, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(576, 576, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=576, bias=False)
            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(576, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (3): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)
            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (4): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(192, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(576, 576, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=576, bias=False)
            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(576, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (5): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(1152, 1152, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1152, bias=False)
            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (6): Identity()
      )
      (20): ModuleList(
        (0): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(192, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)
            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(576, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (1): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152, bias=False)
            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(1152, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (2): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(192, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(576, 576, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=576, bias=False)
            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(576, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (3): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)
            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(1152, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (4): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(192, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(576, 576, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=576, bias=False)
            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(576, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (5): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(1152, 1152, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1152, bias=False)
            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(1152, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
      )
    )
    (feature_mix_layer): Sequential(
      (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
      (2): ReLU6(inplace=True)
    )
    (avgpool): AvgPool2d(kernel_size=7, stride=7, padding=0)
    (classifier): Sequential(
      (0): Linear(in_features=1280, out_features=1000, bias=True)
    )
  )
)
[05/08 02:17:16] SuperNet Training INFO: Trainset Size: 1231167
[05/08 02:17:16] SuperNet Training INFO: Validset Size:   50000
[05/08 02:17:16] SuperNet Training INFO: Compose(
    RandomResizedCrop(size=(224, 224), scale=(0.08, 1.0), ratio=(0.75, 1.3333), interpolation=bilinear)
    RandomHorizontalFlip(p=0.5)
    ToTensor()
    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
)
[05/08 02:17:16] SuperNet Training INFO: --> START mobile0-tbs33840-seed-0
[05/08 02:17:16] SuperNet Training INFO: {0: 0, 1: 0}
[05/08 02:20:03] SuperNet Training INFO: iter:   120/144360  CE: 6.9419  
[05/08 02:21:02] SuperNet Training INFO: iter:   240/144360  CE: 6.8685  
[05/08 02:22:00] SuperNet Training INFO: iter:   360/144360  CE: 6.9013  
[05/08 02:22:58] SuperNet Training INFO: iter:   480/144360  CE: 6.9368  
[05/08 02:23:56] SuperNet Training INFO: iter:   600/144360  CE: 6.9366  
[05/08 02:24:54] SuperNet Training INFO: iter:   720/144360  CE: 6.8869  
[05/08 02:25:53] SuperNet Training INFO: iter:   840/144360  CE: 6.8742  
[05/08 02:26:51] SuperNet Training INFO: iter:   960/144360  CE: 6.9063  
[05/08 02:27:49] SuperNet Training INFO: iter:  1080/144360  CE: 6.8543  
[05/08 02:28:48] SuperNet Training INFO: iter:  1200/144360  CE: 6.9367  
[05/08 02:28:49] SuperNet Training INFO: --> epoch:   1/120  avg CE: 6.8931  lr: 0.11997943949853311  
[05/08 02:30:28] SuperNet Training INFO: iter:  1320/144360  CE: 6.8293  
[05/08 02:31:28] SuperNet Training INFO: iter:  1440/144360  CE: 6.8150  
[05/08 02:32:28] SuperNet Training INFO: iter:  1560/144360  CE: 6.8001  
[05/08 02:33:28] SuperNet Training INFO: iter:  1680/144360  CE: 6.7113  
[05/08 02:34:28] SuperNet Training INFO: iter:  1800/144360  CE: 6.7829  
[05/08 02:35:29] SuperNet Training INFO: iter:  1920/144360  CE: 6.8468  
[05/08 02:36:30] SuperNet Training INFO: iter:  2040/144360  CE: 6.8585  
[05/08 02:37:30] SuperNet Training INFO: iter:  2160/144360  CE: 6.7603  
[05/08 02:38:32] SuperNet Training INFO: iter:  2280/144360  CE: 6.7372  
[05/08 02:39:32] SuperNet Training INFO: iter:  2400/144360  CE: 6.6106  
[05/08 02:39:34] SuperNet Training INFO: --> epoch:   2/120  avg CE: 6.7812  lr: 0.11991777208527424  
[05/08 02:41:11] SuperNet Training INFO: iter:  2520/144360  CE: 6.7329  
[05/08 02:42:11] SuperNet Training INFO: iter:  2640/144360  CE: 6.8046  
[05/08 02:43:11] SuperNet Training INFO: iter:  2760/144360  CE: 6.7000  
[05/08 02:44:12] SuperNet Training INFO: iter:  2880/144360  CE: 6.7194  
[05/08 02:45:11] SuperNet Training INFO: iter:  3000/144360  CE: 6.7245  
[05/08 02:46:11] SuperNet Training INFO: iter:  3120/144360  CE: 6.6804  
[05/08 02:47:13] SuperNet Training INFO: iter:  3240/144360  CE: 6.5181  
[05/08 02:48:13] SuperNet Training INFO: iter:  3360/144360  CE: 6.7276  
[05/08 02:49:13] SuperNet Training INFO: iter:  3480/144360  CE: 6.6888  
[05/08 02:50:12] SuperNet Training INFO: iter:  3600/144360  CE: 6.6039  
[05/08 02:50:16] SuperNet Training INFO: --> epoch:   3/120  avg CE: 6.6670  lr: 0.11981504002398749  
[05/08 02:51:51] SuperNet Training INFO: iter:  3720/144360  CE: 6.4761  
[05/08 02:52:52] SuperNet Training INFO: iter:  3840/144360  CE: 6.7723  
[05/08 02:53:53] SuperNet Training INFO: iter:  3960/144360  CE: 6.6425  
[05/08 02:54:54] SuperNet Training INFO: iter:  4080/144360  CE: 6.4800  
[05/08 02:55:55] SuperNet Training INFO: iter:  4200/144360  CE: 6.4583  
[05/08 02:56:55] SuperNet Training INFO: iter:  4320/144360  CE: 6.4539  
[05/08 02:57:55] SuperNet Training INFO: iter:  4440/144360  CE: 6.4335  
[05/08 02:58:55] SuperNet Training INFO: iter:  4560/144360  CE: 6.4770  
[05/08 02:59:56] SuperNet Training INFO: iter:  4680/144360  CE: 6.3887  
[05/08 03:00:57] SuperNet Training INFO: iter:  4800/144360  CE: 6.4671  
[05/08 03:01:02] SuperNet Training INFO: --> epoch:   4/120  avg CE: 6.5040  lr: 0.11967131372209595  
[05/08 03:02:36] SuperNet Training INFO: iter:  4920/144360  CE: 6.3822  
[05/08 03:03:36] SuperNet Training INFO: iter:  5040/144360  CE: 6.3208  
[05/08 03:04:35] SuperNet Training INFO: iter:  5160/144360  CE: 6.2492  
[05/08 03:05:36] SuperNet Training INFO: iter:  5280/144360  CE: 6.3023  
[05/08 03:06:35] SuperNet Training INFO: iter:  5400/144360  CE: 6.2929  
[05/08 03:07:36] SuperNet Training INFO: iter:  5520/144360  CE: 6.3395  
[05/08 03:08:36] SuperNet Training INFO: iter:  5640/144360  CE: 6.3217  
[05/08 03:09:36] SuperNet Training INFO: iter:  5760/144360  CE: 6.3043  
[05/08 03:10:36] SuperNet Training INFO: iter:  5880/144360  CE: 6.2157  
[05/08 03:11:35] SuperNet Training INFO: iter:  6000/144360  CE: 6.1854  
[05/08 03:11:41] SuperNet Training INFO: --> epoch:   5/120  avg CE: 6.3330  lr: 0.11948669168242801  
[05/08 03:13:14] SuperNet Training INFO: iter:  6120/144360  CE: 6.1988  
[05/08 03:14:14] SuperNet Training INFO: iter:  6240/144360  CE: 6.1530  
[05/08 03:15:14] SuperNet Training INFO: iter:  6360/144360  CE: 6.3509  
[05/08 03:16:14] SuperNet Training INFO: iter:  6480/144360  CE: 6.2635  
[05/08 03:17:16] SuperNet Training INFO: iter:  6600/144360  CE: 6.1822  
[05/08 03:18:15] SuperNet Training INFO: iter:  6720/144360  CE: 6.0646  
[05/08 03:19:15] SuperNet Training INFO: iter:  6840/144360  CE: 6.2445  
[05/08 03:20:16] SuperNet Training INFO: iter:  6960/144360  CE: 6.0698  
[05/08 03:21:16] SuperNet Training INFO: iter:  7080/144360  CE: 6.1238  
[05/08 03:22:16] SuperNet Training INFO: iter:  7200/144360  CE: 6.1093  
[05/08 03:22:24] SuperNet Training INFO: --> epoch:   6/120  avg CE: 6.1768  lr: 0.1192613004357081  
[05/08 03:23:53] SuperNet Training INFO: iter:  7320/144360  CE: 6.2586  
[05/08 03:24:55] SuperNet Training INFO: iter:  7440/144360  CE: 6.0006  
[05/08 03:25:55] SuperNet Training INFO: iter:  7560/144360  CE: 6.0536  
[05/08 03:26:55] SuperNet Training INFO: iter:  7680/144360  CE: 6.2331  
[05/08 03:27:56] SuperNet Training INFO: iter:  7800/144360  CE: 5.9787  
[05/08 03:28:56] SuperNet Training INFO: iter:  7920/144360  CE: 5.9966  
[05/08 03:29:56] SuperNet Training INFO: iter:  8040/144360  CE: 6.0973  
[05/08 03:30:56] SuperNet Training INFO: iter:  8160/144360  CE: 5.8975  
[05/08 03:31:57] SuperNet Training INFO: iter:  8280/144360  CE: 5.7646  
[05/08 03:32:58] SuperNet Training INFO: iter:  8400/144360  CE: 5.8826  
[05/08 03:33:08] SuperNet Training INFO: --> epoch:   7/120  avg CE: 6.0135  lr: 0.11899529445383715  
[05/08 03:34:37] SuperNet Training INFO: iter:  8520/144360  CE: 6.0315  
[05/08 03:35:37] SuperNet Training INFO: iter:  8640/144360  CE: 5.8488  
[05/08 03:36:38] SuperNet Training INFO: iter:  8760/144360  CE: 5.6855  
[05/08 03:37:38] SuperNet Training INFO: iter:  8880/144360  CE: 5.9918  
[05/08 03:38:38] SuperNet Training INFO: iter:  9000/144360  CE: 5.7147  
[05/08 03:39:38] SuperNet Training INFO: iter:  9120/144360  CE: 5.6442  
[05/08 03:40:39] SuperNet Training INFO: iter:  9240/144360  CE: 5.6779  
[05/08 03:41:39] SuperNet Training INFO: iter:  9360/144360  CE: 5.8674  
[05/08 03:42:38] SuperNet Training INFO: iter:  9480/144360  CE: 6.0418  
[05/08 03:43:39] SuperNet Training INFO: iter:  9600/144360  CE: 5.7789  
[05/08 03:43:51] SuperNet Training INFO: --> epoch:   8/120  avg CE: 5.8750  lr: 0.11868885604402826  
[05/08 03:45:18] SuperNet Training INFO: iter:  9720/144360  CE: 5.8092  
[05/08 03:46:17] SuperNet Training INFO: iter:  9840/144360  CE: 5.8633  
[05/08 03:47:16] SuperNet Training INFO: iter:  9960/144360  CE: 5.6648  
[05/08 03:48:16] SuperNet Training INFO: iter: 10080/144360  CE: 5.5952  
[05/08 03:49:16] SuperNet Training INFO: iter: 10200/144360  CE: 5.8596  
[05/08 03:50:17] SuperNet Training INFO: iter: 10320/144360  CE: 5.7509  
[05/08 03:51:17] SuperNet Training INFO: iter: 10440/144360  CE: 5.7151  
[05/08 03:52:18] SuperNet Training INFO: iter: 10560/144360  CE: 5.6281  
[05/08 03:53:18] SuperNet Training INFO: iter: 10680/144360  CE: 5.6946  
[05/08 03:54:17] SuperNet Training INFO: iter: 10800/144360  CE: 5.5146  
[05/08 03:54:30] SuperNet Training INFO: --> epoch:   9/120  avg CE: 5.7544  lr: 0.11834219522386061  
[05/08 03:55:56] SuperNet Training INFO: iter: 10920/144360  CE: 5.7650  
[05/08 03:56:56] SuperNet Training INFO: iter: 11040/144360  CE: 5.5007  
[05/08 03:57:57] SuperNet Training INFO: iter: 11160/144360  CE: 5.5837  
[05/08 03:58:57] SuperNet Training INFO: iter: 11280/144360  CE: 5.5514  
[05/08 03:59:57] SuperNet Training INFO: iter: 11400/144360  CE: 5.2789  
[05/08 04:00:57] SuperNet Training INFO: iter: 11520/144360  CE: 5.6370  
[05/08 04:01:57] SuperNet Training INFO: iter: 11640/144360  CE: 5.6836  
[05/08 04:02:57] SuperNet Training INFO: iter: 11760/144360  CE: 5.6037  
[05/08 04:03:58] SuperNet Training INFO: iter: 11880/144360  CE: 5.4428  
[05/08 04:04:58] SuperNet Training INFO: iter: 12000/144360  CE: 5.4686  
[05/08 04:05:12] SuperNet Training INFO: --> epoch:  10/120  avg CE: 5.6333  lr: 0.1179555495773443  
[05/08 04:06:37] SuperNet Training INFO: iter: 12120/144360  CE: 5.5413  
[05/08 04:07:37] SuperNet Training INFO: iter: 12240/144360  CE: 5.7402  
[05/08 04:08:37] SuperNet Training INFO: iter: 12360/144360  CE: 5.4816  
[05/08 04:09:36] SuperNet Training INFO: iter: 12480/144360  CE: 5.7746  
[05/08 04:10:39] SuperNet Training INFO: iter: 12600/144360  CE: 5.2576  
[05/08 04:11:39] SuperNet Training INFO: iter: 12720/144360  CE: 5.6688  
[05/08 04:12:39] SuperNet Training INFO: iter: 12840/144360  CE: 5.4565  
[05/08 04:13:39] SuperNet Training INFO: iter: 12960/144360  CE: 5.3880  
[05/08 04:14:39] SuperNet Training INFO: iter: 13080/144360  CE: 5.3449  
[05/08 04:15:40] SuperNet Training INFO: iter: 13200/144360  CE: 5.4530  
[05/08 04:15:56] SuperNet Training INFO: --> epoch:  11/120  avg CE: 5.5076  lr: 0.11752918409209158  
[05/08 04:17:19] SuperNet Training INFO: iter: 13320/144360  CE: 5.4833  
[05/08 04:18:20] SuperNet Training INFO: iter: 13440/144360  CE: 5.2532  
[05/08 04:19:20] SuperNet Training INFO: iter: 13560/144360  CE: 5.4835  
[05/08 04:20:20] SuperNet Training INFO: iter: 13680/144360  CE: 5.4210  
[05/08 04:21:20] SuperNet Training INFO: iter: 13800/144360  CE: 5.1471  
[05/08 04:22:21] SuperNet Training INFO: iter: 13920/144360  CE: 5.2336  
[05/08 04:23:20] SuperNet Training INFO: iter: 14040/144360  CE: 5.2913  
[05/08 04:24:21] SuperNet Training INFO: iter: 14160/144360  CE: 5.3322  
[05/08 04:25:22] SuperNet Training INFO: iter: 14280/144360  CE: 5.2790  
[05/08 04:26:23] SuperNet Training INFO: iter: 14400/144360  CE: 5.2426  
[05/08 04:26:40] SuperNet Training INFO: --> epoch:  12/120  avg CE: 5.3865  lr: 0.11706339097770935  
[05/08 04:28:02] SuperNet Training INFO: iter: 14520/144360  CE: 5.3131  
[05/08 04:29:03] SuperNet Training INFO: iter: 14640/144360  CE: 5.2101  
[05/08 04:30:03] SuperNet Training INFO: iter: 14760/144360  CE: 5.2066  
[05/08 04:31:03] SuperNet Training INFO: iter: 14880/144360  CE: 5.1511  
[05/08 04:32:04] SuperNet Training INFO: iter: 15000/144360  CE: 5.4082  
[05/08 04:33:04] SuperNet Training INFO: iter: 15120/144360  CE: 5.1770  
[05/08 04:34:05] SuperNet Training INFO: iter: 15240/144360  CE: 5.1369  
[05/08 04:35:05] SuperNet Training INFO: iter: 15360/144360  CE: 5.1028  
[05/08 04:36:05] SuperNet Training INFO: iter: 15480/144360  CE: 5.0498  
[05/08 04:37:05] SuperNet Training INFO: iter: 15600/144360  CE: 5.1672  
[05/08 04:37:23] SuperNet Training INFO: --> epoch:  13/120  avg CE: 5.2558  lr: 0.11655848946553125  
[05/08 04:38:42] SuperNet Training INFO: iter: 15720/144360  CE: 5.2135  
[05/08 04:39:41] SuperNet Training INFO: iter: 15840/144360  CE: 5.1178  
[05/08 04:40:41] SuperNet Training INFO: iter: 15960/144360  CE: 5.0666  
[05/08 04:41:41] SuperNet Training INFO: iter: 16080/144360  CE: 4.9314  
[05/08 04:42:41] SuperNet Training INFO: iter: 16200/144360  CE: 5.2346  
[05/08 04:43:40] SuperNet Training INFO: iter: 16320/144360  CE: 5.1526  
[05/08 04:44:40] SuperNet Training INFO: iter: 16440/144360  CE: 5.2214  
[05/08 04:45:40] SuperNet Training INFO: iter: 16560/144360  CE: 5.1834  
[05/08 04:46:41] SuperNet Training INFO: iter: 16680/144360  CE: 5.1477  
[05/08 04:47:41] SuperNet Training INFO: iter: 16800/144360  CE: 5.3326  
[05/08 04:48:01] SuperNet Training INFO: --> epoch:  14/120  avg CE: 5.1404  lr: 0.11601482558983225  
[05/08 04:49:21] SuperNet Training INFO: iter: 16920/144360  CE: 5.1689  
[05/08 04:50:22] SuperNet Training INFO: iter: 17040/144360  CE: 5.2602  
[05/08 04:51:22] SuperNet Training INFO: iter: 17160/144360  CE: 4.9546  
[05/08 04:52:23] SuperNet Training INFO: iter: 17280/144360  CE: 4.7353  
[05/08 04:53:23] SuperNet Training INFO: iter: 17400/144360  CE: 5.1409  
[05/08 04:54:23] SuperNet Training INFO: iter: 17520/144360  CE: 5.0374  
[05/08 04:55:24] SuperNet Training INFO: iter: 17640/144360  CE: 5.0890  
[05/08 04:56:23] SuperNet Training INFO: iter: 17760/144360  CE: 4.9409  
[05/08 04:57:24] SuperNet Training INFO: iter: 17880/144360  CE: 4.9389  
[05/08 04:58:25] SuperNet Training INFO: iter: 18000/144360  CE: 5.0114  
[05/08 04:58:46] SuperNet Training INFO: --> epoch:  15/120  avg CE: 5.0374  lr: 0.11543277195067722  
[05/08 05:00:02] SuperNet Training INFO: iter: 18120/144360  CE: 4.7431  
[05/08 05:01:03] SuperNet Training INFO: iter: 18240/144360  CE: 5.0034  
[05/08 05:02:02] SuperNet Training INFO: iter: 18360/144360  CE: 5.0640  
[05/08 05:03:02] SuperNet Training INFO: iter: 18480/144360  CE: 4.9979  
[05/08 05:04:02] SuperNet Training INFO: iter: 18600/144360  CE: 5.2016  
[05/08 05:05:02] SuperNet Training INFO: iter: 18720/144360  CE: 4.5731  
[05/08 05:06:03] SuperNet Training INFO: iter: 18840/144360  CE: 4.7649  
[05/08 05:07:04] SuperNet Training INFO: iter: 18960/144360  CE: 4.9209  
[05/08 05:08:03] SuperNet Training INFO: iter: 19080/144360  CE: 4.8856  
[05/08 05:09:04] SuperNet Training INFO: iter: 19200/144360  CE: 4.9356  
[05/08 05:09:27] SuperNet Training INFO: --> epoch:  16/120  avg CE: 4.9362  lr: 0.1148127274585561  
[05/08 05:10:42] SuperNet Training INFO: iter: 19320/144360  CE: 4.8253  
[05/08 05:11:41] SuperNet Training INFO: iter: 19440/144360  CE: 4.7535  
[05/08 05:12:41] SuperNet Training INFO: iter: 19560/144360  CE: 4.8814  
[05/08 05:13:40] SuperNet Training INFO: iter: 19680/144360  CE: 4.8561  
[05/08 05:14:40] SuperNet Training INFO: iter: 19800/144360  CE: 4.5179  
[05/08 05:15:40] SuperNet Training INFO: iter: 19920/144360  CE: 4.8235  
[05/08 05:16:41] SuperNet Training INFO: iter: 20040/144360  CE: 4.9118  
[05/08 05:17:42] SuperNet Training INFO: iter: 20160/144360  CE: 4.9645  
[05/08 05:18:42] SuperNet Training INFO: iter: 20280/144360  CE: 4.8408  
[05/08 05:19:43] SuperNet Training INFO: iter: 20400/144360  CE: 4.8447  
[05/08 05:20:07] SuperNet Training INFO: --> epoch:  17/120  avg CE: 4.8455  lr: 0.11415511706099139  
[05/08 05:21:21] SuperNet Training INFO: iter: 20520/144360  CE: 5.0302  
[05/08 05:22:22] SuperNet Training INFO: iter: 20640/144360  CE: 4.7265  
[05/08 05:23:23] SuperNet Training INFO: iter: 20760/144360  CE: 4.7227  
[05/08 05:24:23] SuperNet Training INFO: iter: 20880/144360  CE: 5.0304  
[05/08 05:25:23] SuperNet Training INFO: iter: 21000/144360  CE: 4.6583  
[05/08 05:26:23] SuperNet Training INFO: iter: 21120/144360  CE: 4.7932  
[05/08 05:27:23] SuperNet Training INFO: iter: 21240/144360  CE: 4.8486  
[05/08 05:28:23] SuperNet Training INFO: iter: 21360/144360  CE: 4.8655  
[05/08 05:29:23] SuperNet Training INFO: iter: 21480/144360  CE: 4.7137  
[05/08 05:30:23] SuperNet Training INFO: iter: 21600/144360  CE: 4.7020  
[05/08 05:30:48] SuperNet Training INFO: --> epoch:  18/120  avg CE: 4.7654  lr: 0.11346039145130195  
[05/08 05:32:00] SuperNet Training INFO: iter: 21720/144360  CE: 4.5551  
[05/08 05:33:01] SuperNet Training INFO: iter: 21840/144360  CE: 4.7104  
[05/08 05:34:00] SuperNet Training INFO: iter: 21960/144360  CE: 4.7317  
[05/08 05:35:01] SuperNet Training INFO: iter: 22080/144360  CE: 4.4938  
[05/08 05:36:02] SuperNet Training INFO: iter: 22200/144360  CE: 4.6265  
[05/08 05:37:03] SuperNet Training INFO: iter: 22320/144360  CE: 4.7392  
[05/08 05:38:02] SuperNet Training INFO: iter: 22440/144360  CE: 4.8316  
[05/08 05:39:03] SuperNet Training INFO: iter: 22560/144360  CE: 4.7701  
[05/08 05:40:02] SuperNet Training INFO: iter: 22680/144360  CE: 4.7066  
[05/08 05:41:02] SuperNet Training INFO: iter: 22800/144360  CE: 4.7117  
[05/08 05:41:30] SuperNet Training INFO: --> epoch:  19/120  avg CE: 4.6903  lr: 0.11272902675971772  
[05/08 05:42:40] SuperNet Training INFO: iter: 22920/144360  CE: 4.5818  
[05/08 05:43:40] SuperNet Training INFO: iter: 23040/144360  CE: 4.4070  
[05/08 05:44:40] SuperNet Training INFO: iter: 23160/144360  CE: 4.7253  
[05/08 05:45:40] SuperNet Training INFO: iter: 23280/144360  CE: 4.7401  
[05/08 05:46:40] SuperNet Training INFO: iter: 23400/144360  CE: 4.7763  
[05/08 05:47:40] SuperNet Training INFO: iter: 23520/144360  CE: 4.7297  
[05/08 05:48:41] SuperNet Training INFO: iter: 23640/144360  CE: 4.7430  
[05/08 05:49:41] SuperNet Training INFO: iter: 23760/144360  CE: 4.4781  
[05/08 05:50:42] SuperNet Training INFO: iter: 23880/144360  CE: 4.6506  
[05/08 05:51:42] SuperNet Training INFO: iter: 24000/144360  CE: 4.7154  
[05/08 05:52:12] SuperNet Training INFO: --> epoch:  20/120  avg CE: 4.6174  lr: 0.11196152422706572  
[05/08 05:53:22] SuperNet Training INFO: iter: 24120/144360  CE: 4.6664  
[05/08 05:54:21] SuperNet Training INFO: iter: 24240/144360  CE: 4.8486  
[05/08 05:55:21] SuperNet Training INFO: iter: 24360/144360  CE: 4.4856  
[05/08 05:56:22] SuperNet Training INFO: iter: 24480/144360  CE: 4.3058  
[05/08 05:57:22] SuperNet Training INFO: iter: 24600/144360  CE: 4.7345  
[05/08 05:58:22] SuperNet Training INFO: iter: 24720/144360  CE: 4.4977  
[05/08 05:59:22] SuperNet Training INFO: iter: 24840/144360  CE: 4.5429  
[05/08 06:00:22] SuperNet Training INFO: iter: 24960/144360  CE: 4.5064  
[05/08 06:01:22] SuperNet Training INFO: iter: 25080/144360  CE: 4.4793  
[05/08 06:02:23] SuperNet Training INFO: iter: 25200/144360  CE: 4.3565  
[05/08 06:02:55] SuperNet Training INFO: --> epoch:  21/120  avg CE: 4.5381  lr: 0.11115840986124514  
[05/08 06:04:02] SuperNet Training INFO: iter: 25320/144360  CE: 4.4801  
[05/08 06:05:04] SuperNet Training INFO: iter: 25440/144360  CE: 4.6316  
[05/08 06:06:07] SuperNet Training INFO: iter: 25560/144360  CE: 4.8250  
[05/08 06:07:09] SuperNet Training INFO: iter: 25680/144360  CE: 4.4249  
[05/08 06:08:11] SuperNet Training INFO: iter: 25800/144360  CE: 4.3289  
[05/08 06:09:12] SuperNet Training INFO: iter: 25920/144360  CE: 4.5737  
[05/08 06:10:15] SuperNet Training INFO: iter: 26040/144360  CE: 4.5974  
[05/08 06:11:17] SuperNet Training INFO: iter: 26160/144360  CE: 4.4966  
[05/08 06:12:19] SuperNet Training INFO: iter: 26280/144360  CE: 4.6718  
[05/08 06:13:19] SuperNet Training INFO: iter: 26400/144360  CE: 4.2259  
[05/08 06:13:52] SuperNet Training INFO: --> epoch:  22/120  avg CE: 4.4755  lr: 0.11032023407672516  
[05/08 06:14:58] SuperNet Training INFO: iter: 26520/144360  CE: 4.4506  
[05/08 06:15:59] SuperNet Training INFO: iter: 26640/144360  CE: 4.4953  
[05/08 06:17:01] SuperNet Training INFO: iter: 26760/144360  CE: 4.4995  
[05/08 06:18:02] SuperNet Training INFO: iter: 26880/144360  CE: 4.5406  
[05/08 06:19:04] SuperNet Training INFO: iter: 27000/144360  CE: 4.4344  
[05/08 06:20:08] SuperNet Training INFO: iter: 27120/144360  CE: 4.3983  
[05/08 06:21:11] SuperNet Training INFO: iter: 27240/144360  CE: 4.3811  
[05/08 06:22:14] SuperNet Training INFO: iter: 27360/144360  CE: 4.1618  
[05/08 06:23:16] SuperNet Training INFO: iter: 27480/144360  CE: 4.3633  
[05/08 06:24:17] SuperNet Training INFO: iter: 27600/144360  CE: 4.6138  
[05/08 06:24:51] SuperNet Training INFO: --> epoch:  23/120  avg CE: 4.4142  lr: 0.10944757131732062  
[05/08 06:25:57] SuperNet Training INFO: iter: 27720/144360  CE: 4.6370  
[05/08 06:26:58] SuperNet Training INFO: iter: 27840/144360  CE: 4.4759  
[05/08 06:27:59] SuperNet Training INFO: iter: 27960/144360  CE: 4.6229  
[05/08 06:29:01] SuperNet Training INFO: iter: 28080/144360  CE: 4.2949  
[05/08 06:30:03] SuperNet Training INFO: iter: 28200/144360  CE: 4.3467  
[05/08 06:31:05] SuperNet Training INFO: iter: 28320/144360  CE: 4.0247  
[05/08 06:32:07] SuperNet Training INFO: iter: 28440/144360  CE: 4.3167  
[05/08 06:33:09] SuperNet Training INFO: iter: 28560/144360  CE: 4.1086  
[05/08 06:34:10] SuperNet Training INFO: iter: 28680/144360  CE: 4.3109  
[05/08 06:35:10] SuperNet Training INFO: iter: 28800/144360  CE: 4.3671  
[05/08 06:35:46] SuperNet Training INFO: --> epoch:  24/120  avg CE: 4.3724  lr: 0.10854101966249682  
[05/08 06:36:50] SuperNet Training INFO: iter: 28920/144360  CE: 4.5174  
[05/08 06:37:51] SuperNet Training INFO: iter: 29040/144360  CE: 4.1920  
[05/08 06:38:53] SuperNet Training INFO: iter: 29160/144360  CE: 4.4044  
[05/08 06:39:56] SuperNet Training INFO: iter: 29280/144360  CE: 4.3953  
[05/08 06:40:58] SuperNet Training INFO: iter: 29400/144360  CE: 4.2863  
[05/08 06:41:59] SuperNet Training INFO: iter: 29520/144360  CE: 4.1635  
[05/08 06:43:02] SuperNet Training INFO: iter: 29640/144360  CE: 4.2444  
[05/08 06:44:03] SuperNet Training INFO: iter: 29760/144360  CE: 4.1617  
[05/08 06:45:04] SuperNet Training INFO: iter: 29880/144360  CE: 4.1869  
[05/08 06:46:05] SuperNet Training INFO: iter: 30000/144360  CE: 4.1548  
[05/08 06:46:42] SuperNet Training INFO: --> epoch:  25/120  avg CE: 4.3117  lr: 0.10760120041747454  
[05/08 06:47:44] SuperNet Training INFO: iter: 30120/144360  CE: 4.2594  
[05/08 06:48:45] SuperNet Training INFO: iter: 30240/144360  CE: 4.1460  
[05/08 06:49:48] SuperNet Training INFO: iter: 30360/144360  CE: 4.2903  
[05/08 06:50:50] SuperNet Training INFO: iter: 30480/144360  CE: 4.0653  
[05/08 06:51:52] SuperNet Training INFO: iter: 30600/144360  CE: 4.1613  
[05/08 06:52:55] SuperNet Training INFO: iter: 30720/144360  CE: 4.1424  
[05/08 06:53:58] SuperNet Training INFO: iter: 30840/144360  CE: 4.0847  
[05/08 06:55:00] SuperNet Training INFO: iter: 30960/144360  CE: 4.3651  
[05/08 06:56:02] SuperNet Training INFO: iter: 31080/144360  CE: 4.0878  
[05/08 06:57:04] SuperNet Training INFO: iter: 31200/144360  CE: 4.4935  
[05/08 06:57:43] SuperNet Training INFO: --> epoch:  26/120  avg CE: 4.2513  lr: 0.10662875768741867  
[05/08 06:58:43] SuperNet Training INFO: iter: 31320/144360  CE: 4.1833  
[05/08 06:59:46] SuperNet Training INFO: iter: 31440/144360  CE: 4.4006  
[05/08 07:00:48] SuperNet Training INFO: iter: 31560/144360  CE: 4.2809  
[05/08 07:01:51] SuperNet Training INFO: iter: 31680/144360  CE: 4.3404  
[05/08 07:02:53] SuperNet Training INFO: iter: 31800/144360  CE: 4.0390  
[05/08 07:03:55] SuperNet Training INFO: iter: 31920/144360  CE: 4.4695  
[05/08 07:04:57] SuperNet Training INFO: iter: 32040/144360  CE: 4.3812  
[05/08 07:05:59] SuperNet Training INFO: iter: 32160/144360  CE: 4.0862  
[05/08 07:07:00] SuperNet Training INFO: iter: 32280/144360  CE: 4.0885  
[05/08 07:08:01] SuperNet Training INFO: iter: 32400/144360  CE: 4.1656  
[05/08 07:08:42] SuperNet Training INFO: --> epoch:  27/120  avg CE: 4.2068  lr: 0.10562435793600224  
[05/08 07:09:40] SuperNet Training INFO: iter: 32520/144360  CE: 4.2796  
[05/08 07:10:42] SuperNet Training INFO: iter: 32640/144360  CE: 4.4468  
[05/08 07:11:43] SuperNet Training INFO: iter: 32760/144360  CE: 4.3066  
[05/08 07:12:46] SuperNet Training INFO: iter: 32880/144360  CE: 4.1593  
[05/08 07:13:48] SuperNet Training INFO: iter: 33000/144360  CE: 4.1453  
[05/08 07:14:49] SuperNet Training INFO: iter: 33120/144360  CE: 4.1113  
[05/08 07:15:53] SuperNet Training INFO: iter: 33240/144360  CE: 4.5211  
[05/08 07:16:53] SuperNet Training INFO: iter: 33360/144360  CE: 4.3188  
[05/08 07:17:55] SuperNet Training INFO: iter: 33480/144360  CE: 3.9169  
[05/08 07:18:55] SuperNet Training INFO: iter: 33600/144360  CE: 3.9309  
[05/08 07:19:38] SuperNet Training INFO: --> epoch:  28/120  avg CE: 4.1735  lr: 0.10458868952864393  
[05/08 07:20:35] SuperNet Training INFO: iter: 33720/144360  CE: 4.1796  
[05/08 07:21:37] SuperNet Training INFO: iter: 33840/144360  CE: 3.9609  
[05/08 07:22:38] SuperNet Training INFO: iter: 33960/144360  CE: 3.9841  
[05/08 07:23:40] SuperNet Training INFO: iter: 34080/144360  CE: 4.1436  
[05/08 07:24:41] SuperNet Training INFO: iter: 34200/144360  CE: 3.9276  
[05/08 07:25:42] SuperNet Training INFO: iter: 34320/144360  CE: 4.1253  
[05/08 07:26:42] SuperNet Training INFO: iter: 34440/144360  CE: 3.8283  
[05/08 07:27:44] SuperNet Training INFO: iter: 34560/144360  CE: 4.1020  
[05/08 07:28:46] SuperNet Training INFO: iter: 34680/144360  CE: 4.2416  
[05/08 07:29:47] SuperNet Training INFO: iter: 34800/144360  CE: 3.9623  
[05/08 07:30:30] SuperNet Training INFO: --> epoch:  29/120  avg CE: 4.1309  lr: 0.10352246226073762  
[05/08 07:31:26] SuperNet Training INFO: iter: 34920/144360  CE: 4.0629  
[05/08 07:32:27] SuperNet Training INFO: iter: 35040/144360  CE: 4.0623  
[05/08 07:33:28] SuperNet Training INFO: iter: 35160/144360  CE: 4.5100  
[05/08 07:34:28] SuperNet Training INFO: iter: 35280/144360  CE: 4.2249  
[05/08 07:35:29] SuperNet Training INFO: iter: 35400/144360  CE: 4.0569  
[05/08 07:36:31] SuperNet Training INFO: iter: 35520/144360  CE: 4.2760  
[05/08 07:37:32] SuperNet Training INFO: iter: 35640/144360  CE: 4.1238  
[05/08 07:38:34] SuperNet Training INFO: iter: 35760/144360  CE: 4.0753  
[05/08 07:39:36] SuperNet Training INFO: iter: 35880/144360  CE: 4.2074  
[05/08 07:40:38] SuperNet Training INFO: iter: 36000/144360  CE: 3.9950  
[05/08 07:41:24] SuperNet Training INFO: --> epoch:  30/120  avg CE: 4.0979  lr: 0.10242640687119343  
[05/08 07:42:19] SuperNet Training INFO: iter: 36120/144360  CE: 4.1708  
[05/08 07:43:21] SuperNet Training INFO: iter: 36240/144360  CE: 4.2368  
[05/08 07:44:23] SuperNet Training INFO: iter: 36360/144360  CE: 3.8634  
[05/08 07:45:26] SuperNet Training INFO: iter: 36480/144360  CE: 4.2711  
[05/08 07:46:27] SuperNet Training INFO: iter: 36600/144360  CE: 4.2857  
[05/08 07:47:30] SuperNet Training INFO: iter: 36720/144360  CE: 3.8500  
[05/08 07:48:32] SuperNet Training INFO: iter: 36840/144360  CE: 3.8161  
[05/08 07:49:35] SuperNet Training INFO: iter: 36960/144360  CE: 4.0488  
[05/08 07:50:36] SuperNet Training INFO: iter: 37080/144360  CE: 3.9662  
[05/08 07:51:38] SuperNet Training INFO: iter: 37200/144360  CE: 3.9094  
[05/08 07:52:24] SuperNet Training INFO: --> epoch:  31/120  avg CE: 4.0541  lr: 0.10130127454162571  
[05/08 07:53:15] SuperNet Training INFO: iter: 37320/144360  CE: 4.0157  
[05/08 07:54:16] SuperNet Training INFO: iter: 37440/144360  CE: 4.0052  
[05/08 07:55:18] SuperNet Training INFO: iter: 37560/144360  CE: 4.1562  
[05/08 07:56:19] SuperNet Training INFO: iter: 37680/144360  CE: 4.0952  
[05/08 07:57:22] SuperNet Training INFO: iter: 37800/144360  CE: 4.0308  
[05/08 07:58:23] SuperNet Training INFO: iter: 37920/144360  CE: 4.0480  
[05/08 07:59:25] SuperNet Training INFO: iter: 38040/144360  CE: 4.1969  
[05/08 08:00:27] SuperNet Training INFO: iter: 38160/144360  CE: 4.0599  
[05/08 08:01:29] SuperNet Training INFO: iter: 38280/144360  CE: 4.0613  
[05/08 08:02:31] SuperNet Training INFO: iter: 38400/144360  CE: 3.6930  
[05/08 08:03:20] SuperNet Training INFO: --> epoch:  32/120  avg CE: 4.0359  lr: 0.10014783638153192  
[05/08 08:04:11] SuperNet Training INFO: iter: 38520/144360  CE: 3.7242  
[05/08 08:05:12] SuperNet Training INFO: iter: 38640/144360  CE: 4.3274  
[05/08 08:06:13] SuperNet Training INFO: iter: 38760/144360  CE: 4.0171  
[05/08 08:07:15] SuperNet Training INFO: iter: 38880/144360  CE: 4.0926  
[05/08 08:08:18] SuperNet Training INFO: iter: 39000/144360  CE: 4.1006  
[05/08 08:09:19] SuperNet Training INFO: iter: 39120/144360  CE: 3.9032  
[05/08 08:10:22] SuperNet Training INFO: iter: 39240/144360  CE: 4.1070  
[05/08 08:11:23] SuperNet Training INFO: iter: 39360/144360  CE: 4.0687  
[05/08 08:12:23] SuperNet Training INFO: iter: 39480/144360  CE: 3.7925  
[05/08 08:13:23] SuperNet Training INFO: iter: 39600/144360  CE: 4.1786  
[05/08 08:14:11] SuperNet Training INFO: --> epoch:  33/120  avg CE: 3.9994  lr: 0.09896688289981138  
[05/08 08:15:01] SuperNet Training INFO: iter: 39720/144360  CE: 4.1806  
[05/08 08:16:01] SuperNet Training INFO: iter: 39840/144360  CE: 3.7126  
[05/08 08:17:01] SuperNet Training INFO: iter: 39960/144360  CE: 4.0524  
[05/08 08:18:00] SuperNet Training INFO: iter: 40080/144360  CE: 3.7354  
[05/08 08:19:00] SuperNet Training INFO: iter: 40200/144360  CE: 4.3029  
[05/08 08:20:00] SuperNet Training INFO: iter: 40320/144360  CE: 3.9865  
[05/08 08:21:00] SuperNet Training INFO: iter: 40440/144360  CE: 4.1625  
[05/08 08:22:00] SuperNet Training INFO: iter: 40560/144360  CE: 4.0583  
[05/08 08:23:00] SuperNet Training INFO: iter: 40680/144360  CE: 3.9942  
[05/08 08:24:00] SuperNet Training INFO: iter: 40800/144360  CE: 4.2170  
[05/08 08:24:50] SuperNet Training INFO: --> epoch:  34/120  avg CE: 3.9598  lr: 0.09775922346299062  
[05/08 08:25:37] SuperNet Training INFO: iter: 40920/144360  CE: 4.3009  
[05/08 08:26:38] SuperNet Training INFO: iter: 41040/144360  CE: 3.6032  
[05/08 08:27:37] SuperNet Training INFO: iter: 41160/144360  CE: 3.8583  
[05/08 08:28:37] SuperNet Training INFO: iter: 41280/144360  CE: 3.7960  
[05/08 08:29:37] SuperNet Training INFO: iter: 41400/144360  CE: 4.0771  
[05/08 08:30:37] SuperNet Training INFO: iter: 41520/144360  CE: 3.9144  
[05/08 08:31:36] SuperNet Training INFO: iter: 41640/144360  CE: 4.0056  
[05/08 08:32:35] SuperNet Training INFO: iter: 41760/144360  CE: 3.7753  
[05/08 08:33:34] SuperNet Training INFO: iter: 41880/144360  CE: 4.0071  
[05/08 08:34:33] SuperNet Training INFO: iter: 42000/144360  CE: 4.2973  
[05/08 08:35:24] SuperNet Training INFO: --> epoch:  35/120  avg CE: 3.9383  lr: 0.09652568574052359  
[05/08 08:36:10] SuperNet Training INFO: iter: 42120/144360  CE: 3.8160  
[05/08 08:37:10] SuperNet Training INFO: iter: 42240/144360  CE: 4.0531  
[05/08 08:38:11] SuperNet Training INFO: iter: 42360/144360  CE: 4.2661  
[05/08 08:39:12] SuperNet Training INFO: iter: 42480/144360  CE: 4.0951  
[05/08 08:40:12] SuperNet Training INFO: iter: 42600/144360  CE: 3.8951  
[05/08 08:41:13] SuperNet Training INFO: iter: 42720/144360  CE: 3.8843  
[05/08 08:42:12] SuperNet Training INFO: iter: 42840/144360  CE: 3.9295  
[05/08 08:43:13] SuperNet Training INFO: iter: 42960/144360  CE: 3.8111  
[05/08 08:44:13] SuperNet Training INFO: iter: 43080/144360  CE: 3.6893  
[05/08 08:45:14] SuperNet Training INFO: iter: 43200/144360  CE: 3.9091  
[05/08 08:46:08] SuperNet Training INFO: --> epoch:  36/120  avg CE: 3.9146  lr: 0.09526711513754865  
[05/08 08:46:53] SuperNet Training INFO: iter: 43320/144360  CE: 3.9809  
[05/08 08:47:54] SuperNet Training INFO: iter: 43440/144360  CE: 3.8120  
[05/08 08:48:53] SuperNet Training INFO: iter: 43560/144360  CE: 3.7771  
[05/08 08:49:53] SuperNet Training INFO: iter: 43680/144360  CE: 4.2008  
[05/08 08:50:53] SuperNet Training INFO: iter: 43800/144360  CE: 3.6748  
[05/08 08:51:53] SuperNet Training INFO: iter: 43920/144360  CE: 3.8556  
[05/08 08:52:52] SuperNet Training INFO: iter: 44040/144360  CE: 3.7184  
[05/08 08:53:52] SuperNet Training INFO: iter: 44160/144360  CE: 3.7523  
[05/08 08:54:52] SuperNet Training INFO: iter: 44280/144360  CE: 3.8838  
[05/08 08:55:53] SuperNet Training INFO: iter: 44400/144360  CE: 4.1244  
[05/08 08:56:48] SuperNet Training INFO: --> epoch:  37/120  avg CE: 3.8762  lr: 0.0939843742154901  
[05/08 08:57:31] SuperNet Training INFO: iter: 44520/144360  CE: 4.1062  
[05/08 08:58:31] SuperNet Training INFO: iter: 44640/144360  CE: 3.6301  
[05/08 08:59:30] SuperNet Training INFO: iter: 44760/144360  CE: 3.9785  
[05/08 09:00:30] SuperNet Training INFO: iter: 44880/144360  CE: 3.9484  
[05/08 09:01:31] SuperNet Training INFO: iter: 45000/144360  CE: 3.8924  
[05/08 09:02:30] SuperNet Training INFO: iter: 45120/144360  CE: 4.0787  
[05/08 09:03:30] SuperNet Training INFO: iter: 45240/144360  CE: 3.9714  
[05/08 09:04:30] SuperNet Training INFO: iter: 45360/144360  CE: 3.8369  
[05/08 09:05:31] SuperNet Training INFO: iter: 45480/144360  CE: 3.7532  
[05/08 09:06:30] SuperNet Training INFO: iter: 45600/144360  CE: 4.0162  
[05/08 09:07:27] SuperNet Training INFO: --> epoch:  38/120  avg CE: 3.8579  lr: 0.0926783421009017  
[05/08 09:08:10] SuperNet Training INFO: iter: 45720/144360  CE: 3.7028  
[05/08 09:09:09] SuperNet Training INFO: iter: 45840/144360  CE: 3.8514  
[05/08 09:10:09] SuperNet Training INFO: iter: 45960/144360  CE: 3.6072  
[05/08 09:11:09] SuperNet Training INFO: iter: 46080/144360  CE: 3.7488  
[05/08 09:12:09] SuperNet Training INFO: iter: 46200/144360  CE: 4.0117  
[05/08 09:13:09] SuperNet Training INFO: iter: 46320/144360  CE: 3.6032  
[05/08 09:14:09] SuperNet Training INFO: iter: 46440/144360  CE: 3.5829  
[05/08 09:15:10] SuperNet Training INFO: iter: 46560/144360  CE: 4.0116  
[05/08 09:16:10] SuperNet Training INFO: iter: 46680/144360  CE: 3.8117  
[05/08 09:17:09] SuperNet Training INFO: iter: 46800/144360  CE: 3.8208  
[05/08 09:18:06] SuperNet Training INFO: --> epoch:  39/120  avg CE: 3.8433  lr: 0.09134991388295689  
[05/08 09:18:46] SuperNet Training INFO: iter: 46920/144360  CE: 3.7130  
[05/08 09:19:46] SuperNet Training INFO: iter: 47040/144360  CE: 3.7542  
[05/08 09:20:45] SuperNet Training INFO: iter: 47160/144360  CE: 4.2117  
[05/08 09:21:45] SuperNet Training INFO: iter: 47280/144360  CE: 3.7871  
[05/08 09:22:46] SuperNet Training INFO: iter: 47400/144360  CE: 3.8663  
[05/08 09:23:46] SuperNet Training INFO: iter: 47520/144360  CE: 3.8645  
[05/08 09:24:46] SuperNet Training INFO: iter: 47640/144360  CE: 3.8468  
[05/08 09:25:46] SuperNet Training INFO: iter: 47760/144360  CE: 3.4416  
[05/08 09:26:46] SuperNet Training INFO: iter: 47880/144360  CE: 3.4997  
[05/08 09:27:47] SuperNet Training INFO: iter: 48000/144360  CE: 3.8049  
[05/08 09:28:46] SuperNet Training INFO: iter: 48120/144360  CE: 4.0276  
[05/08 09:28:46] SuperNet Training INFO: --> epoch:  40/120  avg CE: 3.8094  lr: 0.0899999999999998  
[05/08 09:30:24] SuperNet Training INFO: iter: 48240/144360  CE: 3.6091  
[05/08 09:31:23] SuperNet Training INFO: iter: 48360/144360  CE: 3.7863  
[05/08 09:32:24] SuperNet Training INFO: iter: 48480/144360  CE: 3.5077  
[05/08 09:33:24] SuperNet Training INFO: iter: 48600/144360  CE: 3.5528  
[05/08 09:34:24] SuperNet Training INFO: iter: 48720/144360  CE: 4.0115  
[05/08 09:35:24] SuperNet Training INFO: iter: 48840/144360  CE: 3.8027  
[05/08 09:36:24] SuperNet Training INFO: iter: 48960/144360  CE: 3.6398  
[05/08 09:37:23] SuperNet Training INFO: iter: 49080/144360  CE: 3.9986  
[05/08 09:38:23] SuperNet Training INFO: iter: 49200/144360  CE: 3.5559  
[05/08 09:39:22] SuperNet Training INFO: iter: 49320/144360  CE: 3.7455  
[05/08 09:39:23] SuperNet Training INFO: --> epoch:  41/120  avg CE: 3.7841  lr: 0.08862952561557644  
[05/08 09:41:00] SuperNet Training INFO: iter: 49440/144360  CE: 3.5084  
[05/08 09:42:00] SuperNet Training INFO: iter: 49560/144360  CE: 3.4642  
[05/08 09:42:59] SuperNet Training INFO: iter: 49680/144360  CE: 3.8522  
[05/08 09:44:01] SuperNet Training INFO: iter: 49800/144360  CE: 3.6588  
[05/08 09:45:01] SuperNet Training INFO: iter: 49920/144360  CE: 3.7654  
[05/08 09:46:01] SuperNet Training INFO: iter: 50040/144360  CE: 3.7540  
[05/08 09:47:01] SuperNet Training INFO: iter: 50160/144360  CE: 3.4449  
[05/08 09:48:01] SuperNet Training INFO: iter: 50280/144360  CE: 3.5013  
[05/08 09:49:02] SuperNet Training INFO: iter: 50400/144360  CE: 4.0697  
[05/08 09:50:02] SuperNet Training INFO: iter: 50520/144360  CE: 3.8321  
[05/08 09:50:04] SuperNet Training INFO: --> epoch:  42/120  avg CE: 3.7538  lr: 0.08723942998437267  
[05/08 09:51:39] SuperNet Training INFO: iter: 50640/144360  CE: 3.6790  
[05/08 09:52:39] SuperNet Training INFO: iter: 50760/144360  CE: 3.8039  
[05/08 09:53:40] SuperNet Training INFO: iter: 50880/144360  CE: 3.8101  
[05/08 09:54:40] SuperNet Training INFO: iter: 51000/144360  CE: 3.9436  
[05/08 09:55:40] SuperNet Training INFO: iter: 51120/144360  CE: 3.8548  
[05/08 09:56:40] SuperNet Training INFO: iter: 51240/144360  CE: 3.6692  
[05/08 09:57:41] SuperNet Training INFO: iter: 51360/144360  CE: 3.2703  
[05/08 09:58:42] SuperNet Training INFO: iter: 51480/144360  CE: 3.8212  
[05/08 09:59:42] SuperNet Training INFO: iter: 51600/144360  CE: 3.3902  
[05/08 10:00:43] SuperNet Training INFO: iter: 51720/144360  CE: 3.6598  
[05/08 10:00:46] SuperNet Training INFO: --> epoch:  43/120  avg CE: 3.7402  lr: 0.08583066580849745  
[05/08 10:02:20] SuperNet Training INFO: iter: 51840/144360  CE: 4.0024  
[05/08 10:03:19] SuperNet Training INFO: iter: 51960/144360  CE: 3.5565  
[05/08 10:04:19] SuperNet Training INFO: iter: 52080/144360  CE: 3.3890  
[05/08 10:05:19] SuperNet Training INFO: iter: 52200/144360  CE: 3.6364  
[05/08 10:06:20] SuperNet Training INFO: iter: 52320/144360  CE: 3.9383  
[05/08 10:07:19] SuperNet Training INFO: iter: 52440/144360  CE: 3.7153  
[05/08 10:08:19] SuperNet Training INFO: iter: 52560/144360  CE: 3.7707  
[05/08 10:09:19] SuperNet Training INFO: iter: 52680/144360  CE: 3.7096  
[05/08 10:10:18] SuperNet Training INFO: iter: 52800/144360  CE: 3.9012  
[05/08 10:11:17] SuperNet Training INFO: iter: 52920/144360  CE: 3.7060  
[05/08 10:11:22] SuperNet Training INFO: --> epoch:  44/120  avg CE: 3.7117  lr: 0.08440419858454766  
[05/08 10:12:54] SuperNet Training INFO: iter: 53040/144360  CE: 3.8341  
[05/08 10:13:55] SuperNet Training INFO: iter: 53160/144360  CE: 4.0284  
[05/08 10:14:55] SuperNet Training INFO: iter: 53280/144360  CE: 3.9822  
[05/08 10:15:54] SuperNet Training INFO: iter: 53400/144360  CE: 3.7751  
[05/08 10:16:55] SuperNet Training INFO: iter: 53520/144360  CE: 3.4719  
[05/08 10:17:55] SuperNet Training INFO: iter: 53640/144360  CE: 3.7069  
[05/08 10:18:54] SuperNet Training INFO: iter: 53760/144360  CE: 3.5499  
[05/08 10:19:54] SuperNet Training INFO: iter: 53880/144360  CE: 3.6202  
[05/08 10:20:54] SuperNet Training INFO: iter: 54000/144360  CE: 3.1990  
[05/08 10:21:54] SuperNet Training INFO: iter: 54120/144360  CE: 3.6375  
[05/08 10:22:01] SuperNet Training INFO: --> epoch:  45/120  avg CE: 3.6962  lr: 0.0829610059419049  
[05/08 10:23:30] SuperNet Training INFO: iter: 54240/144360  CE: 3.5632  
[05/08 10:24:30] SuperNet Training INFO: iter: 54360/144360  CE: 3.6967  
[05/08 10:25:30] SuperNet Training INFO: iter: 54480/144360  CE: 3.4102  
[05/08 10:26:30] SuperNet Training INFO: iter: 54600/144360  CE: 3.7833  
[05/08 10:27:30] SuperNet Training INFO: iter: 54720/144360  CE: 3.3413  
[05/08 10:28:30] SuperNet Training INFO: iter: 54840/144360  CE: 3.6923  
[05/08 10:29:31] SuperNet Training INFO: iter: 54960/144360  CE: 3.5877  
[05/08 10:30:32] SuperNet Training INFO: iter: 55080/144360  CE: 3.7001  
[05/08 10:31:31] SuperNet Training INFO: iter: 55200/144360  CE: 3.6895  
[05/08 10:32:32] SuperNet Training INFO: iter: 55320/144360  CE: 3.3569  
[05/08 10:32:40] SuperNet Training INFO: --> epoch:  46/120  avg CE: 3.6806  lr: 0.08150207697271764  
[05/08 10:34:09] SuperNet Training INFO: iter: 55440/144360  CE: 3.7071  
[05/08 10:35:09] SuperNet Training INFO: iter: 55560/144360  CE: 3.6557  
[05/08 10:36:09] SuperNet Training INFO: iter: 55680/144360  CE: 3.6446  
[05/08 10:37:09] SuperNet Training INFO: iter: 55800/144360  CE: 3.6756  
[05/08 10:38:10] SuperNet Training INFO: iter: 55920/144360  CE: 3.8867  
[05/08 10:39:11] SuperNet Training INFO: iter: 56040/144360  CE: 3.7999  
[05/08 10:40:12] SuperNet Training INFO: iter: 56160/144360  CE: 3.5774  
[05/08 10:41:11] SuperNet Training INFO: iter: 56280/144360  CE: 3.2013  
[05/08 10:42:12] SuperNet Training INFO: iter: 56400/144360  CE: 3.1988  
[05/08 10:43:11] SuperNet Training INFO: iter: 56520/144360  CE: 3.4296  
[05/08 10:43:20] SuperNet Training INFO: --> epoch:  47/120  avg CE: 3.6507  lr: 0.08002841155402596  
[05/08 10:44:47] SuperNet Training INFO: iter: 56640/144360  CE: 3.5154  
[05/08 10:45:47] SuperNet Training INFO: iter: 56760/144360  CE: 3.4531  
[05/08 10:46:47] SuperNet Training INFO: iter: 56880/144360  CE: 3.7347  
[05/08 10:47:47] SuperNet Training INFO: iter: 57000/144360  CE: 3.6937  
[05/08 10:48:46] SuperNet Training INFO: iter: 57120/144360  CE: 3.9620  
[05/08 10:49:46] SuperNet Training INFO: iter: 57240/144360  CE: 3.6881  
[05/08 10:50:45] SuperNet Training INFO: iter: 57360/144360  CE: 3.7571  
[05/08 10:51:45] SuperNet Training INFO: iter: 57480/144360  CE: 3.5944  
[05/08 10:52:45] SuperNet Training INFO: iter: 57600/144360  CE: 3.9418  
[05/08 10:53:44] SuperNet Training INFO: iter: 57720/144360  CE: 3.7160  
[05/08 10:53:55] SuperNet Training INFO: --> epoch:  48/120  avg CE: 3.6382  lr: 0.07854101966249659  
[05/08 10:55:21] SuperNet Training INFO: iter: 57840/144360  CE: 3.6469  
[05/08 10:56:22] SuperNet Training INFO: iter: 57960/144360  CE: 3.6948  
[05/08 10:57:22] SuperNet Training INFO: iter: 58080/144360  CE: 3.6395  
[05/08 10:58:22] SuperNet Training INFO: iter: 58200/144360  CE: 3.5055  
[05/08 10:59:21] SuperNet Training INFO: iter: 58320/144360  CE: 3.4593  
[05/08 11:00:21] SuperNet Training INFO: iter: 58440/144360  CE: 3.5223  
[05/08 11:01:21] SuperNet Training INFO: iter: 58560/144360  CE: 3.8212  
[05/08 11:02:22] SuperNet Training INFO: iter: 58680/144360  CE: 3.8298  
[05/08 11:03:21] SuperNet Training INFO: iter: 58800/144360  CE: 3.2902  
[05/08 11:04:21] SuperNet Training INFO: iter: 58920/144360  CE: 3.8549  
[05/08 11:04:34] SuperNet Training INFO: --> epoch:  49/120  avg CE: 3.6265  lr: 0.07704092068223518  
[05/08 11:05:59] SuperNet Training INFO: iter: 59040/144360  CE: 3.6541  
[05/08 11:06:58] SuperNet Training INFO: iter: 59160/144360  CE: 3.3445  
[05/08 11:07:59] SuperNet Training INFO: iter: 59280/144360  CE: 3.5671  
[05/08 11:09:00] SuperNet Training INFO: iter: 59400/144360  CE: 3.3496  
[05/08 11:10:00] SuperNet Training INFO: iter: 59520/144360  CE: 3.6934  
[05/08 11:10:59] SuperNet Training INFO: iter: 59640/144360  CE: 3.6297  
[05/08 11:11:58] SuperNet Training INFO: iter: 59760/144360  CE: 3.4829  
[05/08 11:12:57] SuperNet Training INFO: iter: 59880/144360  CE: 3.5809  
[05/08 11:13:57] SuperNet Training INFO: iter: 60000/144360  CE: 3.6755  
[05/08 11:14:56] SuperNet Training INFO: iter: 60120/144360  CE: 3.2967  
[05/08 11:15:10] SuperNet Training INFO: --> epoch:  50/120  avg CE: 3.5985  lr: 0.07552914270615126  
[05/08 11:16:33] SuperNet Training INFO: iter: 60240/144360  CE: 3.4242  
[05/08 11:17:33] SuperNet Training INFO: iter: 60360/144360  CE: 3.6189  
[05/08 11:18:33] SuperNet Training INFO: iter: 60480/144360  CE: 3.7124  
[05/08 11:19:33] SuperNet Training INFO: iter: 60600/144360  CE: 3.2835  
[05/08 11:20:32] SuperNet Training INFO: iter: 60720/144360  CE: 3.8106  
[05/08 11:21:32] SuperNet Training INFO: iter: 60840/144360  CE: 3.6216  
[05/08 11:22:32] SuperNet Training INFO: iter: 60960/144360  CE: 3.6446  
[05/08 11:23:31] SuperNet Training INFO: iter: 61080/144360  CE: 3.6024  
[05/08 11:24:31] SuperNet Training INFO: iter: 61200/144360  CE: 3.3537  
[05/08 11:25:31] SuperNet Training INFO: iter: 61320/144360  CE: 3.6632  
[05/08 11:25:46] SuperNet Training INFO: --> epoch:  51/120  avg CE: 3.5935  lr: 0.0740067218313545  
[05/08 11:27:08] SuperNet Training INFO: iter: 61440/144360  CE: 3.5210  
[05/08 11:28:08] SuperNet Training INFO: iter: 61560/144360  CE: 3.3546  
[05/08 11:29:08] SuperNet Training INFO: iter: 61680/144360  CE: 3.4994  
[05/08 11:30:09] SuperNet Training INFO: iter: 61800/144360  CE: 3.5600  
[05/08 11:31:09] SuperNet Training INFO: iter: 61920/144360  CE: 3.6306  
[05/08 11:32:09] SuperNet Training INFO: iter: 62040/144360  CE: 3.6151  
[05/08 11:33:09] SuperNet Training INFO: iter: 62160/144360  CE: 3.3876  
[05/08 11:34:07] SuperNet Training INFO: iter: 62280/144360  CE: 3.1352  
[05/08 11:35:07] SuperNet Training INFO: iter: 62400/144360  CE: 3.8019  
[05/08 11:36:07] SuperNet Training INFO: iter: 62520/144360  CE: 3.6154  
[05/08 11:36:23] SuperNet Training INFO: --> epoch:  52/120  avg CE: 3.5668  lr: 0.07247470144906537  
[05/08 11:37:43] SuperNet Training INFO: iter: 62640/144360  CE: 3.6690  
[05/08 11:38:42] SuperNet Training INFO: iter: 62760/144360  CE: 3.4566  
[05/08 11:39:43] SuperNet Training INFO: iter: 62880/144360  CE: 3.5424  
[05/08 11:40:42] SuperNet Training INFO: iter: 63000/144360  CE: 3.6077  
[05/08 11:41:42] SuperNet Training INFO: iter: 63120/144360  CE: 3.4147  
[05/08 11:42:41] SuperNet Training INFO: iter: 63240/144360  CE: 3.3014  
[05/08 11:43:41] SuperNet Training INFO: iter: 63360/144360  CE: 3.4891  
[05/08 11:44:40] SuperNet Training INFO: iter: 63480/144360  CE: 3.4234  
[05/08 11:45:40] SuperNet Training INFO: iter: 63600/144360  CE: 3.7115  
[05/08 11:46:40] SuperNet Training INFO: iter: 63720/144360  CE: 3.2801  
[05/08 11:46:59] SuperNet Training INFO: --> epoch:  53/120  avg CE: 3.5468  lr: 0.07093413152952865  
[05/08 11:48:17] SuperNet Training INFO: iter: 63840/144360  CE: 3.8250  
[05/08 11:49:17] SuperNet Training INFO: iter: 63960/144360  CE: 3.7872  
[05/08 11:50:17] SuperNet Training INFO: iter: 64080/144360  CE: 3.4528  
[05/08 11:51:17] SuperNet Training INFO: iter: 64200/144360  CE: 3.6660  
[05/08 11:52:18] SuperNet Training INFO: iter: 64320/144360  CE: 3.4689  
[05/08 11:53:19] SuperNet Training INFO: iter: 64440/144360  CE: 3.7649  
[05/08 11:54:18] SuperNet Training INFO: iter: 64560/144360  CE: 3.6676  
[05/08 11:55:18] SuperNet Training INFO: iter: 64680/144360  CE: 3.7244  
[05/08 11:56:18] SuperNet Training INFO: iter: 64800/144360  CE: 3.4704  
[05/08 11:57:17] SuperNet Training INFO: iter: 64920/144360  CE: 3.2851  
[05/08 11:57:37] SuperNet Training INFO: --> epoch:  54/120  avg CE: 3.5442  lr: 0.06938606790241343  
[05/08 11:58:54] SuperNet Training INFO: iter: 65040/144360  CE: 4.1227  
[05/08 11:59:54] SuperNet Training INFO: iter: 65160/144360  CE: 3.5101  
[05/08 12:00:54] SuperNet Training INFO: iter: 65280/144360  CE: 3.6221  
[05/08 12:01:54] SuperNet Training INFO: iter: 65400/144360  CE: 3.3648  
[05/08 12:02:55] SuperNet Training INFO: iter: 65520/144360  CE: 3.6181  
[05/08 12:03:54] SuperNet Training INFO: iter: 65640/144360  CE: 3.3147  
[05/08 12:04:54] SuperNet Training INFO: iter: 65760/144360  CE: 3.2422  
[05/08 12:05:53] SuperNet Training INFO: iter: 65880/144360  CE: 3.4046  
[05/08 12:06:52] SuperNet Training INFO: iter: 66000/144360  CE: 3.3821  
[05/08 12:07:51] SuperNet Training INFO: iter: 66120/144360  CE: 3.5153  
[05/08 12:08:12] SuperNet Training INFO: --> epoch:  55/120  avg CE: 3.5193  lr: 0.06783157153320266  
[05/08 12:09:26] SuperNet Training INFO: iter: 66240/144360  CE: 3.2500  
[05/08 12:10:26] SuperNet Training INFO: iter: 66360/144360  CE: 3.2899  
[05/08 12:11:25] SuperNet Training INFO: iter: 66480/144360  CE: 3.3451  
[05/08 12:12:25] SuperNet Training INFO: iter: 66600/144360  CE: 3.6444  
[05/08 12:13:25] SuperNet Training INFO: iter: 66720/144360  CE: 3.3724  
[05/08 12:14:26] SuperNet Training INFO: iter: 66840/144360  CE: 3.2925  
[05/08 12:15:26] SuperNet Training INFO: iter: 66960/144360  CE: 3.3431  
[05/08 12:16:26] SuperNet Training INFO: iter: 67080/144360  CE: 3.7356  
[05/08 12:17:25] SuperNet Training INFO: iter: 67200/144360  CE: 3.6114  
[05/08 12:18:25] SuperNet Training INFO: iter: 67320/144360  CE: 3.5625  
[05/08 12:18:47] SuperNet Training INFO: --> epoch:  56/120  avg CE: 3.5045  lr: 0.06627170779605904  
[05/08 12:20:00] SuperNet Training INFO: iter: 67440/144360  CE: 3.8020  
[05/08 12:21:00] SuperNet Training INFO: iter: 67560/144360  CE: 3.5097  
[05/08 12:22:00] SuperNet Training INFO: iter: 67680/144360  CE: 3.2490  
[05/08 12:23:00] SuperNet Training INFO: iter: 67800/144360  CE: 3.7408  
[05/08 12:24:00] SuperNet Training INFO: iter: 67920/144360  CE: 3.4415  
[05/08 12:25:00] SuperNet Training INFO: iter: 68040/144360  CE: 3.6092  
[05/08 12:26:00] SuperNet Training INFO: iter: 68160/144360  CE: 3.2511  
[05/08 12:27:00] SuperNet Training INFO: iter: 68280/144360  CE: 3.0730  
[05/08 12:28:00] SuperNet Training INFO: iter: 68400/144360  CE: 3.3773  
[05/08 12:29:01] SuperNet Training INFO: iter: 68520/144360  CE: 3.4656  
[05/08 12:29:26] SuperNet Training INFO: --> epoch:  57/120  avg CE: 3.4887  lr: 0.06470754574367053  
[05/08 12:30:39] SuperNet Training INFO: iter: 68640/144360  CE: 3.8268  
[05/08 12:31:38] SuperNet Training INFO: iter: 68760/144360  CE: 3.5479  
[05/08 12:32:39] SuperNet Training INFO: iter: 68880/144360  CE: 3.3713  
[05/08 12:33:39] SuperNet Training INFO: iter: 69000/144360  CE: 3.6136  
[05/08 12:34:38] SuperNet Training INFO: iter: 69120/144360  CE: 3.2107  
[05/08 12:35:38] SuperNet Training INFO: iter: 69240/144360  CE: 3.5586  
[05/08 12:36:39] SuperNet Training INFO: iter: 69360/144360  CE: 3.8609  
[05/08 12:37:39] SuperNet Training INFO: iter: 69480/144360  CE: 3.3161  
[05/08 12:38:39] SuperNet Training INFO: iter: 69600/144360  CE: 3.5122  
[05/08 12:39:39] SuperNet Training INFO: iter: 69720/144360  CE: 3.3011  
[05/08 12:40:05] SuperNet Training INFO: --> epoch:  58/120  avg CE: 3.4710  lr: 0.06314015737457618  
[05/08 12:41:15] SuperNet Training INFO: iter: 69840/144360  CE: 3.2574  
[05/08 12:42:14] SuperNet Training INFO: iter: 69960/144360  CE: 3.5261  
[05/08 12:43:14] SuperNet Training INFO: iter: 70080/144360  CE: 3.5284  
[05/08 12:44:14] SuperNet Training INFO: iter: 70200/144360  CE: 3.2458  
[05/08 12:45:15] SuperNet Training INFO: iter: 70320/144360  CE: 3.7250  
[05/08 12:46:15] SuperNet Training INFO: iter: 70440/144360  CE: 3.3251  
[05/08 12:47:14] SuperNet Training INFO: iter: 70560/144360  CE: 3.7346  
[05/08 12:48:15] SuperNet Training INFO: iter: 70680/144360  CE: 3.4437  
[05/08 12:49:14] SuperNet Training INFO: iter: 70800/144360  CE: 3.5395  
[05/08 12:50:14] SuperNet Training INFO: iter: 70920/144360  CE: 3.1622  
[05/08 12:50:42] SuperNet Training INFO: --> epoch:  59/120  avg CE: 3.4593  lr: 0.061570616898472125  
[05/08 12:51:51] SuperNet Training INFO: iter: 71040/144360  CE: 3.1595  
[05/08 12:52:51] SuperNet Training INFO: iter: 71160/144360  CE: 3.3644  
[05/08 12:53:51] SuperNet Training INFO: iter: 71280/144360  CE: 3.3484  
[05/08 12:54:51] SuperNet Training INFO: iter: 71400/144360  CE: 3.5564  
[05/08 12:55:51] SuperNet Training INFO: iter: 71520/144360  CE: 3.4881  
[05/08 12:56:52] SuperNet Training INFO: iter: 71640/144360  CE: 3.2440  
[05/08 12:57:52] SuperNet Training INFO: iter: 71760/144360  CE: 3.5591  
[05/08 12:58:51] SuperNet Training INFO: iter: 71880/144360  CE: 3.5370  
[05/08 12:59:50] SuperNet Training INFO: iter: 72000/144360  CE: 3.2804  
[05/08 13:00:50] SuperNet Training INFO: iter: 72120/144360  CE: 3.1991  
[05/08 13:01:19] SuperNet Training INFO: --> epoch:  60/120  avg CE: 3.4464  lr: 0.05999999999999976  
[05/08 13:02:27] SuperNet Training INFO: iter: 72240/144360  CE: 3.2633  
[05/08 13:03:27] SuperNet Training INFO: iter: 72360/144360  CE: 3.2620  
[05/08 13:04:27] SuperNet Training INFO: iter: 72480/144360  CE: 3.2861  
[05/08 13:05:28] SuperNet Training INFO: iter: 72600/144360  CE: 3.6037  
[05/08 13:06:29] SuperNet Training INFO: iter: 72720/144360  CE: 2.9874  
[05/08 13:07:29] SuperNet Training INFO: iter: 72840/144360  CE: 3.2848  
[05/08 13:08:29] SuperNet Training INFO: iter: 72960/144360  CE: 3.6691  
[05/08 13:09:29] SuperNet Training INFO: iter: 73080/144360  CE: 3.3881  
[05/08 13:10:29] SuperNet Training INFO: iter: 73200/144360  CE: 3.5022  
[05/08 13:11:29] SuperNet Training INFO: iter: 73320/144360  CE: 3.7900  
[05/08 13:11:59] SuperNet Training INFO: --> epoch:  61/120  avg CE: 3.4297  lr: 0.058429383101527455  
[05/08 13:13:05] SuperNet Training INFO: iter: 73440/144360  CE: 3.4213  
[05/08 13:14:05] SuperNet Training INFO: iter: 73560/144360  CE: 3.1404  
[05/08 13:15:05] SuperNet Training INFO: iter: 73680/144360  CE: 3.2429  
[05/08 13:16:04] SuperNet Training INFO: iter: 73800/144360  CE: 3.4262  
[05/08 13:17:04] SuperNet Training INFO: iter: 73920/144360  CE: 3.5444  
[05/08 13:18:03] SuperNet Training INFO: iter: 74040/144360  CE: 3.6106  
[05/08 13:19:03] SuperNet Training INFO: iter: 74160/144360  CE: 3.3871  
[05/08 13:20:03] SuperNet Training INFO: iter: 74280/144360  CE: 3.6183  
[05/08 13:21:02] SuperNet Training INFO: iter: 74400/144360  CE: 3.2311  
[05/08 13:22:02] SuperNet Training INFO: iter: 74520/144360  CE: 2.9986  
[05/08 13:22:34] SuperNet Training INFO: --> epoch:  62/120  avg CE: 3.4170  lr: 0.05685984262542327  
[05/08 13:23:38] SuperNet Training INFO: iter: 74640/144360  CE: 3.7939  
[05/08 13:24:38] SuperNet Training INFO: iter: 74760/144360  CE: 3.2645  
[05/08 13:25:38] SuperNet Training INFO: iter: 74880/144360  CE: 3.7902  
[05/08 13:26:39] SuperNet Training INFO: iter: 75000/144360  CE: 3.7144  
[05/08 13:27:39] SuperNet Training INFO: iter: 75120/144360  CE: 3.4669  
[05/08 13:28:39] SuperNet Training INFO: iter: 75240/144360  CE: 3.2846  
[05/08 13:29:40] SuperNet Training INFO: iter: 75360/144360  CE: 3.1090  
[05/08 13:30:39] SuperNet Training INFO: iter: 75480/144360  CE: 3.6748  
[05/08 13:31:39] SuperNet Training INFO: iter: 75600/144360  CE: 3.4136  
[05/08 13:32:39] SuperNet Training INFO: iter: 75720/144360  CE: 3.2099  
[05/08 13:33:12] SuperNet Training INFO: --> epoch:  63/120  avg CE: 3.4059  lr: 0.05529245425632924  
[05/08 13:34:14] SuperNet Training INFO: iter: 75840/144360  CE: 3.2390  
[05/08 13:35:14] SuperNet Training INFO: iter: 75960/144360  CE: 3.4975  
[05/08 13:36:14] SuperNet Training INFO: iter: 76080/144360  CE: 3.4957  
[05/08 13:37:14] SuperNet Training INFO: iter: 76200/144360  CE: 3.3163  
[05/08 13:38:14] SuperNet Training INFO: iter: 76320/144360  CE: 3.3847  
[05/08 13:39:14] SuperNet Training INFO: iter: 76440/144360  CE: 3.4760  
[05/08 13:40:13] SuperNet Training INFO: iter: 76560/144360  CE: 3.4667  
[05/08 13:41:12] SuperNet Training INFO: iter: 76680/144360  CE: 3.3575  
[05/08 13:42:13] SuperNet Training INFO: iter: 76800/144360  CE: 3.3558  
[05/08 13:43:12] SuperNet Training INFO: iter: 76920/144360  CE: 3.2563  
[05/08 13:43:48] SuperNet Training INFO: --> epoch:  64/120  avg CE: 3.3876  lr: 0.05372829220394078  
[05/08 13:44:49] SuperNet Training INFO: iter: 77040/144360  CE: 3.3665  
[05/08 13:45:48] SuperNet Training INFO: iter: 77160/144360  CE: 3.4312  
[05/08 13:46:48] SuperNet Training INFO: iter: 77280/144360  CE: 3.3161  
[05/08 13:47:47] SuperNet Training INFO: iter: 77400/144360  CE: 3.4266  
[05/08 13:48:48] SuperNet Training INFO: iter: 77520/144360  CE: 3.7283  
[05/08 13:49:47] SuperNet Training INFO: iter: 77640/144360  CE: 3.4897  
[05/08 13:50:48] SuperNet Training INFO: iter: 77760/144360  CE: 3.2070  
[05/08 13:51:47] SuperNet Training INFO: iter: 77880/144360  CE: 3.5345  
[05/08 13:52:47] SuperNet Training INFO: iter: 78000/144360  CE: 3.7455  
[05/08 13:53:47] SuperNet Training INFO: iter: 78120/144360  CE: 3.6310  
[05/08 13:54:23] SuperNet Training INFO: --> epoch:  65/120  avg CE: 3.3709  lr: 0.05216842846679694  
[05/08 13:55:23] SuperNet Training INFO: iter: 78240/144360  CE: 3.4468  
[05/08 13:56:23] SuperNet Training INFO: iter: 78360/144360  CE: 3.2832  
[05/08 13:57:22] SuperNet Training INFO: iter: 78480/144360  CE: 3.1503  
[05/08 13:58:22] SuperNet Training INFO: iter: 78600/144360  CE: 3.2474  
[05/08 13:59:22] SuperNet Training INFO: iter: 78720/144360  CE: 3.1569  
[05/08 14:00:22] SuperNet Training INFO: iter: 78840/144360  CE: 3.4852  
[05/08 14:01:21] SuperNet Training INFO: iter: 78960/144360  CE: 3.4759  
[05/08 14:02:21] SuperNet Training INFO: iter: 79080/144360  CE: 3.3013  
[05/08 14:03:20] SuperNet Training INFO: iter: 79200/144360  CE: 3.4057  
[05/08 14:04:20] SuperNet Training INFO: iter: 79320/144360  CE: 3.6018  
[05/08 14:04:58] SuperNet Training INFO: --> epoch:  66/120  avg CE: 3.3574  lr: 0.05061393209758616  
[05/08 14:05:57] SuperNet Training INFO: iter: 79440/144360  CE: 3.6253  
[05/08 14:06:56] SuperNet Training INFO: iter: 79560/144360  CE: 3.2229  
[05/08 14:07:56] SuperNet Training INFO: iter: 79680/144360  CE: 3.4001  
[05/08 14:08:56] SuperNet Training INFO: iter: 79800/144360  CE: 3.3570  
[05/08 14:09:56] SuperNet Training INFO: iter: 79920/144360  CE: 3.3218  
[05/08 14:10:56] SuperNet Training INFO: iter: 80040/144360  CE: 3.4124  
[05/08 14:11:55] SuperNet Training INFO: iter: 80160/144360  CE: 3.2623  
[05/08 14:12:54] SuperNet Training INFO: iter: 80280/144360  CE: 3.1863  
[05/08 14:13:54] SuperNet Training INFO: iter: 80400/144360  CE: 3.4371  
[05/08 14:14:53] SuperNet Training INFO: iter: 80520/144360  CE: 3.4518  
[05/08 14:15:32] SuperNet Training INFO: --> epoch:  67/120  avg CE: 3.3454  lr: 0.04906586847047105  
[05/08 14:16:30] SuperNet Training INFO: iter: 80640/144360  CE: 3.0724  
[05/08 14:17:30] SuperNet Training INFO: iter: 80760/144360  CE: 3.3315  
[05/08 14:18:30] SuperNet Training INFO: iter: 80880/144360  CE: 3.2711  
[05/08 14:19:30] SuperNet Training INFO: iter: 81000/144360  CE: 3.4583  
[05/08 14:20:29] SuperNet Training INFO: iter: 81120/144360  CE: 3.2204  
[05/08 14:21:29] SuperNet Training INFO: iter: 81240/144360  CE: 3.5799  
[05/08 14:22:28] SuperNet Training INFO: iter: 81360/144360  CE: 3.2096  
[05/08 14:23:28] SuperNet Training INFO: iter: 81480/144360  CE: 3.1859  
[05/08 14:24:27] SuperNet Training INFO: iter: 81600/144360  CE: 3.4613  
[05/08 14:25:27] SuperNet Training INFO: iter: 81720/144360  CE: 3.4200  
[05/08 14:26:08] SuperNet Training INFO: --> epoch:  68/120  avg CE: 3.3211  lr: 0.04752529855093431  
[05/08 14:27:03] SuperNet Training INFO: iter: 81840/144360  CE: 3.1138  
[05/08 14:28:04] SuperNet Training INFO: iter: 81960/144360  CE: 3.3642  
[05/08 14:29:04] SuperNet Training INFO: iter: 82080/144360  CE: 3.4527  
[05/08 14:30:05] SuperNet Training INFO: iter: 82200/144360  CE: 3.4956  
[05/08 14:31:04] SuperNet Training INFO: iter: 82320/144360  CE: 3.3653  
[05/08 14:32:04] SuperNet Training INFO: iter: 82440/144360  CE: 3.8309  
[05/08 14:33:03] SuperNet Training INFO: iter: 82560/144360  CE: 3.2708  
[05/08 14:34:04] SuperNet Training INFO: iter: 82680/144360  CE: 3.2031  
[05/08 14:35:04] SuperNet Training INFO: iter: 82800/144360  CE: 3.3051  
[05/08 14:36:04] SuperNet Training INFO: iter: 82920/144360  CE: 3.6277  
[05/08 14:36:46] SuperNet Training INFO: --> epoch:  69/120  avg CE: 3.3225  lr: 0.04599327816864548  
[05/08 14:37:41] SuperNet Training INFO: iter: 83040/144360  CE: 3.3622  
[05/08 14:38:42] SuperNet Training INFO: iter: 83160/144360  CE: 3.4954  
[05/08 14:39:42] SuperNet Training INFO: iter: 83280/144360  CE: 3.2969  
[05/08 14:40:43] SuperNet Training INFO: iter: 83400/144360  CE: 3.4059  
[05/08 14:41:42] SuperNet Training INFO: iter: 83520/144360  CE: 3.2298  
[05/08 14:42:43] SuperNet Training INFO: iter: 83640/144360  CE: 3.0170  
[05/08 14:43:42] SuperNet Training INFO: iter: 83760/144360  CE: 3.0766  
[05/08 14:44:42] SuperNet Training INFO: iter: 83880/144360  CE: 3.4537  
[05/08 14:45:43] SuperNet Training INFO: iter: 84000/144360  CE: 3.2530  
[05/08 14:46:43] SuperNet Training INFO: iter: 84120/144360  CE: 3.1294  
[05/08 14:47:27] SuperNet Training INFO: --> epoch:  70/120  avg CE: 3.3053  lr: 0.04447085729384866  
[05/08 14:48:19] SuperNet Training INFO: iter: 84240/144360  CE: 3.3727  
[05/08 14:49:20] SuperNet Training INFO: iter: 84360/144360  CE: 3.1858  
[05/08 14:50:19] SuperNet Training INFO: iter: 84480/144360  CE: 3.3512  
[05/08 14:51:19] SuperNet Training INFO: iter: 84600/144360  CE: 3.6151  
[05/08 14:52:19] SuperNet Training INFO: iter: 84720/144360  CE: 3.0061  
[05/08 14:53:19] SuperNet Training INFO: iter: 84840/144360  CE: 3.3520  
[05/08 14:54:19] SuperNet Training INFO: iter: 84960/144360  CE: 3.3618  
[05/08 14:55:19] SuperNet Training INFO: iter: 85080/144360  CE: 3.2617  
[05/08 14:56:19] SuperNet Training INFO: iter: 85200/144360  CE: 3.3202  
[05/08 14:57:19] SuperNet Training INFO: iter: 85320/144360  CE: 3.2604  
[05/08 14:58:04] SuperNet Training INFO: --> epoch:  71/120  avg CE: 3.2964  lr: 0.04295907931776456  
[05/08 14:58:55] SuperNet Training INFO: iter: 85440/144360  CE: 3.3024  
[05/08 14:59:55] SuperNet Training INFO: iter: 85560/144360  CE: 3.4281  
[05/08 15:00:54] SuperNet Training INFO: iter: 85680/144360  CE: 3.3261  
[05/08 15:01:54] SuperNet Training INFO: iter: 85800/144360  CE: 3.2924  
[05/08 15:02:53] SuperNet Training INFO: iter: 85920/144360  CE: 3.0043  
[05/08 15:03:53] SuperNet Training INFO: iter: 86040/144360  CE: 3.4637  
[05/08 15:04:53] SuperNet Training INFO: iter: 86160/144360  CE: 3.4847  
[05/08 15:05:53] SuperNet Training INFO: iter: 86280/144360  CE: 3.3620  
[05/08 15:06:52] SuperNet Training INFO: iter: 86400/144360  CE: 3.1949  
[05/08 15:07:52] SuperNet Training INFO: iter: 86520/144360  CE: 3.5835  
[05/08 15:08:39] SuperNet Training INFO: --> epoch:  72/120  avg CE: 3.2811  lr: 0.04145898033750296  
[05/08 15:09:28] SuperNet Training INFO: iter: 86640/144360  CE: 3.1324  
[05/08 15:10:28] SuperNet Training INFO: iter: 86760/144360  CE: 3.1683  
[05/08 15:11:28] SuperNet Training INFO: iter: 86880/144360  CE: 3.1640  
[05/08 15:12:27] SuperNet Training INFO: iter: 87000/144360  CE: 3.1981  
[05/08 15:13:27] SuperNet Training INFO: iter: 87120/144360  CE: 2.9994  
[05/08 15:14:26] SuperNet Training INFO: iter: 87240/144360  CE: 3.4531  
[05/08 15:15:27] SuperNet Training INFO: iter: 87360/144360  CE: 3.1495  
[05/08 15:16:26] SuperNet Training INFO: iter: 87480/144360  CE: 3.4641  
[05/08 15:17:26] SuperNet Training INFO: iter: 87600/144360  CE: 3.2405  
[05/08 15:18:26] SuperNet Training INFO: iter: 87720/144360  CE: 3.0223  
[05/08 15:19:15] SuperNet Training INFO: --> epoch:  73/120  avg CE: 3.2721  lr: 0.03997158844597365  
[05/08 15:20:04] SuperNet Training INFO: iter: 87840/144360  CE: 3.4046  
[05/08 15:21:04] SuperNet Training INFO: iter: 87960/144360  CE: 3.3657  
[05/08 15:22:03] SuperNet Training INFO: iter: 88080/144360  CE: 3.2307  
[05/08 15:23:03] SuperNet Training INFO: iter: 88200/144360  CE: 3.3422  
[05/08 15:24:02] SuperNet Training INFO: iter: 88320/144360  CE: 3.2223  
[05/08 15:25:02] SuperNet Training INFO: iter: 88440/144360  CE: 3.3089  
[05/08 15:26:00] SuperNet Training INFO: iter: 88560/144360  CE: 2.9059  
[05/08 15:26:59] SuperNet Training INFO: iter: 88680/144360  CE: 3.0216  
[05/08 15:27:59] SuperNet Training INFO: iter: 88800/144360  CE: 3.0955  
[05/08 15:28:58] SuperNet Training INFO: iter: 88920/144360  CE: 3.3654  
[05/08 15:29:47] SuperNet Training INFO: --> epoch:  74/120  avg CE: 3.2537  lr: 0.03849792302728192  
[05/08 15:30:34] SuperNet Training INFO: iter: 89040/144360  CE: 3.1640  
[05/08 15:31:33] SuperNet Training INFO: iter: 89160/144360  CE: 3.3208  
[05/08 15:32:33] SuperNet Training INFO: iter: 89280/144360  CE: 3.0290  
[05/08 15:33:33] SuperNet Training INFO: iter: 89400/144360  CE: 3.2745  
[05/08 15:34:34] SuperNet Training INFO: iter: 89520/144360  CE: 3.4508  
[05/08 15:35:33] SuperNet Training INFO: iter: 89640/144360  CE: 3.1379  
[05/08 15:36:33] SuperNet Training INFO: iter: 89760/144360  CE: 3.3859  
[05/08 15:37:34] SuperNet Training INFO: iter: 89880/144360  CE: 3.2311  
[05/08 15:38:33] SuperNet Training INFO: iter: 90000/144360  CE: 3.3481  
[05/08 15:39:34] SuperNet Training INFO: iter: 90120/144360  CE: 3.0941  
[05/08 15:40:27] SuperNet Training INFO: --> epoch:  75/120  avg CE: 3.2473  lr: 0.03703899405809455  
[05/08 15:41:13] SuperNet Training INFO: iter: 90240/144360  CE: 3.3615  
[05/08 15:42:13] SuperNet Training INFO: iter: 90360/144360  CE: 3.0080  
[05/08 15:43:13] SuperNet Training INFO: iter: 90480/144360  CE: 3.5923  
[05/08 15:44:12] SuperNet Training INFO: iter: 90600/144360  CE: 3.0335  
[05/08 15:45:12] SuperNet Training INFO: iter: 90720/144360  CE: 3.1359  
[05/08 15:46:11] SuperNet Training INFO: iter: 90840/144360  CE: 3.4601  
[05/08 15:47:11] SuperNet Training INFO: iter: 90960/144360  CE: 3.2247  
[05/08 15:48:10] SuperNet Training INFO: iter: 91080/144360  CE: 3.4781  
[05/08 15:49:11] SuperNet Training INFO: iter: 91200/144360  CE: 3.2913  
[05/08 15:50:10] SuperNet Training INFO: iter: 91320/144360  CE: 3.1473  
[05/08 15:51:03] SuperNet Training INFO: --> epoch:  76/120  avg CE: 3.2367  lr: 0.035595801415451815  
[05/08 15:51:47] SuperNet Training INFO: iter: 91440/144360  CE: 3.3623  
[05/08 15:52:47] SuperNet Training INFO: iter: 91560/144360  CE: 3.3378  
[05/08 15:53:47] SuperNet Training INFO: iter: 91680/144360  CE: 3.0762  
[05/08 15:54:48] SuperNet Training INFO: iter: 91800/144360  CE: 3.0544  
[05/08 15:55:47] SuperNet Training INFO: iter: 91920/144360  CE: 3.0711  
[05/08 15:56:47] SuperNet Training INFO: iter: 92040/144360  CE: 3.4735  
[05/08 15:57:47] SuperNet Training INFO: iter: 92160/144360  CE: 3.3023  
[05/08 15:58:47] SuperNet Training INFO: iter: 92280/144360  CE: 3.1373  
[05/08 15:59:47] SuperNet Training INFO: iter: 92400/144360  CE: 3.2961  
[05/08 16:00:47] SuperNet Training INFO: iter: 92520/144360  CE: 3.0446  
[05/08 16:01:41] SuperNet Training INFO: --> epoch:  77/120  avg CE: 3.2286  lr: 0.03416933419150217  
[05/08 16:02:23] SuperNet Training INFO: iter: 92640/144360  CE: 3.0161  
[05/08 16:03:23] SuperNet Training INFO: iter: 92760/144360  CE: 3.1180  
[05/08 16:04:22] SuperNet Training INFO: iter: 92880/144360  CE: 3.2408  
[05/08 16:05:22] SuperNet Training INFO: iter: 93000/144360  CE: 3.2058  
[05/08 16:06:23] SuperNet Training INFO: iter: 93120/144360  CE: 3.3681  
[05/08 16:07:23] SuperNet Training INFO: iter: 93240/144360  CE: 3.1994  
[05/08 16:08:23] SuperNet Training INFO: iter: 93360/144360  CE: 3.3462  
[05/08 16:09:22] SuperNet Training INFO: iter: 93480/144360  CE: 3.0109  
[05/08 16:10:23] SuperNet Training INFO: iter: 93600/144360  CE: 3.0681  
[05/08 16:11:23] SuperNet Training INFO: iter: 93720/144360  CE: 3.1220  
[05/08 16:12:19] SuperNet Training INFO: --> epoch:  78/120  avg CE: 3.2266  lr: 0.03276057001562702  
[05/08 16:13:01] SuperNet Training INFO: iter: 93840/144360  CE: 3.2079  
[05/08 16:14:01] SuperNet Training INFO: iter: 93960/144360  CE: 3.0738  
[05/08 16:15:01] SuperNet Training INFO: iter: 94080/144360  CE: 3.1653  
[05/08 16:16:02] SuperNet Training INFO: iter: 94200/144360  CE: 3.1697  
[05/08 16:17:01] SuperNet Training INFO: iter: 94320/144360  CE: 3.0389  
[05/08 16:18:00] SuperNet Training INFO: iter: 94440/144360  CE: 3.3134  
[05/08 16:19:00] SuperNet Training INFO: iter: 94560/144360  CE: 2.9535  
[05/08 16:20:00] SuperNet Training INFO: iter: 94680/144360  CE: 3.3042  
[05/08 16:20:59] SuperNet Training INFO: iter: 94800/144360  CE: 3.2101  
[05/08 16:21:58] SuperNet Training INFO: iter: 94920/144360  CE: 3.1676  
[05/08 16:22:55] SuperNet Training INFO: --> epoch:  79/120  avg CE: 3.1965  lr: 0.031370474384423336  
[05/08 16:23:35] SuperNet Training INFO: iter: 95040/144360  CE: 3.5200  
[05/08 16:24:35] SuperNet Training INFO: iter: 95160/144360  CE: 3.2409  
[05/08 16:25:35] SuperNet Training INFO: iter: 95280/144360  CE: 3.1904  
[05/08 16:26:34] SuperNet Training INFO: iter: 95400/144360  CE: 3.2385  
[05/08 16:27:33] SuperNet Training INFO: iter: 95520/144360  CE: 2.9446  
[05/08 16:28:33] SuperNet Training INFO: iter: 95640/144360  CE: 3.2377  
[05/08 16:29:32] SuperNet Training INFO: iter: 95760/144360  CE: 3.3064  
[05/08 16:30:32] SuperNet Training INFO: iter: 95880/144360  CE: 3.3588  
[05/08 16:31:31] SuperNet Training INFO: iter: 96000/144360  CE: 3.3040  
[05/08 16:32:31] SuperNet Training INFO: iter: 96120/144360  CE: 2.8191  
[05/08 16:33:28] SuperNet Training INFO: iter: 96240/144360  CE: 3.0602  
[05/08 16:33:28] SuperNet Training INFO: --> epoch:  80/120  avg CE: 3.1849  lr: 0.02999999999999983  
[05/08 16:35:05] SuperNet Training INFO: iter: 96360/144360  CE: 3.0860  
[05/08 16:36:04] SuperNet Training INFO: iter: 96480/144360  CE: 2.8274  
[05/08 16:37:03] SuperNet Training INFO: iter: 96600/144360  CE: 2.9171  
[05/08 16:38:03] SuperNet Training INFO: iter: 96720/144360  CE: 3.3085  
[05/08 16:39:03] SuperNet Training INFO: iter: 96840/144360  CE: 3.1395  
[05/08 16:40:03] SuperNet Training INFO: iter: 96960/144360  CE: 3.1502  
[05/08 16:41:03] SuperNet Training INFO: iter: 97080/144360  CE: 3.1267  
[05/08 16:42:03] SuperNet Training INFO: iter: 97200/144360  CE: 3.5022  
[05/08 16:43:03] SuperNet Training INFO: iter: 97320/144360  CE: 3.6956  
[05/08 16:44:03] SuperNet Training INFO: iter: 97440/144360  CE: 2.9889  
[05/08 16:44:04] SuperNet Training INFO: --> epoch:  81/120  avg CE: 3.1811  lr: 0.028650086117042926  
[05/08 16:45:39] SuperNet Training INFO: iter: 97560/144360  CE: 3.4997  
[05/08 16:46:38] SuperNet Training INFO: iter: 97680/144360  CE: 2.9097  
[05/08 16:47:39] SuperNet Training INFO: iter: 97800/144360  CE: 3.2019  
[05/08 16:48:38] SuperNet Training INFO: iter: 97920/144360  CE: 3.1868  
[05/08 16:49:39] SuperNet Training INFO: iter: 98040/144360  CE: 3.1495  
[05/08 16:50:39] SuperNet Training INFO: iter: 98160/144360  CE: 3.3299  
[05/08 16:51:39] SuperNet Training INFO: iter: 98280/144360  CE: 3.2769  
[05/08 16:52:39] SuperNet Training INFO: iter: 98400/144360  CE: 3.1111  
[05/08 16:53:37] SuperNet Training INFO: iter: 98520/144360  CE: 3.1332  
[05/08 16:54:37] SuperNet Training INFO: iter: 98640/144360  CE: 3.3073  
[05/08 16:54:39] SuperNet Training INFO: --> epoch:  82/120  avg CE: 3.1621  lr: 0.027321657899098132  
[05/08 16:56:13] SuperNet Training INFO: iter: 98760/144360  CE: 2.9456  
[05/08 16:57:13] SuperNet Training INFO: iter: 98880/144360  CE: 3.1751  
[05/08 16:58:12] SuperNet Training INFO: iter: 99000/144360  CE: 3.0414  
[05/08 16:59:11] SuperNet Training INFO: iter: 99120/144360  CE: 3.3559  
[05/08 17:00:11] SuperNet Training INFO: iter: 99240/144360  CE: 3.1711  
[05/08 17:01:09] SuperNet Training INFO: iter: 99360/144360  CE: 3.0685  
[05/08 17:02:08] SuperNet Training INFO: iter: 99480/144360  CE: 3.2350  
[05/08 17:03:06] SuperNet Training INFO: iter: 99600/144360  CE: 3.1018  
[05/08 17:04:05] SuperNet Training INFO: iter: 99720/144360  CE: 2.8468  
[05/08 17:05:03] SuperNet Training INFO: iter: 99840/144360  CE: 3.0325  
[05/08 17:05:06] SuperNet Training INFO: --> epoch:  83/120  avg CE: 3.1628  lr: 0.0260156257845098  
[05/08 17:06:38] SuperNet Training INFO: iter: 99960/144360  CE: 3.0794  
[05/08 17:07:38] SuperNet Training INFO: iter: 100080/144360  CE: 3.4551  
[05/08 17:08:38] SuperNet Training INFO: iter: 100200/144360  CE: 3.5801  
[05/08 17:09:38] SuperNet Training INFO: iter: 100320/144360  CE: 3.2288  
[05/08 17:10:37] SuperNet Training INFO: iter: 100440/144360  CE: 3.3559  
[05/08 17:11:36] SuperNet Training INFO: iter: 100560/144360  CE: 3.2289  
[05/08 17:12:36] SuperNet Training INFO: iter: 100680/144360  CE: 2.9239  
[05/08 17:13:35] SuperNet Training INFO: iter: 100800/144360  CE: 3.2055  
[05/08 17:14:34] SuperNet Training INFO: iter: 100920/144360  CE: 3.1331  
[05/08 17:15:34] SuperNet Training INFO: iter: 101040/144360  CE: 3.2367  
[05/08 17:15:39] SuperNet Training INFO: --> epoch:  84/120  avg CE: 3.1434  lr: 0.02473288486245143  
[05/08 17:17:11] SuperNet Training INFO: iter: 101160/144360  CE: 3.1214  
[05/08 17:18:11] SuperNet Training INFO: iter: 101280/144360  CE: 3.2437  
[05/08 17:19:11] SuperNet Training INFO: iter: 101400/144360  CE: 3.1941  
[05/08 17:20:12] SuperNet Training INFO: iter: 101520/144360  CE: 2.8306  
[05/08 17:21:12] SuperNet Training INFO: iter: 101640/144360  CE: 3.2857  
[05/08 17:22:13] SuperNet Training INFO: iter: 101760/144360  CE: 3.0771  
[05/08 17:23:12] SuperNet Training INFO: iter: 101880/144360  CE: 3.1996  
[05/08 17:24:12] SuperNet Training INFO: iter: 102000/144360  CE: 3.2116  
[05/08 17:25:12] SuperNet Training INFO: iter: 102120/144360  CE: 2.9122  
[05/08 17:26:13] SuperNet Training INFO: iter: 102240/144360  CE: 3.0654  
[05/08 17:26:19] SuperNet Training INFO: --> epoch:  85/120  avg CE: 3.1380  lr: 0.023474314259476523  
[05/08 17:27:49] SuperNet Training INFO: iter: 102360/144360  CE: 3.1281  
[05/08 17:28:49] SuperNet Training INFO: iter: 102480/144360  CE: 3.3617  
[05/08 17:29:49] SuperNet Training INFO: iter: 102600/144360  CE: 3.4946  
[05/08 17:30:47] SuperNet Training INFO: iter: 102720/144360  CE: 3.1175  
[05/08 17:31:47] SuperNet Training INFO: iter: 102840/144360  CE: 3.0060  
[05/08 17:32:46] SuperNet Training INFO: iter: 102960/144360  CE: 3.0494  
[05/08 17:33:46] SuperNet Training INFO: iter: 103080/144360  CE: 2.9257  
[05/08 17:34:45] SuperNet Training INFO: iter: 103200/144360  CE: 3.1607  
[05/08 17:35:45] SuperNet Training INFO: iter: 103320/144360  CE: 3.3299  
[05/08 17:36:44] SuperNet Training INFO: iter: 103440/144360  CE: 3.3685  
[05/08 17:36:52] SuperNet Training INFO: --> epoch:  86/120  avg CE: 3.1292  lr: 0.02224077653700959  
[05/08 17:38:19] SuperNet Training INFO: iter: 103560/144360  CE: 3.3071  
[05/08 17:39:19] SuperNet Training INFO: iter: 103680/144360  CE: 2.9789  
[05/08 17:40:18] SuperNet Training INFO: iter: 103800/144360  CE: 2.8471  
[05/08 17:41:18] SuperNet Training INFO: iter: 103920/144360  CE: 3.1560  
[05/08 17:42:17] SuperNet Training INFO: iter: 104040/144360  CE: 3.2419  
[05/08 17:43:18] SuperNet Training INFO: iter: 104160/144360  CE: 3.0982  
[05/08 17:44:17] SuperNet Training INFO: iter: 104280/144360  CE: 2.9685  
[05/08 17:45:17] SuperNet Training INFO: iter: 104400/144360  CE: 3.1134  
[05/08 17:46:18] SuperNet Training INFO: iter: 104520/144360  CE: 3.4102  
[05/08 17:47:17] SuperNet Training INFO: iter: 104640/144360  CE: 3.0655  
[05/08 17:47:27] SuperNet Training INFO: --> epoch:  87/120  avg CE: 3.1068  lr: 0.02103311710018879  
[05/08 17:48:54] SuperNet Training INFO: iter: 104760/144360  CE: 3.0511  
[05/08 17:49:54] SuperNet Training INFO: iter: 104880/144360  CE: 3.1590  
[05/08 17:50:53] SuperNet Training INFO: iter: 105000/144360  CE: 3.1824  
[05/08 17:51:53] SuperNet Training INFO: iter: 105120/144360  CE: 3.1537  
[05/08 17:52:52] SuperNet Training INFO: iter: 105240/144360  CE: 3.2400  
[05/08 17:53:51] SuperNet Training INFO: iter: 105360/144360  CE: 3.1953  
[05/08 17:54:51] SuperNet Training INFO: iter: 105480/144360  CE: 3.2146  
[05/08 17:55:51] SuperNet Training INFO: iter: 105600/144360  CE: 3.2810  
[05/08 17:56:51] SuperNet Training INFO: iter: 105720/144360  CE: 3.1908  
[05/08 17:57:50] SuperNet Training INFO: iter: 105840/144360  CE: 3.4623  
[05/08 17:58:01] SuperNet Training INFO: --> epoch:  88/120  avg CE: 3.1078  lr: 0.019852163618468272  
[05/08 17:59:27] SuperNet Training INFO: iter: 105960/144360  CE: 3.2625  
[05/08 18:00:26] SuperNet Training INFO: iter: 106080/144360  CE: 2.7570  
[05/08 18:01:26] SuperNet Training INFO: iter: 106200/144360  CE: 3.0387  
[05/08 18:02:26] SuperNet Training INFO: iter: 106320/144360  CE: 2.9014  
[05/08 18:03:26] SuperNet Training INFO: iter: 106440/144360  CE: 2.9027  
[05/08 18:04:25] SuperNet Training INFO: iter: 106560/144360  CE: 3.0581  
[05/08 18:05:23] SuperNet Training INFO: iter: 106680/144360  CE: 2.8416  
[05/08 18:06:24] SuperNet Training INFO: iter: 106800/144360  CE: 3.1136  
[05/08 18:07:23] SuperNet Training INFO: iter: 106920/144360  CE: 3.0540  
[05/08 18:08:22] SuperNet Training INFO: iter: 107040/144360  CE: 3.1293  
[05/08 18:08:35] SuperNet Training INFO: --> epoch:  89/120  avg CE: 3.1003  lr: 0.018698725458374543  
[05/08 18:09:58] SuperNet Training INFO: iter: 107160/144360  CE: 3.0621  
[05/08 18:10:58] SuperNet Training INFO: iter: 107280/144360  CE: 3.0686  
[05/08 18:11:57] SuperNet Training INFO: iter: 107400/144360  CE: 2.9775  
[05/08 18:12:56] SuperNet Training INFO: iter: 107520/144360  CE: 2.8553  
[05/08 18:13:56] SuperNet Training INFO: iter: 107640/144360  CE: 3.4285  
[05/08 18:14:55] SuperNet Training INFO: iter: 107760/144360  CE: 2.7207  
[05/08 18:15:55] SuperNet Training INFO: iter: 107880/144360  CE: 2.9850  
[05/08 18:16:53] SuperNet Training INFO: iter: 108000/144360  CE: 2.9822  
[05/08 18:17:52] SuperNet Training INFO: iter: 108120/144360  CE: 3.0739  
[05/08 18:18:51] SuperNet Training INFO: iter: 108240/144360  CE: 3.3677  
[05/08 18:19:05] SuperNet Training INFO: --> epoch:  90/120  avg CE: 3.0778  lr: 0.01757359312880703  
[05/08 18:20:27] SuperNet Training INFO: iter: 108360/144360  CE: 3.1973  
[05/08 18:21:26] SuperNet Training INFO: iter: 108480/144360  CE: 3.2252  
[05/08 18:22:26] SuperNet Training INFO: iter: 108600/144360  CE: 2.9890  
[05/08 18:23:25] SuperNet Training INFO: iter: 108720/144360  CE: 3.0904  
[05/08 18:24:24] SuperNet Training INFO: iter: 108840/144360  CE: 2.9935  
[05/08 18:25:24] SuperNet Training INFO: iter: 108960/144360  CE: 2.9392  
[05/08 18:26:25] SuperNet Training INFO: iter: 109080/144360  CE: 3.1442  
[05/08 18:27:24] SuperNet Training INFO: iter: 109200/144360  CE: 3.0638  
[05/08 18:28:24] SuperNet Training INFO: iter: 109320/144360  CE: 3.1110  
[05/08 18:29:23] SuperNet Training INFO: iter: 109440/144360  CE: 3.0666  
[05/08 18:29:39] SuperNet Training INFO: --> epoch:  91/120  avg CE: 3.0643  lr: 0.016477537739262627  
[05/08 18:31:00] SuperNet Training INFO: iter: 109560/144360  CE: 2.9354  
[05/08 18:32:00] SuperNet Training INFO: iter: 109680/144360  CE: 2.9149  
[05/08 18:33:00] SuperNet Training INFO: iter: 109800/144360  CE: 3.0004  
[05/08 18:34:01] SuperNet Training INFO: iter: 109920/144360  CE: 2.8848  
[05/08 18:35:00] SuperNet Training INFO: iter: 110040/144360  CE: 2.8842  
[05/08 18:36:00] SuperNet Training INFO: iter: 110160/144360  CE: 2.9527  
[05/08 18:37:01] SuperNet Training INFO: iter: 110280/144360  CE: 2.9775  
[05/08 18:38:02] SuperNet Training INFO: iter: 110400/144360  CE: 3.0349  
[05/08 18:39:01] SuperNet Training INFO: iter: 110520/144360  CE: 2.9580  
[05/08 18:40:00] SuperNet Training INFO: iter: 110640/144360  CE: 3.1681  
[05/08 18:40:18] SuperNet Training INFO: --> epoch:  92/120  avg CE: 3.0678  lr: 0.015411310471356233  
[05/08 18:41:37] SuperNet Training INFO: iter: 110760/144360  CE: 3.1768  
[05/08 18:42:37] SuperNet Training INFO: iter: 110880/144360  CE: 2.9849  
[05/08 18:43:37] SuperNet Training INFO: iter: 111000/144360  CE: 2.9124  
[05/08 18:44:37] SuperNet Training INFO: iter: 111120/144360  CE: 3.2498  
[05/08 18:45:37] SuperNet Training INFO: iter: 111240/144360  CE: 2.8302  
[05/08 18:46:37] SuperNet Training INFO: iter: 111360/144360  CE: 2.7590  
[05/08 18:47:37] SuperNet Training INFO: iter: 111480/144360  CE: 2.8930  
[05/08 18:48:37] SuperNet Training INFO: iter: 111600/144360  CE: 3.0488  
[05/08 18:49:36] SuperNet Training INFO: iter: 111720/144360  CE: 3.0377  
[05/08 18:50:36] SuperNet Training INFO: iter: 111840/144360  CE: 3.1513  
[05/08 18:50:54] SuperNet Training INFO: --> epoch:  93/120  avg CE: 3.0481  lr: 0.014375642063998082  
[05/08 18:52:12] SuperNet Training INFO: iter: 111960/144360  CE: 3.2915  
[05/08 18:53:11] SuperNet Training INFO: iter: 112080/144360  CE: 3.4252  
[05/08 18:54:10] SuperNet Training INFO: iter: 112200/144360  CE: 2.8216  
[05/08 18:55:10] SuperNet Training INFO: iter: 112320/144360  CE: 3.0740  
[05/08 18:56:10] SuperNet Training INFO: iter: 112440/144360  CE: 3.2439  
[05/08 18:57:10] SuperNet Training INFO: iter: 112560/144360  CE: 3.1316  
[05/08 18:58:10] SuperNet Training INFO: iter: 112680/144360  CE: 2.9733  
[05/08 18:59:09] SuperNet Training INFO: iter: 112800/144360  CE: 2.8772  
[05/08 19:00:09] SuperNet Training INFO: iter: 112920/144360  CE: 2.8505  
[05/08 19:01:08] SuperNet Training INFO: iter: 113040/144360  CE: 2.8249  
[05/08 19:01:28] SuperNet Training INFO: --> epoch:  94/120  avg CE: 3.0477  lr: 0.01337124231258167  
[05/08 19:02:44] SuperNet Training INFO: iter: 113160/144360  CE: 3.0354  
[05/08 19:03:44] SuperNet Training INFO: iter: 113280/144360  CE: 3.0622  
[05/08 19:04:44] SuperNet Training INFO: iter: 113400/144360  CE: 2.9356  
[05/08 19:05:43] SuperNet Training INFO: iter: 113520/144360  CE: 3.3546  
[05/08 19:06:43] SuperNet Training INFO: iter: 113640/144360  CE: 3.2049  
[05/08 19:07:42] SuperNet Training INFO: iter: 113760/144360  CE: 2.7454  
[05/08 19:08:41] SuperNet Training INFO: iter: 113880/144360  CE: 2.9186  
[05/08 19:09:40] SuperNet Training INFO: iter: 114000/144360  CE: 2.8574  
[05/08 19:10:39] SuperNet Training INFO: iter: 114120/144360  CE: 2.8807  
[05/08 19:11:37] SuperNet Training INFO: iter: 114240/144360  CE: 2.9821  
[05/08 19:11:58] SuperNet Training INFO: --> epoch:  95/120  avg CE: 3.0369  lr: 0.012398799582525794  
[05/08 19:13:14] SuperNet Training INFO: iter: 114360/144360  CE: 3.1363  
[05/08 19:14:13] SuperNet Training INFO: iter: 114480/144360  CE: 2.7987  
[05/08 19:15:12] SuperNet Training INFO: iter: 114600/144360  CE: 2.8653  
[05/08 19:16:12] SuperNet Training INFO: iter: 114720/144360  CE: 3.0359  
[05/08 19:17:13] SuperNet Training INFO: iter: 114840/144360  CE: 3.0585  
[05/08 19:18:13] SuperNet Training INFO: iter: 114960/144360  CE: 3.1794  
[05/08 19:19:14] SuperNet Training INFO: iter: 115080/144360  CE: 2.7921  
[05/08 19:20:13] SuperNet Training INFO: iter: 115200/144360  CE: 3.1189  
[05/08 19:21:13] SuperNet Training INFO: iter: 115320/144360  CE: 2.8087  
[05/08 19:22:14] SuperNet Training INFO: iter: 115440/144360  CE: 2.9584  
[05/08 19:22:37] SuperNet Training INFO: --> epoch:  96/120  avg CE: 3.0281  lr: 0.01145898033750309  
[05/08 19:23:50] SuperNet Training INFO: iter: 115560/144360  CE: 3.3244  
[05/08 19:24:51] SuperNet Training INFO: iter: 115680/144360  CE: 2.8436  
[05/08 19:25:50] SuperNet Training INFO: iter: 115800/144360  CE: 3.1060  
[05/08 19:26:50] SuperNet Training INFO: iter: 115920/144360  CE: 2.9039  
[05/08 19:27:50] SuperNet Training INFO: iter: 116040/144360  CE: 2.9641  
[05/08 19:28:51] SuperNet Training INFO: iter: 116160/144360  CE: 2.6569  
[05/08 19:29:50] SuperNet Training INFO: iter: 116280/144360  CE: 2.7351  
[05/08 19:30:51] SuperNet Training INFO: iter: 116400/144360  CE: 2.9697  
[05/08 19:31:50] SuperNet Training INFO: iter: 116520/144360  CE: 3.1393  
[05/08 19:32:50] SuperNet Training INFO: iter: 116640/144360  CE: 2.9255  
[05/08 19:33:14] SuperNet Training INFO: --> epoch:  97/120  avg CE: 3.0149  lr: 0.01055242868267905  
[05/08 19:34:26] SuperNet Training INFO: iter: 116760/144360  CE: 3.2383  
[05/08 19:35:26] SuperNet Training INFO: iter: 116880/144360  CE: 2.6275  
[05/08 19:36:26] SuperNet Training INFO: iter: 117000/144360  CE: 2.9732  
[05/08 19:37:26] SuperNet Training INFO: iter: 117120/144360  CE: 3.0594  
[05/08 19:38:25] SuperNet Training INFO: iter: 117240/144360  CE: 2.8096  
[05/08 19:39:24] SuperNet Training INFO: iter: 117360/144360  CE: 3.0753  
[05/08 19:40:23] SuperNet Training INFO: iter: 117480/144360  CE: 2.9118  
[05/08 19:41:22] SuperNet Training INFO: iter: 117600/144360  CE: 3.0307  
[05/08 19:42:20] SuperNet Training INFO: iter: 117720/144360  CE: 3.0778  
[05/08 19:43:18] SuperNet Training INFO: iter: 117840/144360  CE: 3.1734  
[05/08 19:43:43] SuperNet Training INFO: --> epoch:  98/120  avg CE: 3.0036  lr: 0.009679765923274538  
[05/08 19:44:54] SuperNet Training INFO: iter: 117960/144360  CE: 3.0205  
[05/08 19:45:55] SuperNet Training INFO: iter: 118080/144360  CE: 3.0446  
[05/08 19:46:56] SuperNet Training INFO: iter: 118200/144360  CE: 3.0988  
[05/08 19:47:56] SuperNet Training INFO: iter: 118320/144360  CE: 2.7313  
[05/08 19:48:56] SuperNet Training INFO: iter: 118440/144360  CE: 3.2618  
[05/08 19:49:56] SuperNet Training INFO: iter: 118560/144360  CE: 2.7244  
[05/08 19:50:55] SuperNet Training INFO: iter: 118680/144360  CE: 2.7381  
[05/08 19:51:55] SuperNet Training INFO: iter: 118800/144360  CE: 3.0617  
[05/08 19:52:56] SuperNet Training INFO: iter: 118920/144360  CE: 2.9262  
[05/08 19:53:57] SuperNet Training INFO: iter: 119040/144360  CE: 3.1351  
[05/08 19:54:23] SuperNet Training INFO: --> epoch:  99/120  avg CE: 2.9974  lr: 0.008841590138754444  
[05/08 19:55:31] SuperNet Training INFO: iter: 119160/144360  CE: 2.8353  
[05/08 19:56:31] SuperNet Training INFO: iter: 119280/144360  CE: 2.8471  
[05/08 19:57:31] SuperNet Training INFO: iter: 119400/144360  CE: 3.1761  
[05/08 19:58:31] SuperNet Training INFO: iter: 119520/144360  CE: 2.8721  
[05/08 19:59:31] SuperNet Training INFO: iter: 119640/144360  CE: 3.0155  
[05/08 20:00:31] SuperNet Training INFO: iter: 119760/144360  CE: 2.9930  
[05/08 20:01:30] SuperNet Training INFO: iter: 119880/144360  CE: 3.1520  
[05/08 20:02:30] SuperNet Training INFO: iter: 120000/144360  CE: 3.0544  
[05/08 20:03:28] SuperNet Training INFO: iter: 120120/144360  CE: 2.8724  
[05/08 20:04:28] SuperNet Training INFO: iter: 120240/144360  CE: 2.9385  
[05/08 20:04:56] SuperNet Training INFO: --> epoch: 100/120  avg CE: 2.9872  lr: 0.00803847577293368  
[05/08 20:06:04] SuperNet Training INFO: iter: 120360/144360  CE: 2.8042  
[05/08 20:07:05] SuperNet Training INFO: iter: 120480/144360  CE: 2.7551  
[05/08 20:08:05] SuperNet Training INFO: iter: 120600/144360  CE: 3.0725  
[05/08 20:09:04] SuperNet Training INFO: iter: 120720/144360  CE: 3.3896  
[05/08 20:10:04] SuperNet Training INFO: iter: 120840/144360  CE: 3.1467  
[05/08 20:11:04] SuperNet Training INFO: iter: 120960/144360  CE: 2.9429  
[05/08 20:12:04] SuperNet Training INFO: iter: 121080/144360  CE: 2.9496  
[05/08 20:13:03] SuperNet Training INFO: iter: 121200/144360  CE: 2.7872  
[05/08 20:14:03] SuperNet Training INFO: iter: 121320/144360  CE: 3.0901  
[05/08 20:15:02] SuperNet Training INFO: iter: 121440/144360  CE: 2.8254  
[05/08 20:15:33] SuperNet Training INFO: --> epoch: 101/120  avg CE: 2.9873  lr: 0.007270973240282054  
[05/08 20:16:39] SuperNet Training INFO: iter: 121560/144360  CE: 3.1408  
[05/08 20:17:39] SuperNet Training INFO: iter: 121680/144360  CE: 2.9529  
[05/08 20:18:39] SuperNet Training INFO: iter: 121800/144360  CE: 2.8964  
[05/08 20:19:39] SuperNet Training INFO: iter: 121920/144360  CE: 2.8000  
[05/08 20:20:39] SuperNet Training INFO: iter: 122040/144360  CE: 3.0783  
[05/08 20:21:39] SuperNet Training INFO: iter: 122160/144360  CE: 3.2719  
[05/08 20:22:39] SuperNet Training INFO: iter: 122280/144360  CE: 2.8376  
[05/08 20:23:39] SuperNet Training INFO: iter: 122400/144360  CE: 3.1270  
[05/08 20:24:39] SuperNet Training INFO: iter: 122520/144360  CE: 3.1299  
[05/08 20:25:38] SuperNet Training INFO: iter: 122640/144360  CE: 3.1113  
[05/08 20:26:09] SuperNet Training INFO: --> epoch: 102/120  avg CE: 2.9788  lr: 0.006539608548697928  
[05/08 20:27:14] SuperNet Training INFO: iter: 122760/144360  CE: 3.0150  
[05/08 20:28:14] SuperNet Training INFO: iter: 122880/144360  CE: 2.8004  
[05/08 20:29:14] SuperNet Training INFO: iter: 123000/144360  CE: 3.0065  
[05/08 20:30:13] SuperNet Training INFO: iter: 123120/144360  CE: 3.0401  
[05/08 20:31:13] SuperNet Training INFO: iter: 123240/144360  CE: 2.9221  
[05/08 20:32:13] SuperNet Training INFO: iter: 123360/144360  CE: 2.8435  
[05/08 20:33:13] SuperNet Training INFO: iter: 123480/144360  CE: 3.1210  
[05/08 20:34:13] SuperNet Training INFO: iter: 123600/144360  CE: 2.9545  
[05/08 20:35:12] SuperNet Training INFO: iter: 123720/144360  CE: 3.3197  
[05/08 20:36:12] SuperNet Training INFO: iter: 123840/144360  CE: 3.0276  
[05/08 20:36:45] SuperNet Training INFO: --> epoch: 103/120  avg CE: 2.9761  lr: 0.00584488293900834  
[05/08 20:37:47] SuperNet Training INFO: iter: 123960/144360  CE: 3.0285  
[05/08 20:38:47] SuperNet Training INFO: iter: 124080/144360  CE: 3.0551  
[05/08 20:39:47] SuperNet Training INFO: iter: 124200/144360  CE: 2.8071  
[05/08 20:40:48] SuperNet Training INFO: iter: 124320/144360  CE: 2.9483  
[05/08 20:41:47] SuperNet Training INFO: iter: 124440/144360  CE: 2.8317  
[05/08 20:42:48] SuperNet Training INFO: iter: 124560/144360  CE: 2.7602  
[05/08 20:43:47] SuperNet Training INFO: iter: 124680/144360  CE: 3.1350  
[05/08 20:44:47] SuperNet Training INFO: iter: 124800/144360  CE: 2.7764  
[05/08 20:45:47] SuperNet Training INFO: iter: 124920/144360  CE: 3.0280  
[05/08 20:46:47] SuperNet Training INFO: iter: 125040/144360  CE: 2.8863  
[05/08 20:47:22] SuperNet Training INFO: --> epoch: 104/120  avg CE: 2.9653  lr: 0.005187272541443939  
[05/08 20:48:22] SuperNet Training INFO: iter: 125160/144360  CE: 3.2060  
[05/08 20:49:21] SuperNet Training INFO: iter: 125280/144360  CE: 3.1620  
[05/08 20:50:20] SuperNet Training INFO: iter: 125400/144360  CE: 3.1484  
[05/08 20:51:20] SuperNet Training INFO: iter: 125520/144360  CE: 2.9789  
[05/08 20:52:19] SuperNet Training INFO: iter: 125640/144360  CE: 3.1456  
[05/08 20:53:19] SuperNet Training INFO: iter: 125760/144360  CE: 2.9061  
[05/08 20:54:18] SuperNet Training INFO: iter: 125880/144360  CE: 2.9351  
[05/08 20:55:17] SuperNet Training INFO: iter: 126000/144360  CE: 2.6834  
[05/08 20:56:17] SuperNet Training INFO: iter: 126120/144360  CE: 2.7309  
[05/08 20:57:15] SuperNet Training INFO: iter: 126240/144360  CE: 3.0073  
[05/08 20:57:53] SuperNet Training INFO: --> epoch: 105/120  avg CE: 2.9599  lr: 0.00456722804932279  
[05/08 20:58:52] SuperNet Training INFO: iter: 126360/144360  CE: 2.9388  
[05/08 20:59:53] SuperNet Training INFO: iter: 126480/144360  CE: 2.9631  
[05/08 21:00:52] SuperNet Training INFO: iter: 126600/144360  CE: 2.9361  
[05/08 21:01:53] SuperNet Training INFO: iter: 126720/144360  CE: 3.0522  
[05/08 21:02:52] SuperNet Training INFO: iter: 126840/144360  CE: 2.9800  
[05/08 21:03:52] SuperNet Training INFO: iter: 126960/144360  CE: 2.9353  
[05/08 21:04:52] SuperNet Training INFO: iter: 127080/144360  CE: 2.9174  
[05/08 21:05:51] SuperNet Training INFO: iter: 127200/144360  CE: 3.0293  
[05/08 21:06:51] SuperNet Training INFO: iter: 127320/144360  CE: 2.9851  
[05/08 21:07:50] SuperNet Training INFO: iter: 127440/144360  CE: 2.7373  
[05/08 21:08:28] SuperNet Training INFO: --> epoch: 106/120  avg CE: 2.9462  lr: 0.003985174410167894  
[05/08 21:09:25] SuperNet Training INFO: iter: 127560/144360  CE: 2.8098  
[05/08 21:10:24] SuperNet Training INFO: iter: 127680/144360  CE: 2.8263  
[05/08 21:11:24] SuperNet Training INFO: iter: 127800/144360  CE: 2.9066  
[05/08 21:12:23] SuperNet Training INFO: iter: 127920/144360  CE: 2.9341  
[05/08 21:13:23] SuperNet Training INFO: iter: 128040/144360  CE: 2.7010  
[05/08 21:14:22] SuperNet Training INFO: iter: 128160/144360  CE: 3.0955  
[05/08 21:15:23] SuperNet Training INFO: iter: 128280/144360  CE: 2.7823  
[05/08 21:16:23] SuperNet Training INFO: iter: 128400/144360  CE: 2.7460  
[05/08 21:17:22] SuperNet Training INFO: iter: 128520/144360  CE: 2.8533  
[05/08 21:18:22] SuperNet Training INFO: iter: 128640/144360  CE: 2.9720  
[05/08 21:19:01] SuperNet Training INFO: --> epoch: 107/120  avg CE: 2.9446  lr: 0.003441510534469298  
[05/08 21:19:58] SuperNet Training INFO: iter: 128760/144360  CE: 3.0508  
[05/08 21:21:00] SuperNet Training INFO: iter: 128880/144360  CE: 2.5509  
[05/08 21:21:59] SuperNet Training INFO: iter: 129000/144360  CE: 3.0778  
[05/08 21:23:00] SuperNet Training INFO: iter: 129120/144360  CE: 3.1219  
[05/08 21:24:00] SuperNet Training INFO: iter: 129240/144360  CE: 3.0653  
[05/08 21:25:00] SuperNet Training INFO: iter: 129360/144360  CE: 2.6764  
[05/08 21:26:00] SuperNet Training INFO: iter: 129480/144360  CE: 2.8811  
[05/08 21:27:00] SuperNet Training INFO: iter: 129600/144360  CE: 3.1113  
[05/08 21:27:59] SuperNet Training INFO: iter: 129720/144360  CE: 2.9224  
[05/08 21:28:59] SuperNet Training INFO: iter: 129840/144360  CE: 3.2362  
[05/08 21:29:41] SuperNet Training INFO: --> epoch: 108/120  avg CE: 2.9402  lr: 0.002936609022290792  
[05/08 21:30:35] SuperNet Training INFO: iter: 129960/144360  CE: 2.9786  
[05/08 21:31:35] SuperNet Training INFO: iter: 130080/144360  CE: 2.8116  
[05/08 21:32:35] SuperNet Training INFO: iter: 130200/144360  CE: 2.8662  
[05/08 21:33:35] SuperNet Training INFO: iter: 130320/144360  CE: 2.9432  
[05/08 21:34:35] SuperNet Training INFO: iter: 130440/144360  CE: 3.0327  
[05/08 21:35:34] SuperNet Training INFO: iter: 130560/144360  CE: 2.8073  
[05/08 21:36:34] SuperNet Training INFO: iter: 130680/144360  CE: 3.1565  
[05/08 21:37:34] SuperNet Training INFO: iter: 130800/144360  CE: 3.1108  
[05/08 21:38:34] SuperNet Training INFO: iter: 130920/144360  CE: 3.1940  
[05/08 21:39:34] SuperNet Training INFO: iter: 131040/144360  CE: 2.9679  
[05/08 21:40:16] SuperNet Training INFO: --> epoch: 109/120  avg CE: 2.9353  lr: 0.0024708159079084185  
[05/08 21:41:09] SuperNet Training INFO: iter: 131160/144360  CE: 2.8857  
[05/08 21:42:09] SuperNet Training INFO: iter: 131280/144360  CE: 2.9820  
[05/08 21:43:08] SuperNet Training INFO: iter: 131400/144360  CE: 2.6928  
[05/08 21:44:07] SuperNet Training INFO: iter: 131520/144360  CE: 2.8315  
[05/08 21:45:07] SuperNet Training INFO: iter: 131640/144360  CE: 3.0893  
[05/08 21:46:06] SuperNet Training INFO: iter: 131760/144360  CE: 2.6750  
[05/08 21:47:05] SuperNet Training INFO: iter: 131880/144360  CE: 3.2187  
[05/08 21:48:06] SuperNet Training INFO: iter: 132000/144360  CE: 2.9086  
[05/08 21:49:05] SuperNet Training INFO: iter: 132120/144360  CE: 2.8941  
[05/08 21:50:04] SuperNet Training INFO: iter: 132240/144360  CE: 3.0681  
[05/08 21:50:47] SuperNet Training INFO: --> epoch: 110/120  avg CE: 2.9360  lr: 0.0020444504226559065  
[05/08 21:51:41] SuperNet Training INFO: iter: 132360/144360  CE: 3.1143  
[05/08 21:52:39] SuperNet Training INFO: iter: 132480/144360  CE: 3.2184  
[05/08 21:53:39] SuperNet Training INFO: iter: 132600/144360  CE: 3.0107  
[05/08 21:54:38] SuperNet Training INFO: iter: 132720/144360  CE: 2.7299  
[05/08 21:55:38] SuperNet Training INFO: iter: 132840/144360  CE: 3.1750  
[05/08 21:56:37] SuperNet Training INFO: iter: 132960/144360  CE: 2.8293  
[05/08 21:57:35] SuperNet Training INFO: iter: 133080/144360  CE: 2.9624  
[05/08 21:58:34] SuperNet Training INFO: iter: 133200/144360  CE: 2.6405  
[05/08 21:59:34] SuperNet Training INFO: iter: 133320/144360  CE: 2.8493  
[05/08 22:00:33] SuperNet Training INFO: iter: 133440/144360  CE: 3.0022  
[05/08 22:01:17] SuperNet Training INFO: --> epoch: 111/120  avg CE: 2.9274  lr: 0.0016578047761394107  
[05/08 22:02:07] SuperNet Training INFO: iter: 133560/144360  CE: 2.9817  
[05/08 22:03:07] SuperNet Training INFO: iter: 133680/144360  CE: 2.8337  
[05/08 22:04:06] SuperNet Training INFO: iter: 133800/144360  CE: 3.0346  
[05/08 22:05:06] SuperNet Training INFO: iter: 133920/144360  CE: 3.1842  
[05/08 22:06:05] SuperNet Training INFO: iter: 134040/144360  CE: 2.7103  
[05/08 22:07:06] SuperNet Training INFO: iter: 134160/144360  CE: 2.8820  
[05/08 22:08:04] SuperNet Training INFO: iter: 134280/144360  CE: 2.8591  
[05/08 22:09:03] SuperNet Training INFO: iter: 134400/144360  CE: 2.8090  
[05/08 22:10:03] SuperNet Training INFO: iter: 134520/144360  CE: 2.6986  
[05/08 22:11:02] SuperNet Training INFO: iter: 134640/144360  CE: 2.9395  
[05/08 22:11:48] SuperNet Training INFO: --> epoch: 112/120  avg CE: 2.9311  lr: 0.0013111439559716617  
[05/08 22:12:36] SuperNet Training INFO: iter: 134760/144360  CE: 2.8123  
[05/08 22:13:36] SuperNet Training INFO: iter: 134880/144360  CE: 3.1432  
[05/08 22:14:36] SuperNet Training INFO: iter: 135000/144360  CE: 3.0709  
[05/08 22:15:36] SuperNet Training INFO: iter: 135120/144360  CE: 2.9394  
[05/08 22:16:37] SuperNet Training INFO: iter: 135240/144360  CE: 2.9834  
[05/08 22:17:36] SuperNet Training INFO: iter: 135360/144360  CE: 3.1941  
[05/08 22:18:36] SuperNet Training INFO: iter: 135480/144360  CE: 2.7324  
[05/08 22:19:35] SuperNet Training INFO: iter: 135600/144360  CE: 2.9807  
[05/08 22:20:34] SuperNet Training INFO: iter: 135720/144360  CE: 2.7897  
[05/08 22:21:34] SuperNet Training INFO: iter: 135840/144360  CE: 2.6249  
[05/08 22:22:23] SuperNet Training INFO: --> epoch: 113/120  avg CE: 2.9252  lr: 0.0010047055461627253  
[05/08 22:23:10] SuperNet Training INFO: iter: 135960/144360  CE: 3.0027  
[05/08 22:24:09] SuperNet Training INFO: iter: 136080/144360  CE: 3.0174  
[05/08 22:25:10] SuperNet Training INFO: iter: 136200/144360  CE: 2.8301  
[05/08 22:26:11] SuperNet Training INFO: iter: 136320/144360  CE: 3.1242  
[05/08 22:27:10] SuperNet Training INFO: iter: 136440/144360  CE: 3.0172  
[05/08 22:28:09] SuperNet Training INFO: iter: 136560/144360  CE: 2.9173  
[05/08 22:29:09] SuperNet Training INFO: iter: 136680/144360  CE: 2.9816  
[05/08 22:30:09] SuperNet Training INFO: iter: 136800/144360  CE: 2.6719  
[05/08 22:31:08] SuperNet Training INFO: iter: 136920/144360  CE: 2.8354  
[05/08 22:32:08] SuperNet Training INFO: iter: 137040/144360  CE: 3.1845  
[05/08 22:32:57] SuperNet Training INFO: --> epoch: 114/120  avg CE: 2.9239  lr: 0.000738699564291742  
[05/08 22:33:44] SuperNet Training INFO: iter: 137160/144360  CE: 2.8010  
[05/08 22:34:43] SuperNet Training INFO: iter: 137280/144360  CE: 2.6518  
[05/08 22:35:41] SuperNet Training INFO: iter: 137400/144360  CE: 2.6768  
[05/08 22:36:41] SuperNet Training INFO: iter: 137520/144360  CE: 2.9308  
[05/08 22:37:40] SuperNet Training INFO: iter: 137640/144360  CE: 2.8184  
[05/08 22:38:40] SuperNet Training INFO: iter: 137760/144360  CE: 3.1107  
[05/08 22:39:40] SuperNet Training INFO: iter: 137880/144360  CE: 3.0444  
[05/08 22:40:38] SuperNet Training INFO: iter: 138000/144360  CE: 2.9576  
[05/08 22:41:39] SuperNet Training INFO: iter: 138120/144360  CE: 2.9376  
[05/08 22:42:39] SuperNet Training INFO: iter: 138240/144360  CE: 3.1552  
[05/08 22:43:31] SuperNet Training INFO: --> epoch: 115/120  avg CE: 2.9128  lr: 0.0005133083175713779  
[05/08 22:44:15] SuperNet Training INFO: iter: 138360/144360  CE: 2.9862  
[05/08 22:45:14] SuperNet Training INFO: iter: 138480/144360  CE: 3.1265  
[05/08 22:46:15] SuperNet Training INFO: iter: 138600/144360  CE: 2.9444  
[05/08 22:47:14] SuperNet Training INFO: iter: 138720/144360  CE: 2.9371  
[05/08 22:48:15] SuperNet Training INFO: iter: 138840/144360  CE: 2.9456  
[05/08 22:49:14] SuperNet Training INFO: iter: 138960/144360  CE: 3.1584  
[05/08 22:50:15] SuperNet Training INFO: iter: 139080/144360  CE: 3.0470  
[05/08 22:51:14] SuperNet Training INFO: iter: 139200/144360  CE: 3.0406  
[05/08 22:52:13] SuperNet Training INFO: iter: 139320/144360  CE: 2.9061  
[05/08 22:53:13] SuperNet Training INFO: iter: 139440/144360  CE: 2.9268  
[05/08 22:54:06] SuperNet Training INFO: --> epoch: 116/120  avg CE: 2.9125  lr: 0.00032868627790359544  
[05/08 22:54:49] SuperNet Training INFO: iter: 139560/144360  CE: 2.9884  
[05/08 22:55:50] SuperNet Training INFO: iter: 139680/144360  CE: 2.7011  
[05/08 22:56:50] SuperNet Training INFO: iter: 139800/144360  CE: 2.7384  
[05/08 22:57:50] SuperNet Training INFO: iter: 139920/144360  CE: 2.9045  
[05/08 22:58:50] SuperNet Training INFO: iter: 140040/144360  CE: 2.7755  
[05/08 22:59:49] SuperNet Training INFO: iter: 140160/144360  CE: 2.8940  
[05/08 23:00:49] SuperNet Training INFO: iter: 140280/144360  CE: 2.7766  
[05/08 23:01:49] SuperNet Training INFO: iter: 140400/144360  CE: 2.9169  
[05/08 23:02:49] SuperNet Training INFO: iter: 140520/144360  CE: 3.1027  
[05/08 23:03:48] SuperNet Training INFO: iter: 140640/144360  CE: 2.8903  
[05/08 23:04:41] SuperNet Training INFO: --> epoch: 117/120  avg CE: 2.9148  lr: 0.00018495997601232129  
[05/08 23:05:24] SuperNet Training INFO: iter: 140760/144360  CE: 2.9697  
[05/08 23:06:23] SuperNet Training INFO: iter: 140880/144360  CE: 3.1151  
[05/08 23:07:23] SuperNet Training INFO: iter: 141000/144360  CE: 2.8857  
[05/08 23:08:22] SuperNet Training INFO: iter: 141120/144360  CE: 3.0504  
[05/08 23:09:21] SuperNet Training INFO: iter: 141240/144360  CE: 3.2558  
[05/08 23:10:20] SuperNet Training INFO: iter: 141360/144360  CE: 3.0070  
[05/08 23:11:19] SuperNet Training INFO: iter: 141480/144360  CE: 2.6586  
[05/08 23:12:18] SuperNet Training INFO: iter: 141600/144360  CE: 2.8605  
[05/08 23:13:17] SuperNet Training INFO: iter: 141720/144360  CE: 2.7706  
[05/08 23:14:16] SuperNet Training INFO: iter: 141840/144360  CE: 2.7953  
[05/08 23:15:13] SuperNet Training INFO: --> epoch: 118/120  avg CE: 2.9120  lr: 8.222791472556962e-05  
[05/08 23:15:53] SuperNet Training INFO: iter: 141960/144360  CE: 2.7631  
[05/08 23:16:53] SuperNet Training INFO: iter: 142080/144360  CE: 2.8523  
[05/08 23:17:52] SuperNet Training INFO: iter: 142200/144360  CE: 3.1405  
[05/08 23:18:51] SuperNet Training INFO: iter: 142320/144360  CE: 2.9292  
[05/08 23:19:51] SuperNet Training INFO: iter: 142440/144360  CE: 2.9546  
[05/08 23:20:51] SuperNet Training INFO: iter: 142560/144360  CE: 2.8621  
[05/08 23:21:50] SuperNet Training INFO: iter: 142680/144360  CE: 2.7973  
[05/08 23:22:49] SuperNet Training INFO: iter: 142800/144360  CE: 3.0164  
[05/08 23:23:47] SuperNet Training INFO: iter: 142920/144360  CE: 3.0258  
[05/08 23:24:46] SuperNet Training INFO: iter: 143040/144360  CE: 3.0471  
[05/08 23:25:42] SuperNet Training INFO: --> epoch: 119/120  avg CE: 2.9064  lr: 2.0560501466564365e-05  
[05/08 23:26:21] SuperNet Training INFO: iter: 143160/144360  CE: 2.6837  
[05/08 23:27:20] SuperNet Training INFO: iter: 143280/144360  CE: 2.7289  
[05/08 23:28:19] SuperNet Training INFO: iter: 143400/144360  CE: 2.9144  
[05/08 23:29:19] SuperNet Training INFO: iter: 143520/144360  CE: 2.7161  
[05/08 23:30:18] SuperNet Training INFO: iter: 143640/144360  CE: 2.8868  
[05/08 23:31:19] SuperNet Training INFO: iter: 143760/144360  CE: 2.9940  
[05/08 23:32:19] SuperNet Training INFO: iter: 143880/144360  CE: 3.0989  
[05/08 23:33:18] SuperNet Training INFO: iter: 144000/144360  CE: 2.9973  
[05/08 23:34:17] SuperNet Training INFO: iter: 144120/144360  CE: 2.9666  
[05/08 23:35:16] SuperNet Training INFO: iter: 144240/144360  CE: 2.8017  
[05/08 23:36:15] SuperNet Training INFO: iter: 144360/144360  CE: 3.1479  
[05/08 23:36:15] SuperNet Training INFO: --> epoch: 120/120  avg CE: 2.9089  lr: 0.0  
[05/08 23:36:15] SuperNet Training INFO: --> END mobile0-tbs33840-seed-0
[05/08 23:36:15] SuperNet Training INFO: {0: 72180, 1: 72180}
[05/08 23:36:21] SuperNet Training INFO: ELAPSED TIME: 76773.8(s) = 21(h) 19(m)
