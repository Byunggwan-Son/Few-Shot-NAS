[05/07 02:45:09] SuperNet Training INFO: tag                 : mobile0-tbs3
[05/07 02:45:09] SuperNet Training INFO: seed                : 0
[05/07 02:45:09] SuperNet Training INFO: thresholds          : [36, 38]
[05/07 02:45:09] SuperNet Training INFO: data_path           : ../../../dataset/ILSVRC2012
[05/07 02:45:09] SuperNet Training INFO: save_path           : ./SuperNet
[05/07 02:45:09] SuperNet Training INFO: search_space        : proxyless
[05/07 02:45:09] SuperNet Training INFO: valid_size          : 50000
[05/07 02:45:09] SuperNet Training INFO: num_gpus            : 8
[05/07 02:45:09] SuperNet Training INFO: workers             : 4
[05/07 02:45:09] SuperNet Training INFO: interval_ep_eval    : 8
[05/07 02:45:09] SuperNet Training INFO: train_batch_size    : 1024
[05/07 02:45:09] SuperNet Training INFO: test_batch_size     : 256
[05/07 02:45:09] SuperNet Training INFO: max_epoch           : 120
[05/07 02:45:09] SuperNet Training INFO: learning_rate       : 0.12
[05/07 02:45:09] SuperNet Training INFO: momentum            : 0.9
[05/07 02:45:09] SuperNet Training INFO: weight_decay        : 4e-05
[05/07 02:45:09] SuperNet Training INFO: nesterov            : True
[05/07 02:45:09] SuperNet Training INFO: lr_schedule_type    : cosine
[05/07 02:45:09] SuperNet Training INFO: warmup              : False
[05/07 02:45:09] SuperNet Training INFO: label_smooth        : 0.1
[05/07 02:45:09] SuperNet Training INFO: rank                : 0
[05/07 02:45:09] SuperNet Training INFO: gpu                 : 0
[05/07 02:45:09] SuperNet Training INFO: save_name           : mobile0-tbs3-seed-0
[05/07 02:45:09] SuperNet Training INFO: log_path            : ./SuperNet/logs/mobile0-tbs3-seed-0.txt
[05/07 02:45:09] SuperNet Training INFO: ckpt_path           : ./SuperNet/checkpoint/mobile0-tbs3-seed-0.pt
[05/07 02:45:09] SuperNet Training INFO: dist_url            : tcp://127.0.0.1:23456
[05/07 02:45:09] SuperNet Training INFO: world_size          : 8
[05/07 02:45:09] SuperNet Training INFO: distributed         : True
[05/07 02:45:09] SuperNet Training INFO: ['3x3_MBConv3', '3x3_MBConv6', '5x5_MBConv3', '5x5_MBConv6', '7x7_MBConv3', '7x7_MBConv6', 'Identity']
[05/07 02:45:48] SuperNet Training INFO: DistributedDataParallel(
  (module): SuperNet(
    (first_conv): Sequential(
      (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
      (2): ReLU6(inplace=True)
    )
    (first_block): InvertedResidual(
      (depth_conv): Sequential(
        (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)
        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
        (2): ReLU6(inplace=True)
      )
      (point_linear): Sequential(
        (0): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
      )
    )
    (blocks): ModuleList(
      (0): ModuleList(
        (0): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(16, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(48, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=48, bias=False)
            (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(48, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (1): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)
            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(96, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (2): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(16, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(48, 48, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=48, bias=False)
            (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(48, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (3): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(96, 96, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=96, bias=False)
            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(96, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (4): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(16, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(48, 48, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), groups=48, bias=False)
            (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(48, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (5): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(96, 96, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), groups=96, bias=False)
            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(96, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
      )
      (1): ModuleList(
        (0): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(32, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(96, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (1): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (2): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(32, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96, bias=False)
            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(96, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (3): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(192, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=192, bias=False)
            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (4): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(32, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96, bias=False)
            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(96, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (5): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192, bias=False)
            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (6): Identity()
      )
      (2): ModuleList(
        (0): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(32, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(96, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (1): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (2): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(32, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96, bias=False)
            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(96, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (3): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(192, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=192, bias=False)
            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (4): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(32, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96, bias=False)
            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(96, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (5): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192, bias=False)
            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (6): Identity()
      )
      (3): ModuleList(
        (0): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(32, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(96, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (1): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (2): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(32, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96, bias=False)
            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(96, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (3): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(192, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=192, bias=False)
            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (4): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(32, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96, bias=False)
            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(96, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (5): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192, bias=False)
            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (6): Identity()
      )
      (4): ModuleList(
        (0): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(32, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)
            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(96, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (1): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)
            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(192, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (2): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(32, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(96, 96, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=96, bias=False)
            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(96, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (3): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(192, 192, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=192, bias=False)
            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(192, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (4): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(32, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(96, 96, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), groups=96, bias=False)
            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(96, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (5): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(192, 192, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), groups=192, bias=False)
            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(192, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
      )
      (5): ModuleList(
        (0): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(120, 120, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=120, bias=False)
            (1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(120, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (1): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(240, 240, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=240, bias=False)
            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (2): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(120, 120, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=120, bias=False)
            (1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(120, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (3): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(240, 240, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=240, bias=False)
            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (4): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(120, 120, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=120, bias=False)
            (1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(120, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (5): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(240, 240, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=240, bias=False)
            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (6): Identity()
      )
      (6): ModuleList(
        (0): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(120, 120, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=120, bias=False)
            (1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(120, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (1): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(240, 240, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=240, bias=False)
            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (2): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(120, 120, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=120, bias=False)
            (1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(120, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (3): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(240, 240, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=240, bias=False)
            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (4): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(120, 120, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=120, bias=False)
            (1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(120, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (5): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(240, 240, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=240, bias=False)
            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (6): Identity()
      )
      (7): ModuleList(
        (0): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(120, 120, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=120, bias=False)
            (1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(120, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (1): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(240, 240, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=240, bias=False)
            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (2): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(120, 120, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=120, bias=False)
            (1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(120, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (3): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(240, 240, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=240, bias=False)
            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (4): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(120, 120, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=120, bias=False)
            (1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(120, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (5): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(240, 240, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=240, bias=False)
            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (6): Identity()
      )
      (8): ModuleList(
        (0): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(120, 120, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=120, bias=False)
            (1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(120, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (1): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(240, 240, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=240, bias=False)
            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (2): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(120, 120, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=120, bias=False)
            (1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(120, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (3): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(240, 240, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=240, bias=False)
            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (4): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(120, 120, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), groups=120, bias=False)
            (1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(120, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (5): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(240, 240, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), groups=240, bias=False)
            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
      )
      (9): ModuleList(
        (0): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(80, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(240, 240, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=240, bias=False)
            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (1): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)
            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (2): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(80, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(240, 240, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=240, bias=False)
            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (3): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(480, 480, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=480, bias=False)
            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (4): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(80, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(240, 240, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=240, bias=False)
            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (5): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(480, 480, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=480, bias=False)
            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (6): Identity()
      )
      (10): ModuleList(
        (0): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(80, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(240, 240, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=240, bias=False)
            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (1): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)
            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (2): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(80, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(240, 240, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=240, bias=False)
            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (3): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(480, 480, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=480, bias=False)
            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (4): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(80, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(240, 240, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=240, bias=False)
            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (5): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(480, 480, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=480, bias=False)
            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (6): Identity()
      )
      (11): ModuleList(
        (0): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(80, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(240, 240, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=240, bias=False)
            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (1): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)
            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (2): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(80, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(240, 240, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=240, bias=False)
            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (3): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(480, 480, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=480, bias=False)
            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (4): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(80, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(240, 240, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=240, bias=False)
            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (5): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(480, 480, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=480, bias=False)
            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (6): Identity()
      )
      (12): ModuleList(
        (0): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(80, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(240, 240, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=240, bias=False)
            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(240, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (1): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)
            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(480, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (2): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(80, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(240, 240, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=240, bias=False)
            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(240, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (3): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(480, 480, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=480, bias=False)
            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(480, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (4): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(80, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(240, 240, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=240, bias=False)
            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(240, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (5): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(480, 480, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=480, bias=False)
            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(480, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
      )
      (13): ModuleList(
        (0): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(96, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
            (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(288, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (1): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)
            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (2): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(96, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(288, 288, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=288, bias=False)
            (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(288, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (3): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(576, 576, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=576, bias=False)
            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (4): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(96, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(288, 288, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=288, bias=False)
            (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(288, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (5): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(576, 576, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=576, bias=False)
            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (6): Identity()
      )
      (14): ModuleList(
        (0): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(96, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
            (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(288, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (1): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)
            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (2): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(96, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(288, 288, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=288, bias=False)
            (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(288, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (3): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(576, 576, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=576, bias=False)
            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (4): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(96, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(288, 288, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=288, bias=False)
            (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(288, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (5): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(576, 576, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=576, bias=False)
            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (6): Identity()
      )
      (15): ModuleList(
        (0): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(96, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
            (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(288, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (1): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)
            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (2): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(96, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(288, 288, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=288, bias=False)
            (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(288, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (3): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(576, 576, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=576, bias=False)
            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (4): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(96, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(288, 288, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=288, bias=False)
            (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(288, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (5): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(576, 576, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=576, bias=False)
            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (6): Identity()
      )
      (16): ModuleList(
        (0): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(96, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(288, 288, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=288, bias=False)
            (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(288, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (1): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=576, bias=False)
            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(576, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (2): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(96, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(288, 288, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=288, bias=False)
            (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(288, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (3): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(576, 576, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=576, bias=False)
            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(576, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (4): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(96, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(288, 288, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), groups=288, bias=False)
            (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(288, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (5): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(576, 576, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), groups=576, bias=False)
            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(576, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
      )
      (17): ModuleList(
        (0): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(192, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)
            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(576, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (1): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152, bias=False)
            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (2): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(192, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(576, 576, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=576, bias=False)
            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(576, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (3): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)
            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (4): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(192, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(576, 576, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=576, bias=False)
            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(576, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (5): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(1152, 1152, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1152, bias=False)
            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (6): Identity()
      )
      (18): ModuleList(
        (0): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(192, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)
            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(576, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (1): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152, bias=False)
            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (2): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(192, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(576, 576, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=576, bias=False)
            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(576, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (3): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)
            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (4): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(192, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(576, 576, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=576, bias=False)
            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(576, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (5): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(1152, 1152, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1152, bias=False)
            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (6): Identity()
      )
      (19): ModuleList(
        (0): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(192, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)
            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(576, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (1): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152, bias=False)
            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (2): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(192, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(576, 576, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=576, bias=False)
            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(576, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (3): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)
            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (4): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(192, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(576, 576, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=576, bias=False)
            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(576, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (5): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(1152, 1152, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1152, bias=False)
            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (6): Identity()
      )
      (20): ModuleList(
        (0): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(192, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)
            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(576, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (1): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152, bias=False)
            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(1152, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (2): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(192, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(576, 576, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=576, bias=False)
            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(576, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (3): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)
            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(1152, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (4): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(192, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(576, 576, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=576, bias=False)
            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(576, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (5): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(1152, 1152, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1152, bias=False)
            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(1152, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
      )
    )
    (feature_mix_layer): Sequential(
      (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
      (2): ReLU6(inplace=True)
    )
    (avgpool): AvgPool2d(kernel_size=7, stride=7, padding=0)
    (classifier): Sequential(
      (0): Linear(in_features=1280, out_features=1000, bias=True)
    )
  )
)
[05/07 02:46:14] SuperNet Training INFO: Trainset Size: 1231167
[05/07 02:46:14] SuperNet Training INFO: Validset Size:   50000
[05/07 02:46:14] SuperNet Training INFO: Compose(
    RandomResizedCrop(size=(224, 224), scale=(0.08, 1.0), ratio=(0.75, 1.3333), interpolation=bilinear)
    RandomHorizontalFlip(p=0.5)
    ToTensor()
    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
)
[05/07 02:46:14] SuperNet Training INFO: --> START mobile0-tbs3-seed-0
[05/07 02:46:14] SuperNet Training INFO: {0: 0, 1: 0}
[05/07 02:48:43] SuperNet Training INFO: iter:   120/144360  CE: 6.9279  
[05/07 02:49:46] SuperNet Training INFO: iter:   240/144360  CE: 6.8793  
[05/07 02:50:49] SuperNet Training INFO: iter:   360/144360  CE: 6.9189  
[05/07 02:51:49] SuperNet Training INFO: iter:   480/144360  CE: 6.9318  
[05/07 02:52:49] SuperNet Training INFO: iter:   600/144360  CE: 6.8482  
[05/07 02:53:48] SuperNet Training INFO: iter:   720/144360  CE: 6.9125  
[05/07 02:54:48] SuperNet Training INFO: iter:   840/144360  CE: 6.8835  
[05/07 02:55:49] SuperNet Training INFO: iter:   960/144360  CE: 6.8973  
[05/07 02:56:50] SuperNet Training INFO: iter:  1080/144360  CE: 6.9395  
[05/07 02:57:49] SuperNet Training INFO: iter:  1200/144360  CE: 6.9174  
[05/07 02:57:50] SuperNet Training INFO: --> epoch:   1/120  avg CE: 6.8944  lr: 0.11997943949853311  
[05/07 02:59:29] SuperNet Training INFO: iter:  1320/144360  CE: 6.7611  
[05/07 03:00:30] SuperNet Training INFO: iter:  1440/144360  CE: 6.8078  
[05/07 03:01:30] SuperNet Training INFO: iter:  1560/144360  CE: 6.7489  
[05/07 03:02:32] SuperNet Training INFO: iter:  1680/144360  CE: 6.7584  
[05/07 03:03:32] SuperNet Training INFO: iter:  1800/144360  CE: 6.7978  
[05/07 03:04:33] SuperNet Training INFO: iter:  1920/144360  CE: 6.8473  
[05/07 03:05:33] SuperNet Training INFO: iter:  2040/144360  CE: 6.7855  
[05/07 03:06:32] SuperNet Training INFO: iter:  2160/144360  CE: 6.7081  
[05/07 03:07:32] SuperNet Training INFO: iter:  2280/144360  CE: 6.6798  
[05/07 03:08:32] SuperNet Training INFO: iter:  2400/144360  CE: 6.6474  
[05/07 03:08:33] SuperNet Training INFO: --> epoch:   2/120  avg CE: 6.7718  lr: 0.11991777208527424  
[05/07 03:10:10] SuperNet Training INFO: iter:  2520/144360  CE: 6.7051  
[05/07 03:11:11] SuperNet Training INFO: iter:  2640/144360  CE: 6.7157  
[05/07 03:12:13] SuperNet Training INFO: iter:  2760/144360  CE: 6.6698  
[05/07 03:13:15] SuperNet Training INFO: iter:  2880/144360  CE: 6.6171  
[05/07 03:14:16] SuperNet Training INFO: iter:  3000/144360  CE: 6.6834  
[05/07 03:15:17] SuperNet Training INFO: iter:  3120/144360  CE: 6.6343  
[05/07 03:16:19] SuperNet Training INFO: iter:  3240/144360  CE: 6.5126  
[05/07 03:17:21] SuperNet Training INFO: iter:  3360/144360  CE: 6.6980  
[05/07 03:18:22] SuperNet Training INFO: iter:  3480/144360  CE: 6.6443  
[05/07 03:19:22] SuperNet Training INFO: iter:  3600/144360  CE: 6.5545  
[05/07 03:19:26] SuperNet Training INFO: --> epoch:   3/120  avg CE: 6.6364  lr: 0.11981504002398749  
[05/07 03:21:00] SuperNet Training INFO: iter:  3720/144360  CE: 6.5432  
[05/07 03:22:01] SuperNet Training INFO: iter:  3840/144360  CE: 6.6468  
[05/07 03:23:02] SuperNet Training INFO: iter:  3960/144360  CE: 6.6546  
[05/07 03:24:03] SuperNet Training INFO: iter:  4080/144360  CE: 6.4059  
[05/07 03:25:04] SuperNet Training INFO: iter:  4200/144360  CE: 6.4757  
[05/07 03:26:05] SuperNet Training INFO: iter:  4320/144360  CE: 6.3998  
[05/07 03:27:05] SuperNet Training INFO: iter:  4440/144360  CE: 6.4994  
[05/07 03:28:04] SuperNet Training INFO: iter:  4560/144360  CE: 6.4847  
[05/07 03:29:06] SuperNet Training INFO: iter:  4680/144360  CE: 6.2640  
[05/07 03:30:07] SuperNet Training INFO: iter:  4800/144360  CE: 6.4875  
[05/07 03:30:12] SuperNet Training INFO: --> epoch:   4/120  avg CE: 6.4693  lr: 0.11967131372209595  
[05/07 03:31:46] SuperNet Training INFO: iter:  4920/144360  CE: 6.3738  
[05/07 03:32:47] SuperNet Training INFO: iter:  5040/144360  CE: 6.1722  
[05/07 03:33:48] SuperNet Training INFO: iter:  5160/144360  CE: 6.3033  
[05/07 03:34:47] SuperNet Training INFO: iter:  5280/144360  CE: 6.2572  
[05/07 03:35:47] SuperNet Training INFO: iter:  5400/144360  CE: 6.2681  
[05/07 03:36:47] SuperNet Training INFO: iter:  5520/144360  CE: 6.3334  
[05/07 03:37:47] SuperNet Training INFO: iter:  5640/144360  CE: 6.0913  
[05/07 03:38:46] SuperNet Training INFO: iter:  5760/144360  CE: 6.2040  
[05/07 03:39:46] SuperNet Training INFO: iter:  5880/144360  CE: 6.2052  
[05/07 03:40:45] SuperNet Training INFO: iter:  6000/144360  CE: 6.2612  
[05/07 03:40:51] SuperNet Training INFO: --> epoch:   5/120  avg CE: 6.3070  lr: 0.11948669168242801  
[05/07 03:42:24] SuperNet Training INFO: iter:  6120/144360  CE: 6.1548  
[05/07 03:43:25] SuperNet Training INFO: iter:  6240/144360  CE: 6.1285  
[05/07 03:44:27] SuperNet Training INFO: iter:  6360/144360  CE: 6.2130  
[05/07 03:45:29] SuperNet Training INFO: iter:  6480/144360  CE: 6.1917  
[05/07 03:46:32] SuperNet Training INFO: iter:  6600/144360  CE: 6.2069  
[05/07 03:47:33] SuperNet Training INFO: iter:  6720/144360  CE: 6.1279  
[05/07 03:48:34] SuperNet Training INFO: iter:  6840/144360  CE: 6.2264  
[05/07 03:49:35] SuperNet Training INFO: iter:  6960/144360  CE: 6.1513  
[05/07 03:50:36] SuperNet Training INFO: iter:  7080/144360  CE: 6.1197  
[05/07 03:51:37] SuperNet Training INFO: iter:  7200/144360  CE: 6.0754  
[05/07 03:51:45] SuperNet Training INFO: --> epoch:   6/120  avg CE: 6.1633  lr: 0.1192613004357081  
[05/07 03:53:14] SuperNet Training INFO: iter:  7320/144360  CE: 6.1335  
[05/07 03:54:15] SuperNet Training INFO: iter:  7440/144360  CE: 6.0385  
[05/07 03:55:16] SuperNet Training INFO: iter:  7560/144360  CE: 6.0590  
[05/07 03:56:17] SuperNet Training INFO: iter:  7680/144360  CE: 6.1947  
[05/07 03:57:16] SuperNet Training INFO: iter:  7800/144360  CE: 6.0052  
[05/07 03:58:17] SuperNet Training INFO: iter:  7920/144360  CE: 5.9378  
[05/07 03:59:18] SuperNet Training INFO: iter:  8040/144360  CE: 6.0056  
[05/07 04:00:17] SuperNet Training INFO: iter:  8160/144360  CE: 5.9189  
[05/07 04:01:18] SuperNet Training INFO: iter:  8280/144360  CE: 5.9198  
[05/07 04:02:18] SuperNet Training INFO: iter:  8400/144360  CE: 5.8534  
[05/07 04:02:28] SuperNet Training INFO: --> epoch:   7/120  avg CE: 5.9980  lr: 0.11899529445383715  
[05/07 04:03:56] SuperNet Training INFO: iter:  8520/144360  CE: 5.8325  
[05/07 04:04:58] SuperNet Training INFO: iter:  8640/144360  CE: 5.9285  
[05/07 04:06:01] SuperNet Training INFO: iter:  8760/144360  CE: 5.8572  
[05/07 04:07:01] SuperNet Training INFO: iter:  8880/144360  CE: 5.9147  
[05/07 04:08:02] SuperNet Training INFO: iter:  9000/144360  CE: 5.7916  
[05/07 04:09:05] SuperNet Training INFO: iter:  9120/144360  CE: 5.6005  
[05/07 04:10:06] SuperNet Training INFO: iter:  9240/144360  CE: 5.6225  
[05/07 04:11:08] SuperNet Training INFO: iter:  9360/144360  CE: 5.7026  
[05/07 04:12:09] SuperNet Training INFO: iter:  9480/144360  CE: 5.8552  
[05/07 04:13:11] SuperNet Training INFO: iter:  9600/144360  CE: 5.7855  
[05/07 04:13:22] SuperNet Training INFO: --> epoch:   8/120  avg CE: 5.8589  lr: 0.11868885604402826  
[05/07 04:14:48] SuperNet Training INFO: iter:  9720/144360  CE: 5.7982  
[05/07 04:15:48] SuperNet Training INFO: iter:  9840/144360  CE: 5.9435  
[05/07 04:16:48] SuperNet Training INFO: iter:  9960/144360  CE: 5.7858  
[05/07 04:17:49] SuperNet Training INFO: iter: 10080/144360  CE: 5.8224  
[05/07 04:18:51] SuperNet Training INFO: iter: 10200/144360  CE: 5.8208  
[05/07 04:19:52] SuperNet Training INFO: iter: 10320/144360  CE: 5.6322  
[05/07 04:20:51] SuperNet Training INFO: iter: 10440/144360  CE: 5.8422  
[05/07 04:21:52] SuperNet Training INFO: iter: 10560/144360  CE: 5.4767  
[05/07 04:22:53] SuperNet Training INFO: iter: 10680/144360  CE: 5.8250  
[05/07 04:23:52] SuperNet Training INFO: iter: 10800/144360  CE: 5.5547  
[05/07 04:24:04] SuperNet Training INFO: --> epoch:   9/120  avg CE: 5.7263  lr: 0.11834219522386061  
[05/07 04:25:29] SuperNet Training INFO: iter: 10920/144360  CE: 5.5469  
[05/07 04:26:29] SuperNet Training INFO: iter: 11040/144360  CE: 5.4232  
[05/07 04:27:29] SuperNet Training INFO: iter: 11160/144360  CE: 5.5704  
[05/07 04:28:28] SuperNet Training INFO: iter: 11280/144360  CE: 5.5235  
[05/07 04:29:27] SuperNet Training INFO: iter: 11400/144360  CE: 5.5145  
[05/07 04:30:27] SuperNet Training INFO: iter: 11520/144360  CE: 5.6718  
[05/07 04:31:27] SuperNet Training INFO: iter: 11640/144360  CE: 5.5495  
[05/07 04:32:28] SuperNet Training INFO: iter: 11760/144360  CE: 5.5452  
[05/07 04:33:29] SuperNet Training INFO: iter: 11880/144360  CE: 5.4002  
[05/07 04:34:32] SuperNet Training INFO: iter: 12000/144360  CE: 5.5041  
[05/07 04:34:46] SuperNet Training INFO: --> epoch:  10/120  avg CE: 5.5998  lr: 0.1179555495773443  
[05/07 04:36:09] SuperNet Training INFO: iter: 12120/144360  CE: 5.5047  
[05/07 04:37:10] SuperNet Training INFO: iter: 12240/144360  CE: 5.2388  
[05/07 04:38:11] SuperNet Training INFO: iter: 12360/144360  CE: 5.3447  
[05/07 04:39:13] SuperNet Training INFO: iter: 12480/144360  CE: 5.6880  
[05/07 04:40:16] SuperNet Training INFO: iter: 12600/144360  CE: 5.2733  
[05/07 04:41:17] SuperNet Training INFO: iter: 12720/144360  CE: 5.7331  
[05/07 04:42:18] SuperNet Training INFO: iter: 12840/144360  CE: 5.3409  
[05/07 04:43:18] SuperNet Training INFO: iter: 12960/144360  CE: 5.3905  
[05/07 04:44:18] SuperNet Training INFO: iter: 13080/144360  CE: 5.4831  
[05/07 04:45:19] SuperNet Training INFO: iter: 13200/144360  CE: 5.4334  
[05/07 04:45:34] SuperNet Training INFO: --> epoch:  11/120  avg CE: 5.4645  lr: 0.11752918409209158  
[05/07 04:46:57] SuperNet Training INFO: iter: 13320/144360  CE: 5.3118  
[05/07 04:47:59] SuperNet Training INFO: iter: 13440/144360  CE: 5.0281  
[05/07 04:49:00] SuperNet Training INFO: iter: 13560/144360  CE: 5.3714  
[05/07 04:50:00] SuperNet Training INFO: iter: 13680/144360  CE: 5.2961  
[05/07 04:51:00] SuperNet Training INFO: iter: 13800/144360  CE: 5.2001  
[05/07 04:51:59] SuperNet Training INFO: iter: 13920/144360  CE: 5.2723  
[05/07 04:52:58] SuperNet Training INFO: iter: 14040/144360  CE: 5.2335  
[05/07 04:53:57] SuperNet Training INFO: iter: 14160/144360  CE: 5.2670  
[05/07 04:54:58] SuperNet Training INFO: iter: 14280/144360  CE: 5.0793  
[05/07 04:55:59] SuperNet Training INFO: iter: 14400/144360  CE: 5.0887  
[05/07 04:56:15] SuperNet Training INFO: --> epoch:  12/120  avg CE: 5.3466  lr: 0.11706339097770935  
[05/07 04:57:36] SuperNet Training INFO: iter: 14520/144360  CE: 5.2304  
[05/07 04:58:38] SuperNet Training INFO: iter: 14640/144360  CE: 5.0973  
[05/07 04:59:40] SuperNet Training INFO: iter: 14760/144360  CE: 5.1386  
[05/07 05:00:41] SuperNet Training INFO: iter: 14880/144360  CE: 5.2613  
[05/07 05:01:42] SuperNet Training INFO: iter: 15000/144360  CE: 5.4123  
[05/07 05:02:44] SuperNet Training INFO: iter: 15120/144360  CE: 5.0000  
[05/07 05:03:45] SuperNet Training INFO: iter: 15240/144360  CE: 5.0376  
[05/07 05:04:44] SuperNet Training INFO: iter: 15360/144360  CE: 5.0138  
[05/07 05:05:43] SuperNet Training INFO: iter: 15480/144360  CE: 4.9630  
[05/07 05:06:44] SuperNet Training INFO: iter: 15600/144360  CE: 5.0035  
[05/07 05:07:01] SuperNet Training INFO: --> epoch:  13/120  avg CE: 5.2184  lr: 0.11655848946553125  
[05/07 05:08:22] SuperNet Training INFO: iter: 15720/144360  CE: 5.0963  
[05/07 05:09:23] SuperNet Training INFO: iter: 15840/144360  CE: 4.9090  
[05/07 05:10:25] SuperNet Training INFO: iter: 15960/144360  CE: 5.0442  
[05/07 05:11:25] SuperNet Training INFO: iter: 16080/144360  CE: 4.9658  
[05/07 05:12:26] SuperNet Training INFO: iter: 16200/144360  CE: 5.1951  
[05/07 05:13:27] SuperNet Training INFO: iter: 16320/144360  CE: 5.5175  
[05/07 05:14:28] SuperNet Training INFO: iter: 16440/144360  CE: 5.0842  
[05/07 05:15:29] SuperNet Training INFO: iter: 16560/144360  CE: 5.2318  
[05/07 05:16:30] SuperNet Training INFO: iter: 16680/144360  CE: 5.2168  
[05/07 05:17:33] SuperNet Training INFO: iter: 16800/144360  CE: 4.9243  
[05/07 05:17:52] SuperNet Training INFO: --> epoch:  14/120  avg CE: 5.1046  lr: 0.11601482558983225  
[05/07 05:19:11] SuperNet Training INFO: iter: 16920/144360  CE: 4.9262  
[05/07 05:20:13] SuperNet Training INFO: iter: 17040/144360  CE: 5.0731  
[05/07 05:21:15] SuperNet Training INFO: iter: 17160/144360  CE: 4.8756  
[05/07 05:22:17] SuperNet Training INFO: iter: 17280/144360  CE: 4.8231  
[05/07 05:23:19] SuperNet Training INFO: iter: 17400/144360  CE: 5.2052  
[05/07 05:24:20] SuperNet Training INFO: iter: 17520/144360  CE: 4.8392  
[05/07 05:25:22] SuperNet Training INFO: iter: 17640/144360  CE: 4.8292  
[05/07 05:26:24] SuperNet Training INFO: iter: 17760/144360  CE: 4.9342  
[05/07 05:27:26] SuperNet Training INFO: iter: 17880/144360  CE: 4.9024  
[05/07 05:28:26] SuperNet Training INFO: iter: 18000/144360  CE: 4.9441  
[05/07 05:28:48] SuperNet Training INFO: --> epoch:  15/120  avg CE: 4.9942  lr: 0.11543277195067722  
[05/07 05:30:03] SuperNet Training INFO: iter: 18120/144360  CE: 4.6212  
[05/07 05:31:04] SuperNet Training INFO: iter: 18240/144360  CE: 4.8905  
[05/07 05:32:04] SuperNet Training INFO: iter: 18360/144360  CE: 4.8805  
[05/07 05:33:04] SuperNet Training INFO: iter: 18480/144360  CE: 5.0089  
[05/07 05:34:03] SuperNet Training INFO: iter: 18600/144360  CE: 5.1391  
[05/07 05:35:02] SuperNet Training INFO: iter: 18720/144360  CE: 4.8245  
[05/07 05:36:03] SuperNet Training INFO: iter: 18840/144360  CE: 5.0824  
[05/07 05:37:03] SuperNet Training INFO: iter: 18960/144360  CE: 4.7239  
[05/07 05:38:01] SuperNet Training INFO: iter: 19080/144360  CE: 4.9634  
[05/07 05:39:00] SuperNet Training INFO: iter: 19200/144360  CE: 4.8564  
[05/07 05:39:23] SuperNet Training INFO: --> epoch:  16/120  avg CE: 4.8964  lr: 0.1148127274585561  
[05/07 05:40:37] SuperNet Training INFO: iter: 19320/144360  CE: 4.7478  
[05/07 05:41:39] SuperNet Training INFO: iter: 19440/144360  CE: 4.6700  
[05/07 05:42:38] SuperNet Training INFO: iter: 19560/144360  CE: 4.9725  
[05/07 05:43:38] SuperNet Training INFO: iter: 19680/144360  CE: 4.7377  
[05/07 05:44:38] SuperNet Training INFO: iter: 19800/144360  CE: 4.6465  
[05/07 05:45:39] SuperNet Training INFO: iter: 19920/144360  CE: 4.5804  
[05/07 05:46:39] SuperNet Training INFO: iter: 20040/144360  CE: 4.7785  
[05/07 05:47:40] SuperNet Training INFO: iter: 20160/144360  CE: 4.8027  
[05/07 05:48:41] SuperNet Training INFO: iter: 20280/144360  CE: 4.8518  
[05/07 05:49:44] SuperNet Training INFO: iter: 20400/144360  CE: 4.7467  
[05/07 05:50:09] SuperNet Training INFO: --> epoch:  17/120  avg CE: 4.8027  lr: 0.11415511706099139  
[05/07 05:51:22] SuperNet Training INFO: iter: 20520/144360  CE: 4.7541  
[05/07 05:52:24] SuperNet Training INFO: iter: 20640/144360  CE: 4.6257  
[05/07 05:53:26] SuperNet Training INFO: iter: 20760/144360  CE: 4.5161  
[05/07 05:54:27] SuperNet Training INFO: iter: 20880/144360  CE: 4.9627  
[05/07 05:55:27] SuperNet Training INFO: iter: 21000/144360  CE: 4.6888  
[05/07 05:56:28] SuperNet Training INFO: iter: 21120/144360  CE: 5.0950  
[05/07 05:57:28] SuperNet Training INFO: iter: 21240/144360  CE: 4.6610  
[05/07 05:58:30] SuperNet Training INFO: iter: 21360/144360  CE: 4.6121  
[05/07 05:59:31] SuperNet Training INFO: iter: 21480/144360  CE: 4.4853  
[05/07 06:00:32] SuperNet Training INFO: iter: 21600/144360  CE: 4.8444  
[05/07 06:00:59] SuperNet Training INFO: --> epoch:  18/120  avg CE: 4.7197  lr: 0.11346039145130195  
[05/07 06:02:12] SuperNet Training INFO: iter: 21720/144360  CE: 4.6676  
[05/07 06:03:12] SuperNet Training INFO: iter: 21840/144360  CE: 4.6572  
[05/07 06:04:13] SuperNet Training INFO: iter: 21960/144360  CE: 4.6805  
[05/07 06:05:14] SuperNet Training INFO: iter: 22080/144360  CE: 4.5506  
[05/07 06:06:15] SuperNet Training INFO: iter: 22200/144360  CE: 4.5355  
[05/07 06:07:15] SuperNet Training INFO: iter: 22320/144360  CE: 4.4772  
[05/07 06:08:16] SuperNet Training INFO: iter: 22440/144360  CE: 4.9478  
[05/07 06:09:15] SuperNet Training INFO: iter: 22560/144360  CE: 4.6578  
[05/07 06:10:15] SuperNet Training INFO: iter: 22680/144360  CE: 4.4557  
[05/07 06:11:15] SuperNet Training INFO: iter: 22800/144360  CE: 4.6383  
[05/07 06:11:42] SuperNet Training INFO: --> epoch:  19/120  avg CE: 4.6418  lr: 0.11272902675971772  
[05/07 06:12:52] SuperNet Training INFO: iter: 22920/144360  CE: 4.7345  
[05/07 06:13:54] SuperNet Training INFO: iter: 23040/144360  CE: 4.3749  
[05/07 06:14:54] SuperNet Training INFO: iter: 23160/144360  CE: 4.7041  
[05/07 06:15:54] SuperNet Training INFO: iter: 23280/144360  CE: 4.6802  
[05/07 06:16:54] SuperNet Training INFO: iter: 23400/144360  CE: 4.7038  
[05/07 06:17:54] SuperNet Training INFO: iter: 23520/144360  CE: 4.6884  
[05/07 06:18:54] SuperNet Training INFO: iter: 23640/144360  CE: 4.8776  
[05/07 06:19:54] SuperNet Training INFO: iter: 23760/144360  CE: 4.5359  
[05/07 06:20:54] SuperNet Training INFO: iter: 23880/144360  CE: 4.7494  
[05/07 06:21:53] SuperNet Training INFO: iter: 24000/144360  CE: 4.5557  
[05/07 06:22:23] SuperNet Training INFO: --> epoch:  20/120  avg CE: 4.5809  lr: 0.11196152422706572  
[05/07 06:23:31] SuperNet Training INFO: iter: 24120/144360  CE: 4.5060  
[05/07 06:24:32] SuperNet Training INFO: iter: 24240/144360  CE: 4.9762  
[05/07 06:25:32] SuperNet Training INFO: iter: 24360/144360  CE: 4.6054  
[05/07 06:26:33] SuperNet Training INFO: iter: 24480/144360  CE: 4.2328  
[05/07 06:27:34] SuperNet Training INFO: iter: 24600/144360  CE: 4.4983  
[05/07 06:28:34] SuperNet Training INFO: iter: 24720/144360  CE: 4.2275  
[05/07 06:29:35] SuperNet Training INFO: iter: 24840/144360  CE: 4.6387  
[05/07 06:30:37] SuperNet Training INFO: iter: 24960/144360  CE: 4.4209  
[05/07 06:31:38] SuperNet Training INFO: iter: 25080/144360  CE: 4.3040  
[05/07 06:32:39] SuperNet Training INFO: iter: 25200/144360  CE: 4.3317  
[05/07 06:33:10] SuperNet Training INFO: --> epoch:  21/120  avg CE: 4.5114  lr: 0.11115840986124514  
[05/07 06:34:16] SuperNet Training INFO: iter: 25320/144360  CE: 4.4885  
[05/07 06:35:17] SuperNet Training INFO: iter: 25440/144360  CE: 4.5867  
[05/07 06:36:17] SuperNet Training INFO: iter: 25560/144360  CE: 4.8230  
[05/07 06:37:19] SuperNet Training INFO: iter: 25680/144360  CE: 4.3899  
[05/07 06:38:19] SuperNet Training INFO: iter: 25800/144360  CE: 4.4115  
[05/07 06:39:18] SuperNet Training INFO: iter: 25920/144360  CE: 4.5820  
[05/07 06:40:19] SuperNet Training INFO: iter: 26040/144360  CE: 4.5139  
[05/07 06:41:20] SuperNet Training INFO: iter: 26160/144360  CE: 4.5634  
[05/07 06:42:19] SuperNet Training INFO: iter: 26280/144360  CE: 4.7168  
[05/07 06:43:19] SuperNet Training INFO: iter: 26400/144360  CE: 4.0852  
[05/07 06:43:50] SuperNet Training INFO: --> epoch:  22/120  avg CE: 4.4537  lr: 0.11032023407672516  
[05/07 06:44:56] SuperNet Training INFO: iter: 26520/144360  CE: 4.1654  
[05/07 06:45:58] SuperNet Training INFO: iter: 26640/144360  CE: 4.4233  
[05/07 06:47:01] SuperNet Training INFO: iter: 26760/144360  CE: 4.4930  
[05/07 06:48:02] SuperNet Training INFO: iter: 26880/144360  CE: 4.3446  
[05/07 06:49:04] SuperNet Training INFO: iter: 27000/144360  CE: 4.0326  
[05/07 06:50:06] SuperNet Training INFO: iter: 27120/144360  CE: 4.5022  
[05/07 06:51:08] SuperNet Training INFO: iter: 27240/144360  CE: 4.2821  
[05/07 06:52:08] SuperNet Training INFO: iter: 27360/144360  CE: 4.3022  
[05/07 06:53:08] SuperNet Training INFO: iter: 27480/144360  CE: 4.3338  
[05/07 06:54:07] SuperNet Training INFO: iter: 27600/144360  CE: 4.7848  
[05/07 06:54:40] SuperNet Training INFO: --> epoch:  23/120  avg CE: 4.3981  lr: 0.10944757131732062  
[05/07 06:55:44] SuperNet Training INFO: iter: 27720/144360  CE: 4.4442  
[05/07 06:56:45] SuperNet Training INFO: iter: 27840/144360  CE: 4.4394  
[05/07 06:57:46] SuperNet Training INFO: iter: 27960/144360  CE: 4.2846  
[05/07 06:58:48] SuperNet Training INFO: iter: 28080/144360  CE: 4.2110  
[05/07 06:59:50] SuperNet Training INFO: iter: 28200/144360  CE: 4.4165  
[05/07 07:00:51] SuperNet Training INFO: iter: 28320/144360  CE: 4.1992  
[05/07 07:01:51] SuperNet Training INFO: iter: 28440/144360  CE: 4.5072  
[05/07 07:02:50] SuperNet Training INFO: iter: 28560/144360  CE: 4.1081  
[05/07 07:03:50] SuperNet Training INFO: iter: 28680/144360  CE: 4.3187  
[05/07 07:04:49] SuperNet Training INFO: iter: 28800/144360  CE: 4.2218  
[05/07 07:05:25] SuperNet Training INFO: --> epoch:  24/120  avg CE: 4.3505  lr: 0.10854101966249682  
[05/07 07:06:28] SuperNet Training INFO: iter: 28920/144360  CE: 4.3432  
[05/07 07:07:29] SuperNet Training INFO: iter: 29040/144360  CE: 4.3118  
[05/07 07:08:30] SuperNet Training INFO: iter: 29160/144360  CE: 4.5814  
[05/07 07:09:34] SuperNet Training INFO: iter: 29280/144360  CE: 4.4008  
[05/07 07:10:35] SuperNet Training INFO: iter: 29400/144360  CE: 4.3891  
[05/07 07:11:36] SuperNet Training INFO: iter: 29520/144360  CE: 4.0446  
[05/07 07:12:38] SuperNet Training INFO: iter: 29640/144360  CE: 4.1523  
[05/07 07:13:39] SuperNet Training INFO: iter: 29760/144360  CE: 4.4613  
[05/07 07:14:40] SuperNet Training INFO: iter: 29880/144360  CE: 4.2562  
[05/07 07:15:41] SuperNet Training INFO: iter: 30000/144360  CE: 4.1488  
[05/07 07:16:18] SuperNet Training INFO: --> epoch:  25/120  avg CE: 4.3065  lr: 0.10760120041747454  
[05/07 07:17:18] SuperNet Training INFO: iter: 30120/144360  CE: 4.3309  
[05/07 07:18:19] SuperNet Training INFO: iter: 30240/144360  CE: 3.9677  
[05/07 07:19:21] SuperNet Training INFO: iter: 30360/144360  CE: 4.2546  
[05/07 07:20:22] SuperNet Training INFO: iter: 30480/144360  CE: 4.2473  
[05/07 07:21:25] SuperNet Training INFO: iter: 30600/144360  CE: 4.2212  
[05/07 07:22:27] SuperNet Training INFO: iter: 30720/144360  CE: 4.0883  
[05/07 07:23:27] SuperNet Training INFO: iter: 30840/144360  CE: 4.2753  
[05/07 07:24:28] SuperNet Training INFO: iter: 30960/144360  CE: 4.1569  
[05/07 07:25:29] SuperNet Training INFO: iter: 31080/144360  CE: 4.1112  
[05/07 07:26:29] SuperNet Training INFO: iter: 31200/144360  CE: 4.2659  
[05/07 07:27:07] SuperNet Training INFO: --> epoch:  26/120  avg CE: 4.2549  lr: 0.10662875768741867  
[05/07 07:28:06] SuperNet Training INFO: iter: 31320/144360  CE: 4.1452  
[05/07 07:29:08] SuperNet Training INFO: iter: 31440/144360  CE: 4.4151  
[05/07 07:30:09] SuperNet Training INFO: iter: 31560/144360  CE: 4.1661  
[05/07 07:31:09] SuperNet Training INFO: iter: 31680/144360  CE: 4.1274  
[05/07 07:32:10] SuperNet Training INFO: iter: 31800/144360  CE: 4.1411  
[05/07 07:33:11] SuperNet Training INFO: iter: 31920/144360  CE: 4.3812  
[05/07 07:34:13] SuperNet Training INFO: iter: 32040/144360  CE: 4.3294  
[05/07 07:35:15] SuperNet Training INFO: iter: 32160/144360  CE: 4.1274  
[05/07 07:36:14] SuperNet Training INFO: iter: 32280/144360  CE: 4.2484  
[05/07 07:37:15] SuperNet Training INFO: iter: 32400/144360  CE: 4.2329  
[05/07 07:37:54] SuperNet Training INFO: --> epoch:  27/120  avg CE: 4.2158  lr: 0.10562435793600224  
[05/07 07:38:53] SuperNet Training INFO: iter: 32520/144360  CE: 4.3180  
[05/07 07:39:54] SuperNet Training INFO: iter: 32640/144360  CE: 4.3111  
[05/07 07:40:55] SuperNet Training INFO: iter: 32760/144360  CE: 4.1721  
[05/07 07:41:57] SuperNet Training INFO: iter: 32880/144360  CE: 4.0991  
[05/07 07:42:58] SuperNet Training INFO: iter: 33000/144360  CE: 4.0889  
[05/07 07:43:59] SuperNet Training INFO: iter: 33120/144360  CE: 4.1675  
[05/07 07:45:00] SuperNet Training INFO: iter: 33240/144360  CE: 4.2623  
[05/07 07:45:59] SuperNet Training INFO: iter: 33360/144360  CE: 4.2935  
[05/07 07:46:58] SuperNet Training INFO: iter: 33480/144360  CE: 4.0592  
[05/07 07:47:56] SuperNet Training INFO: iter: 33600/144360  CE: 3.9222  
[05/07 07:48:37] SuperNet Training INFO: --> epoch:  28/120  avg CE: 4.1756  lr: 0.10458868952864393  
[05/07 07:49:33] SuperNet Training INFO: iter: 33720/144360  CE: 4.2503  
[05/07 07:50:34] SuperNet Training INFO: iter: 33840/144360  CE: 4.0432  
[05/07 07:51:36] SuperNet Training INFO: iter: 33960/144360  CE: 3.9845  
[05/07 07:52:38] SuperNet Training INFO: iter: 34080/144360  CE: 4.1793  
[05/07 07:53:39] SuperNet Training INFO: iter: 34200/144360  CE: 3.8499  
[05/07 07:54:41] SuperNet Training INFO: iter: 34320/144360  CE: 4.3155  
[05/07 07:55:40] SuperNet Training INFO: iter: 34440/144360  CE: 4.1575  
[05/07 07:56:41] SuperNet Training INFO: iter: 34560/144360  CE: 4.0618  
[05/07 07:57:41] SuperNet Training INFO: iter: 34680/144360  CE: 4.1288  
[05/07 07:58:40] SuperNet Training INFO: iter: 34800/144360  CE: 3.9804  
[05/07 07:59:22] SuperNet Training INFO: --> epoch:  29/120  avg CE: 4.1508  lr: 0.10352246226073762  
[05/07 08:00:18] SuperNet Training INFO: iter: 34920/144360  CE: 4.0395  
[05/07 08:01:19] SuperNet Training INFO: iter: 35040/144360  CE: 4.0546  
[05/07 08:02:20] SuperNet Training INFO: iter: 35160/144360  CE: 4.3931  
[05/07 08:03:21] SuperNet Training INFO: iter: 35280/144360  CE: 3.7514  
[05/07 08:04:22] SuperNet Training INFO: iter: 35400/144360  CE: 3.8911  
[05/07 08:05:23] SuperNet Training INFO: iter: 35520/144360  CE: 4.0363  
[05/07 08:06:24] SuperNet Training INFO: iter: 35640/144360  CE: 4.3627  
[05/07 08:07:26] SuperNet Training INFO: iter: 35760/144360  CE: 4.1549  
[05/07 08:08:27] SuperNet Training INFO: iter: 35880/144360  CE: 4.0085  
[05/07 08:09:28] SuperNet Training INFO: iter: 36000/144360  CE: 3.9271  
[05/07 08:10:14] SuperNet Training INFO: --> epoch:  30/120  avg CE: 4.1095  lr: 0.10242640687119343  
[05/07 08:11:07] SuperNet Training INFO: iter: 36120/144360  CE: 4.0404  
[05/07 08:12:10] SuperNet Training INFO: iter: 36240/144360  CE: 4.3089  
[05/07 08:13:12] SuperNet Training INFO: iter: 36360/144360  CE: 3.8746  
[05/07 08:14:15] SuperNet Training INFO: iter: 36480/144360  CE: 4.2956  
[05/07 08:15:17] SuperNet Training INFO: iter: 36600/144360  CE: 4.1978  
[05/07 08:16:17] SuperNet Training INFO: iter: 36720/144360  CE: 4.1421  
[05/07 08:17:18] SuperNet Training INFO: iter: 36840/144360  CE: 3.9934  
[05/07 08:18:20] SuperNet Training INFO: iter: 36960/144360  CE: 4.2041  
[05/07 08:19:21] SuperNet Training INFO: iter: 37080/144360  CE: 4.1558  
[05/07 08:20:22] SuperNet Training INFO: iter: 37200/144360  CE: 3.7008  
[05/07 08:21:08] SuperNet Training INFO: --> epoch:  31/120  avg CE: 4.0858  lr: 0.10130127454162571  
[05/07 08:21:59] SuperNet Training INFO: iter: 37320/144360  CE: 3.9944  
[05/07 08:23:00] SuperNet Training INFO: iter: 37440/144360  CE: 3.9232  
[05/07 08:24:02] SuperNet Training INFO: iter: 37560/144360  CE: 4.3065  
[05/07 08:25:03] SuperNet Training INFO: iter: 37680/144360  CE: 4.1988  
[05/07 08:26:06] SuperNet Training INFO: iter: 37800/144360  CE: 3.9351  
[05/07 08:27:07] SuperNet Training INFO: iter: 37920/144360  CE: 3.9386  
[05/07 08:28:08] SuperNet Training INFO: iter: 38040/144360  CE: 4.0317  
[05/07 08:29:08] SuperNet Training INFO: iter: 38160/144360  CE: 3.9297  
[05/07 08:30:08] SuperNet Training INFO: iter: 38280/144360  CE: 4.2356  
[05/07 08:31:09] SuperNet Training INFO: iter: 38400/144360  CE: 3.8765  
[05/07 08:31:56] SuperNet Training INFO: --> epoch:  32/120  avg CE: 4.0654  lr: 0.10014783638153192  
[05/07 08:32:46] SuperNet Training INFO: iter: 38520/144360  CE: 3.9215  
[05/07 08:33:47] SuperNet Training INFO: iter: 38640/144360  CE: 4.4884  
[05/07 08:34:48] SuperNet Training INFO: iter: 38760/144360  CE: 4.0019  
[05/07 08:35:50] SuperNet Training INFO: iter: 38880/144360  CE: 4.0930  
[05/07 08:36:52] SuperNet Training INFO: iter: 39000/144360  CE: 4.1577  
[05/07 08:37:52] SuperNet Training INFO: iter: 39120/144360  CE: 3.7375  
[05/07 08:38:54] SuperNet Training INFO: iter: 39240/144360  CE: 3.9743  
[05/07 08:39:56] SuperNet Training INFO: iter: 39360/144360  CE: 4.1813  
[05/07 08:40:59] SuperNet Training INFO: iter: 39480/144360  CE: 3.9354  
[05/07 08:42:00] SuperNet Training INFO: iter: 39600/144360  CE: 4.0251  
[05/07 08:42:49] SuperNet Training INFO: --> epoch:  33/120  avg CE: 4.0355  lr: 0.09896688289981138  
[05/07 08:43:37] SuperNet Training INFO: iter: 39720/144360  CE: 4.0697  
[05/07 08:44:38] SuperNet Training INFO: iter: 39840/144360  CE: 3.8846  
[05/07 08:45:39] SuperNet Training INFO: iter: 39960/144360  CE: 4.2534  
[05/07 08:46:40] SuperNet Training INFO: iter: 40080/144360  CE: 3.5875  
[05/07 08:47:41] SuperNet Training INFO: iter: 40200/144360  CE: 4.3694  
[05/07 08:48:42] SuperNet Training INFO: iter: 40320/144360  CE: 3.9933  
[05/07 08:49:42] SuperNet Training INFO: iter: 40440/144360  CE: 3.9683  
[05/07 08:50:44] SuperNet Training INFO: iter: 40560/144360  CE: 4.1900  
[05/07 08:51:44] SuperNet Training INFO: iter: 40680/144360  CE: 3.8664  
[05/07 08:52:45] SuperNet Training INFO: iter: 40800/144360  CE: 4.1108  
[05/07 08:53:36] SuperNet Training INFO: --> epoch:  34/120  avg CE: 4.0036  lr: 0.09775922346299062  
[05/07 08:54:23] SuperNet Training INFO: iter: 40920/144360  CE: 4.2364  
[05/07 08:55:26] SuperNet Training INFO: iter: 41040/144360  CE: 3.7082  
[05/07 08:56:28] SuperNet Training INFO: iter: 41160/144360  CE: 3.7856  
[05/07 08:57:29] SuperNet Training INFO: iter: 41280/144360  CE: 3.8726  
[05/07 08:58:32] SuperNet Training INFO: iter: 41400/144360  CE: 4.1233  
[05/07 08:59:34] SuperNet Training INFO: iter: 41520/144360  CE: 3.8867  
[05/07 09:00:36] SuperNet Training INFO: iter: 41640/144360  CE: 4.1069  
[05/07 09:01:36] SuperNet Training INFO: iter: 41760/144360  CE: 3.9605  
[05/07 09:02:37] SuperNet Training INFO: iter: 41880/144360  CE: 4.0033  
[05/07 09:03:37] SuperNet Training INFO: iter: 42000/144360  CE: 4.3305  
[05/07 09:04:29] SuperNet Training INFO: --> epoch:  35/120  avg CE: 3.9782  lr: 0.09652568574052359  
[05/07 09:05:14] SuperNet Training INFO: iter: 42120/144360  CE: 4.0754  
[05/07 09:06:15] SuperNet Training INFO: iter: 42240/144360  CE: 3.9522  
[05/07 09:07:17] SuperNet Training INFO: iter: 42360/144360  CE: 3.9491  
[05/07 09:08:17] SuperNet Training INFO: iter: 42480/144360  CE: 4.1287  
[05/07 09:09:19] SuperNet Training INFO: iter: 42600/144360  CE: 3.8615  
[05/07 09:10:21] SuperNet Training INFO: iter: 42720/144360  CE: 4.1036  
[05/07 09:11:22] SuperNet Training INFO: iter: 42840/144360  CE: 4.0388  
[05/07 09:12:24] SuperNet Training INFO: iter: 42960/144360  CE: 4.0548  
[05/07 09:13:27] SuperNet Training INFO: iter: 43080/144360  CE: 3.7680  
[05/07 09:14:28] SuperNet Training INFO: iter: 43200/144360  CE: 3.9645  
[05/07 09:15:21] SuperNet Training INFO: --> epoch:  36/120  avg CE: 3.9564  lr: 0.09526711513754865  
[05/07 09:16:05] SuperNet Training INFO: iter: 43320/144360  CE: 3.8765  
[05/07 09:17:04] SuperNet Training INFO: iter: 43440/144360  CE: 4.2194  
[05/07 09:18:04] SuperNet Training INFO: iter: 43560/144360  CE: 4.0904  
[05/07 09:19:05] SuperNet Training INFO: iter: 43680/144360  CE: 4.2163  
[05/07 09:20:06] SuperNet Training INFO: iter: 43800/144360  CE: 3.6047  
[05/07 09:21:08] SuperNet Training INFO: iter: 43920/144360  CE: 4.0003  
[05/07 09:22:08] SuperNet Training INFO: iter: 44040/144360  CE: 3.9347  
[05/07 09:23:09] SuperNet Training INFO: iter: 44160/144360  CE: 3.7480  
[05/07 09:24:09] SuperNet Training INFO: iter: 44280/144360  CE: 3.6796  
[05/07 09:25:10] SuperNet Training INFO: iter: 44400/144360  CE: 4.0565  
[05/07 09:26:06] SuperNet Training INFO: --> epoch:  37/120  avg CE: 3.9177  lr: 0.0939843742154901  
[05/07 09:26:49] SuperNet Training INFO: iter: 44520/144360  CE: 4.1676  
[05/07 09:27:50] SuperNet Training INFO: iter: 44640/144360  CE: 3.8844  
[05/07 09:28:51] SuperNet Training INFO: iter: 44760/144360  CE: 3.9595  
[05/07 09:29:52] SuperNet Training INFO: iter: 44880/144360  CE: 3.5933  
[05/07 09:30:52] SuperNet Training INFO: iter: 45000/144360  CE: 4.1513  
[05/07 09:31:52] SuperNet Training INFO: iter: 45120/144360  CE: 4.0771  
[05/07 09:32:54] SuperNet Training INFO: iter: 45240/144360  CE: 4.1082  
[05/07 09:33:55] SuperNet Training INFO: iter: 45360/144360  CE: 4.0879  
[05/07 09:34:55] SuperNet Training INFO: iter: 45480/144360  CE: 4.0183  
[05/07 09:35:55] SuperNet Training INFO: iter: 45600/144360  CE: 4.1524  
[05/07 09:36:50] SuperNet Training INFO: --> epoch:  38/120  avg CE: 3.9008  lr: 0.0926783421009017  
[05/07 09:37:31] SuperNet Training INFO: iter: 45720/144360  CE: 3.7418  
[05/07 09:38:34] SuperNet Training INFO: iter: 45840/144360  CE: 4.0425  
[05/07 09:39:36] SuperNet Training INFO: iter: 45960/144360  CE: 3.8074  
[05/07 09:40:38] SuperNet Training INFO: iter: 46080/144360  CE: 3.8377  
[05/07 09:41:39] SuperNet Training INFO: iter: 46200/144360  CE: 4.0673  
[05/07 09:42:42] SuperNet Training INFO: iter: 46320/144360  CE: 3.5104  
[05/07 09:43:44] SuperNet Training INFO: iter: 46440/144360  CE: 3.6881  
[05/07 09:44:46] SuperNet Training INFO: iter: 46560/144360  CE: 4.0688  
[05/07 09:45:48] SuperNet Training INFO: iter: 46680/144360  CE: 3.9089  
[05/07 09:46:50] SuperNet Training INFO: iter: 46800/144360  CE: 3.7600  
[05/07 09:47:51] SuperNet Training INFO: --> epoch:  39/120  avg CE: 3.8859  lr: 0.09134991388295689  
[05/07 09:48:30] SuperNet Training INFO: iter: 46920/144360  CE: 3.7830  
[05/07 09:49:31] SuperNet Training INFO: iter: 47040/144360  CE: 3.9012  
[05/07 09:50:32] SuperNet Training INFO: iter: 47160/144360  CE: 3.9349  
[05/07 09:51:32] SuperNet Training INFO: iter: 47280/144360  CE: 4.1174  
[05/07 09:52:31] SuperNet Training INFO: iter: 47400/144360  CE: 3.9903  
[05/07 09:53:31] SuperNet Training INFO: iter: 47520/144360  CE: 3.7985  
[05/07 09:54:29] SuperNet Training INFO: iter: 47640/144360  CE: 3.8626  
[05/07 09:55:28] SuperNet Training INFO: iter: 47760/144360  CE: 3.3111  
[05/07 09:56:27] SuperNet Training INFO: iter: 47880/144360  CE: 3.6761  
[05/07 09:57:26] SuperNet Training INFO: iter: 48000/144360  CE: 3.7619  
[05/07 09:58:24] SuperNet Training INFO: iter: 48120/144360  CE: 3.9844  
[05/07 09:58:24] SuperNet Training INFO: --> epoch:  40/120  avg CE: 3.8532  lr: 0.0899999999999998  
[05/07 10:00:02] SuperNet Training INFO: iter: 48240/144360  CE: 3.3587  
[05/07 10:01:02] SuperNet Training INFO: iter: 48360/144360  CE: 3.7971  
[05/07 10:02:04] SuperNet Training INFO: iter: 48480/144360  CE: 3.5801  
[05/07 10:03:04] SuperNet Training INFO: iter: 48600/144360  CE: 3.6949  
[05/07 10:04:04] SuperNet Training INFO: iter: 48720/144360  CE: 4.0347  
[05/07 10:05:04] SuperNet Training INFO: iter: 48840/144360  CE: 3.9290  
[05/07 10:06:06] SuperNet Training INFO: iter: 48960/144360  CE: 3.7128  
[05/07 10:07:07] SuperNet Training INFO: iter: 49080/144360  CE: 3.8937  
[05/07 10:08:08] SuperNet Training INFO: iter: 49200/144360  CE: 3.8507  
[05/07 10:09:10] SuperNet Training INFO: iter: 49320/144360  CE: 3.7027  
[05/07 10:09:11] SuperNet Training INFO: --> epoch:  41/120  avg CE: 3.8263  lr: 0.08862952561557644  
[05/07 10:10:48] SuperNet Training INFO: iter: 49440/144360  CE: 3.4539  
[05/07 10:11:48] SuperNet Training INFO: iter: 49560/144360  CE: 3.5018  
[05/07 10:12:48] SuperNet Training INFO: iter: 49680/144360  CE: 3.7002  
[05/07 10:13:49] SuperNet Training INFO: iter: 49800/144360  CE: 3.5526  
[05/07 10:14:48] SuperNet Training INFO: iter: 49920/144360  CE: 3.5914  
[05/07 10:15:48] SuperNet Training INFO: iter: 50040/144360  CE: 3.9410  
[05/07 10:16:48] SuperNet Training INFO: iter: 50160/144360  CE: 3.7292  
[05/07 10:17:48] SuperNet Training INFO: iter: 50280/144360  CE: 3.5212  
[05/07 10:18:48] SuperNet Training INFO: iter: 50400/144360  CE: 3.9594  
[05/07 10:19:48] SuperNet Training INFO: iter: 50520/144360  CE: 3.6472  
[05/07 10:19:50] SuperNet Training INFO: --> epoch:  42/120  avg CE: 3.8007  lr: 0.08723942998437267  
[05/07 10:21:25] SuperNet Training INFO: iter: 50640/144360  CE: 3.7832  
[05/07 10:22:27] SuperNet Training INFO: iter: 50760/144360  CE: 3.8279  
[05/07 10:23:29] SuperNet Training INFO: iter: 50880/144360  CE: 3.7175  
[05/07 10:24:30] SuperNet Training INFO: iter: 51000/144360  CE: 3.5950  
[05/07 10:25:30] SuperNet Training INFO: iter: 51120/144360  CE: 3.7978  
[05/07 10:26:30] SuperNet Training INFO: iter: 51240/144360  CE: 3.7463  
[05/07 10:27:30] SuperNet Training INFO: iter: 51360/144360  CE: 3.5888  
[05/07 10:28:31] SuperNet Training INFO: iter: 51480/144360  CE: 4.0461  
[05/07 10:29:31] SuperNet Training INFO: iter: 51600/144360  CE: 3.3550  
[05/07 10:30:31] SuperNet Training INFO: iter: 51720/144360  CE: 3.6408  
[05/07 10:30:35] SuperNet Training INFO: --> epoch:  43/120  avg CE: 3.7820  lr: 0.08583066580849745  
[05/07 10:32:09] SuperNet Training INFO: iter: 51840/144360  CE: 3.9304  
[05/07 10:33:10] SuperNet Training INFO: iter: 51960/144360  CE: 3.5346  
[05/07 10:34:11] SuperNet Training INFO: iter: 52080/144360  CE: 3.4065  
[05/07 10:35:13] SuperNet Training INFO: iter: 52200/144360  CE: 3.8642  
[05/07 10:36:13] SuperNet Training INFO: iter: 52320/144360  CE: 3.9176  
[05/07 10:37:14] SuperNet Training INFO: iter: 52440/144360  CE: 3.8356  
[05/07 10:38:13] SuperNet Training INFO: iter: 52560/144360  CE: 3.4936  
[05/07 10:39:13] SuperNet Training INFO: iter: 52680/144360  CE: 3.4761  
[05/07 10:40:13] SuperNet Training INFO: iter: 52800/144360  CE: 3.7215  
[05/07 10:41:13] SuperNet Training INFO: iter: 52920/144360  CE: 3.7502  
[05/07 10:41:18] SuperNet Training INFO: --> epoch:  44/120  avg CE: 3.7619  lr: 0.08440419858454766  
[05/07 10:42:49] SuperNet Training INFO: iter: 53040/144360  CE: 3.7961  
[05/07 10:43:51] SuperNet Training INFO: iter: 53160/144360  CE: 3.9948  
[05/07 10:44:52] SuperNet Training INFO: iter: 53280/144360  CE: 4.0245  
[05/07 10:45:54] SuperNet Training INFO: iter: 53400/144360  CE: 3.7313  
[05/07 10:46:53] SuperNet Training INFO: iter: 53520/144360  CE: 3.5027  
[05/07 10:47:53] SuperNet Training INFO: iter: 53640/144360  CE: 3.9511  
[05/07 10:48:53] SuperNet Training INFO: iter: 53760/144360  CE: 3.6964  
[05/07 10:49:53] SuperNet Training INFO: iter: 53880/144360  CE: 3.6324  
[05/07 10:50:53] SuperNet Training INFO: iter: 54000/144360  CE: 3.5387  
[05/07 10:51:55] SuperNet Training INFO: iter: 54120/144360  CE: 3.7996  
[05/07 10:52:01] SuperNet Training INFO: --> epoch:  45/120  avg CE: 3.7528  lr: 0.0829610059419049  
[05/07 10:53:31] SuperNet Training INFO: iter: 54240/144360  CE: 3.8289  
[05/07 10:54:31] SuperNet Training INFO: iter: 54360/144360  CE: 3.6016  
[05/07 10:55:32] SuperNet Training INFO: iter: 54480/144360  CE: 3.4379  
[05/07 10:56:32] SuperNet Training INFO: iter: 54600/144360  CE: 3.8419  
[05/07 10:57:33] SuperNet Training INFO: iter: 54720/144360  CE: 3.3877  
[05/07 10:58:34] SuperNet Training INFO: iter: 54840/144360  CE: 3.5999  
[05/07 10:59:34] SuperNet Training INFO: iter: 54960/144360  CE: 3.7184  
[05/07 11:00:36] SuperNet Training INFO: iter: 55080/144360  CE: 3.8702  
[05/07 11:01:36] SuperNet Training INFO: iter: 55200/144360  CE: 3.5946  
[05/07 11:02:37] SuperNet Training INFO: iter: 55320/144360  CE: 3.3352  
[05/07 11:02:45] SuperNet Training INFO: --> epoch:  46/120  avg CE: 3.7201  lr: 0.08150207697271764  
[05/07 11:04:15] SuperNet Training INFO: iter: 55440/144360  CE: 3.5147  
[05/07 11:05:15] SuperNet Training INFO: iter: 55560/144360  CE: 3.6825  
[05/07 11:06:15] SuperNet Training INFO: iter: 55680/144360  CE: 3.6877  
[05/07 11:07:17] SuperNet Training INFO: iter: 55800/144360  CE: 3.5105  
[05/07 11:08:17] SuperNet Training INFO: iter: 55920/144360  CE: 3.8766  
[05/07 11:09:17] SuperNet Training INFO: iter: 56040/144360  CE: 3.6807  
[05/07 11:10:19] SuperNet Training INFO: iter: 56160/144360  CE: 3.4396  
[05/07 11:11:21] SuperNet Training INFO: iter: 56280/144360  CE: 3.3247  
[05/07 11:12:21] SuperNet Training INFO: iter: 56400/144360  CE: 3.4074  
[05/07 11:13:21] SuperNet Training INFO: iter: 56520/144360  CE: 3.4685  
[05/07 11:13:30] SuperNet Training INFO: --> epoch:  47/120  avg CE: 3.6981  lr: 0.08002841155402596  
[05/07 11:14:57] SuperNet Training INFO: iter: 56640/144360  CE: 3.6713  
[05/07 11:15:58] SuperNet Training INFO: iter: 56760/144360  CE: 3.7234  
[05/07 11:16:57] SuperNet Training INFO: iter: 56880/144360  CE: 3.8084  
[05/07 11:17:57] SuperNet Training INFO: iter: 57000/144360  CE: 3.7408  
[05/07 11:18:58] SuperNet Training INFO: iter: 57120/144360  CE: 3.8496  
[05/07 11:19:59] SuperNet Training INFO: iter: 57240/144360  CE: 3.5796  
[05/07 11:20:59] SuperNet Training INFO: iter: 57360/144360  CE: 3.8764  
[05/07 11:21:59] SuperNet Training INFO: iter: 57480/144360  CE: 3.6593  
[05/07 11:23:00] SuperNet Training INFO: iter: 57600/144360  CE: 3.7072  
[05/07 11:24:00] SuperNet Training INFO: iter: 57720/144360  CE: 3.7400  
[05/07 11:24:11] SuperNet Training INFO: --> epoch:  48/120  avg CE: 3.6874  lr: 0.07854101966249659  
[05/07 11:25:37] SuperNet Training INFO: iter: 57840/144360  CE: 3.5954  
[05/07 11:26:36] SuperNet Training INFO: iter: 57960/144360  CE: 3.7762  
[05/07 11:27:36] SuperNet Training INFO: iter: 58080/144360  CE: 3.4921  
[05/07 11:28:37] SuperNet Training INFO: iter: 58200/144360  CE: 3.4864  
[05/07 11:29:38] SuperNet Training INFO: iter: 58320/144360  CE: 3.4990  
[05/07 11:30:38] SuperNet Training INFO: iter: 58440/144360  CE: 3.6458  
[05/07 11:31:39] SuperNet Training INFO: iter: 58560/144360  CE: 3.7541  
[05/07 11:32:38] SuperNet Training INFO: iter: 58680/144360  CE: 3.9197  
[05/07 11:33:38] SuperNet Training INFO: iter: 58800/144360  CE: 3.5213  
[05/07 11:34:39] SuperNet Training INFO: iter: 58920/144360  CE: 3.6868  
[05/07 11:34:52] SuperNet Training INFO: --> epoch:  49/120  avg CE: 3.6666  lr: 0.07704092068223518  
[05/07 11:36:17] SuperNet Training INFO: iter: 59040/144360  CE: 3.7086  
[05/07 11:37:19] SuperNet Training INFO: iter: 59160/144360  CE: 3.6143  
[05/07 11:38:20] SuperNet Training INFO: iter: 59280/144360  CE: 3.7347  
[05/07 11:39:21] SuperNet Training INFO: iter: 59400/144360  CE: 3.3159  
[05/07 11:40:22] SuperNet Training INFO: iter: 59520/144360  CE: 3.5612  
[05/07 11:41:23] SuperNet Training INFO: iter: 59640/144360  CE: 3.5027  
[05/07 11:42:25] SuperNet Training INFO: iter: 59760/144360  CE: 3.7287  
[05/07 11:43:26] SuperNet Training INFO: iter: 59880/144360  CE: 3.4503  
[05/07 11:44:25] SuperNet Training INFO: iter: 60000/144360  CE: 3.8292  
[05/07 11:45:26] SuperNet Training INFO: iter: 60120/144360  CE: 3.4067  
[05/07 11:45:40] SuperNet Training INFO: --> epoch:  50/120  avg CE: 3.6413  lr: 0.07552914270615126  
[05/07 11:47:03] SuperNet Training INFO: iter: 60240/144360  CE: 3.3817  
[05/07 11:48:04] SuperNet Training INFO: iter: 60360/144360  CE: 3.5872  
[05/07 11:49:02] SuperNet Training INFO: iter: 60480/144360  CE: 3.6919  
[05/07 11:50:03] SuperNet Training INFO: iter: 60600/144360  CE: 3.2892  
[05/07 11:51:04] SuperNet Training INFO: iter: 60720/144360  CE: 3.7882  
[05/07 11:52:04] SuperNet Training INFO: iter: 60840/144360  CE: 3.5492  
[05/07 11:53:06] SuperNet Training INFO: iter: 60960/144360  CE: 3.6583  
[05/07 11:54:07] SuperNet Training INFO: iter: 61080/144360  CE: 3.5003  
[05/07 11:55:07] SuperNet Training INFO: iter: 61200/144360  CE: 3.6955  
[05/07 11:56:08] SuperNet Training INFO: iter: 61320/144360  CE: 3.8059  
[05/07 11:56:23] SuperNet Training INFO: --> epoch:  51/120  avg CE: 3.6414  lr: 0.0740067218313545  
[05/07 11:57:43] SuperNet Training INFO: iter: 61440/144360  CE: 3.8041  
[05/07 11:58:43] SuperNet Training INFO: iter: 61560/144360  CE: 3.6041  
[05/07 11:59:44] SuperNet Training INFO: iter: 61680/144360  CE: 3.5139  
[05/07 12:00:45] SuperNet Training INFO: iter: 61800/144360  CE: 3.7942  
[05/07 12:01:46] SuperNet Training INFO: iter: 61920/144360  CE: 3.7939  
[05/07 12:02:48] SuperNet Training INFO: iter: 62040/144360  CE: 3.7693  
[05/07 12:03:48] SuperNet Training INFO: iter: 62160/144360  CE: 3.3724  
[05/07 12:04:49] SuperNet Training INFO: iter: 62280/144360  CE: 3.3321  
[05/07 12:05:50] SuperNet Training INFO: iter: 62400/144360  CE: 3.6829  
[05/07 12:06:52] SuperNet Training INFO: iter: 62520/144360  CE: 3.5859  
[05/07 12:07:08] SuperNet Training INFO: --> epoch:  52/120  avg CE: 3.6169  lr: 0.07247470144906537  
[05/07 12:08:29] SuperNet Training INFO: iter: 62640/144360  CE: 3.9364  
[05/07 12:09:30] SuperNet Training INFO: iter: 62760/144360  CE: 3.5874  
[05/07 12:10:31] SuperNet Training INFO: iter: 62880/144360  CE: 3.6916  
[05/07 12:11:33] SuperNet Training INFO: iter: 63000/144360  CE: 3.5733  
[05/07 12:12:34] SuperNet Training INFO: iter: 63120/144360  CE: 3.3122  
[05/07 12:13:36] SuperNet Training INFO: iter: 63240/144360  CE: 3.5371  
[05/07 12:14:37] SuperNet Training INFO: iter: 63360/144360  CE: 3.3316  
[05/07 12:15:39] SuperNet Training INFO: iter: 63480/144360  CE: 3.1605  
[05/07 12:16:38] SuperNet Training INFO: iter: 63600/144360  CE: 3.5653  
[05/07 12:17:38] SuperNet Training INFO: iter: 63720/144360  CE: 3.5161  
[05/07 12:17:56] SuperNet Training INFO: --> epoch:  53/120  avg CE: 3.5961  lr: 0.07093413152952865  
[05/07 12:19:13] SuperNet Training INFO: iter: 63840/144360  CE: 3.5945  
[05/07 12:20:15] SuperNet Training INFO: iter: 63960/144360  CE: 3.7634  
[05/07 12:21:16] SuperNet Training INFO: iter: 64080/144360  CE: 3.2955  
[05/07 12:22:17] SuperNet Training INFO: iter: 64200/144360  CE: 3.8965  
[05/07 12:23:18] SuperNet Training INFO: iter: 64320/144360  CE: 3.3733  
[05/07 12:24:18] SuperNet Training INFO: iter: 64440/144360  CE: 3.7188  
[05/07 12:25:18] SuperNet Training INFO: iter: 64560/144360  CE: 3.7527  
[05/07 12:26:18] SuperNet Training INFO: iter: 64680/144360  CE: 3.3908  
[05/07 12:27:18] SuperNet Training INFO: iter: 64800/144360  CE: 3.5881  
[05/07 12:28:18] SuperNet Training INFO: iter: 64920/144360  CE: 3.1529  
[05/07 12:28:38] SuperNet Training INFO: --> epoch:  54/120  avg CE: 3.5839  lr: 0.06938606790241343  
[05/07 12:29:53] SuperNet Training INFO: iter: 65040/144360  CE: 4.1037  
[05/07 12:30:53] SuperNet Training INFO: iter: 65160/144360  CE: 3.3350  
[05/07 12:31:53] SuperNet Training INFO: iter: 65280/144360  CE: 3.6009  
[05/07 12:32:53] SuperNet Training INFO: iter: 65400/144360  CE: 3.4429  
[05/07 12:33:53] SuperNet Training INFO: iter: 65520/144360  CE: 3.5305  
[05/07 12:34:52] SuperNet Training INFO: iter: 65640/144360  CE: 3.2170  
[05/07 12:35:54] SuperNet Training INFO: iter: 65760/144360  CE: 3.5278  
[05/07 12:36:53] SuperNet Training INFO: iter: 65880/144360  CE: 3.3263  
[05/07 12:37:55] SuperNet Training INFO: iter: 66000/144360  CE: 3.5283  
[05/07 12:38:56] SuperNet Training INFO: iter: 66120/144360  CE: 3.6336  
[05/07 12:39:18] SuperNet Training INFO: --> epoch:  55/120  avg CE: 3.5660  lr: 0.06783157153320266  
[05/07 12:40:32] SuperNet Training INFO: iter: 66240/144360  CE: 3.4570  
[05/07 12:41:32] SuperNet Training INFO: iter: 66360/144360  CE: 3.2249  
[05/07 12:42:32] SuperNet Training INFO: iter: 66480/144360  CE: 3.7031  
[05/07 12:43:33] SuperNet Training INFO: iter: 66600/144360  CE: 3.6879  
[05/07 12:44:35] SuperNet Training INFO: iter: 66720/144360  CE: 3.4143  
[05/07 12:45:38] SuperNet Training INFO: iter: 66840/144360  CE: 3.3812  
[05/07 12:46:38] SuperNet Training INFO: iter: 66960/144360  CE: 3.5201  
[05/07 12:47:38] SuperNet Training INFO: iter: 67080/144360  CE: 3.7357  
[05/07 12:48:38] SuperNet Training INFO: iter: 67200/144360  CE: 3.5500  
[05/07 12:49:39] SuperNet Training INFO: iter: 67320/144360  CE: 3.5168  
[05/07 12:50:02] SuperNet Training INFO: --> epoch:  56/120  avg CE: 3.5510  lr: 0.06627170779605904  
[05/07 12:51:15] SuperNet Training INFO: iter: 67440/144360  CE: 3.5689  
[05/07 12:52:15] SuperNet Training INFO: iter: 67560/144360  CE: 3.4775  
[05/07 12:53:15] SuperNet Training INFO: iter: 67680/144360  CE: 3.2898  
[05/07 12:54:15] SuperNet Training INFO: iter: 67800/144360  CE: 3.6187  
[05/07 12:55:15] SuperNet Training INFO: iter: 67920/144360  CE: 3.6313  
[05/07 12:56:15] SuperNet Training INFO: iter: 68040/144360  CE: 3.6356  
[05/07 12:57:17] SuperNet Training INFO: iter: 68160/144360  CE: 3.2906  
[05/07 12:58:18] SuperNet Training INFO: iter: 68280/144360  CE: 3.2634  
[05/07 12:59:20] SuperNet Training INFO: iter: 68400/144360  CE: 3.3408  
[05/07 13:00:22] SuperNet Training INFO: iter: 68520/144360  CE: 3.4723  
[05/07 13:00:47] SuperNet Training INFO: --> epoch:  57/120  avg CE: 3.5344  lr: 0.06470754574367053  
[05/07 13:02:00] SuperNet Training INFO: iter: 68640/144360  CE: 3.8536  
[05/07 13:03:03] SuperNet Training INFO: iter: 68760/144360  CE: 3.4894  
[05/07 13:04:04] SuperNet Training INFO: iter: 68880/144360  CE: 3.3763  
[05/07 13:05:05] SuperNet Training INFO: iter: 69000/144360  CE: 3.5911  
[05/07 13:06:06] SuperNet Training INFO: iter: 69120/144360  CE: 3.3311  
[05/07 13:07:07] SuperNet Training INFO: iter: 69240/144360  CE: 3.4730  
[05/07 13:08:08] SuperNet Training INFO: iter: 69360/144360  CE: 3.7589  
[05/07 13:09:11] SuperNet Training INFO: iter: 69480/144360  CE: 3.2310  
[05/07 13:10:12] SuperNet Training INFO: iter: 69600/144360  CE: 3.4707  
[05/07 13:11:13] SuperNet Training INFO: iter: 69720/144360  CE: 3.4455  
[05/07 13:11:39] SuperNet Training INFO: --> epoch:  58/120  avg CE: 3.5221  lr: 0.06314015737457618  
[05/07 13:12:49] SuperNet Training INFO: iter: 69840/144360  CE: 3.4417  
[05/07 13:13:49] SuperNet Training INFO: iter: 69960/144360  CE: 3.5045  
[05/07 13:14:49] SuperNet Training INFO: iter: 70080/144360  CE: 3.5476  
[05/07 13:15:49] SuperNet Training INFO: iter: 70200/144360  CE: 3.2991  
[05/07 13:16:48] SuperNet Training INFO: iter: 70320/144360  CE: 3.7027  
[05/07 13:17:49] SuperNet Training INFO: iter: 70440/144360  CE: 3.3741  
[05/07 13:18:49] SuperNet Training INFO: iter: 70560/144360  CE: 3.7798  
[05/07 13:19:49] SuperNet Training INFO: iter: 70680/144360  CE: 3.5632  
[05/07 13:20:50] SuperNet Training INFO: iter: 70800/144360  CE: 3.3672  
[05/07 13:21:51] SuperNet Training INFO: iter: 70920/144360  CE: 3.2058  
[05/07 13:22:18] SuperNet Training INFO: --> epoch:  59/120  avg CE: 3.5023  lr: 0.061570616898472125  
[05/07 13:23:26] SuperNet Training INFO: iter: 71040/144360  CE: 3.4095  
[05/07 13:24:26] SuperNet Training INFO: iter: 71160/144360  CE: 3.2889  
[05/07 13:25:27] SuperNet Training INFO: iter: 71280/144360  CE: 3.6341  
[05/07 13:26:28] SuperNet Training INFO: iter: 71400/144360  CE: 3.6065  
[05/07 13:27:28] SuperNet Training INFO: iter: 71520/144360  CE: 3.3879  
[05/07 13:28:31] SuperNet Training INFO: iter: 71640/144360  CE: 3.4633  
[05/07 13:29:31] SuperNet Training INFO: iter: 71760/144360  CE: 3.3925  
[05/07 13:30:31] SuperNet Training INFO: iter: 71880/144360  CE: 3.5832  
[05/07 13:31:32] SuperNet Training INFO: iter: 72000/144360  CE: 3.6453  
[05/07 13:32:33] SuperNet Training INFO: iter: 72120/144360  CE: 3.4292  
[05/07 13:33:02] SuperNet Training INFO: --> epoch:  60/120  avg CE: 3.4919  lr: 0.05999999999999976  
[05/07 13:34:09] SuperNet Training INFO: iter: 72240/144360  CE: 3.4548  
[05/07 13:35:08] SuperNet Training INFO: iter: 72360/144360  CE: 3.4356  
[05/07 13:36:09] SuperNet Training INFO: iter: 72480/144360  CE: 3.3805  
[05/07 13:37:09] SuperNet Training INFO: iter: 72600/144360  CE: 3.3762  
[05/07 13:38:11] SuperNet Training INFO: iter: 72720/144360  CE: 3.2450  
[05/07 13:39:13] SuperNet Training INFO: iter: 72840/144360  CE: 3.3852  
[05/07 13:40:15] SuperNet Training INFO: iter: 72960/144360  CE: 3.4269  
[05/07 13:41:16] SuperNet Training INFO: iter: 73080/144360  CE: 3.4514  
[05/07 13:42:17] SuperNet Training INFO: iter: 73200/144360  CE: 3.4601  
[05/07 13:43:18] SuperNet Training INFO: iter: 73320/144360  CE: 3.6575  
[05/07 13:43:49] SuperNet Training INFO: --> epoch:  61/120  avg CE: 3.4882  lr: 0.058429383101527455  
[05/07 13:44:54] SuperNet Training INFO: iter: 73440/144360  CE: 3.6483  
[05/07 13:45:55] SuperNet Training INFO: iter: 73560/144360  CE: 3.2199  
[05/07 13:46:57] SuperNet Training INFO: iter: 73680/144360  CE: 3.4672  
[05/07 13:47:58] SuperNet Training INFO: iter: 73800/144360  CE: 3.5545  
[05/07 13:48:59] SuperNet Training INFO: iter: 73920/144360  CE: 3.4417  
[05/07 13:50:00] SuperNet Training INFO: iter: 74040/144360  CE: 3.5658  
[05/07 13:51:01] SuperNet Training INFO: iter: 74160/144360  CE: 3.5434  
[05/07 13:52:02] SuperNet Training INFO: iter: 74280/144360  CE: 3.6423  
[05/07 13:53:03] SuperNet Training INFO: iter: 74400/144360  CE: 3.3403  
[05/07 13:54:03] SuperNet Training INFO: iter: 74520/144360  CE: 3.1956  
[05/07 13:54:36] SuperNet Training INFO: --> epoch:  62/120  avg CE: 3.4608  lr: 0.05685984262542327  
[05/07 13:55:39] SuperNet Training INFO: iter: 74640/144360  CE: 3.7102  
[05/07 13:56:38] SuperNet Training INFO: iter: 74760/144360  CE: 3.5787  
[05/07 13:57:38] SuperNet Training INFO: iter: 74880/144360  CE: 3.9450  
[05/07 13:58:38] SuperNet Training INFO: iter: 75000/144360  CE: 3.6933  
[05/07 13:59:39] SuperNet Training INFO: iter: 75120/144360  CE: 3.6885  
[05/07 14:00:39] SuperNet Training INFO: iter: 75240/144360  CE: 3.2594  
[05/07 14:01:41] SuperNet Training INFO: iter: 75360/144360  CE: 3.4566  
[05/07 14:02:42] SuperNet Training INFO: iter: 75480/144360  CE: 3.5062  
[05/07 14:03:41] SuperNet Training INFO: iter: 75600/144360  CE: 3.3534  
[05/07 14:04:41] SuperNet Training INFO: iter: 75720/144360  CE: 3.1042  
[05/07 14:05:15] SuperNet Training INFO: --> epoch:  63/120  avg CE: 3.4467  lr: 0.05529245425632924  
[05/07 14:06:16] SuperNet Training INFO: iter: 75840/144360  CE: 3.2741  
[05/07 14:07:16] SuperNet Training INFO: iter: 75960/144360  CE: 3.3764  
[05/07 14:08:16] SuperNet Training INFO: iter: 76080/144360  CE: 3.5185  
[05/07 14:09:16] SuperNet Training INFO: iter: 76200/144360  CE: 3.2624  
[05/07 14:10:16] SuperNet Training INFO: iter: 76320/144360  CE: 3.3975  
[05/07 14:11:17] SuperNet Training INFO: iter: 76440/144360  CE: 3.4443  
[05/07 14:12:17] SuperNet Training INFO: iter: 76560/144360  CE: 3.3461  
[05/07 14:13:18] SuperNet Training INFO: iter: 76680/144360  CE: 3.3771  
[05/07 14:14:17] SuperNet Training INFO: iter: 76800/144360  CE: 3.4653  
[05/07 14:15:17] SuperNet Training INFO: iter: 76920/144360  CE: 3.5711  
[05/07 14:15:53] SuperNet Training INFO: --> epoch:  64/120  avg CE: 3.4303  lr: 0.05372829220394078  
[05/07 14:16:54] SuperNet Training INFO: iter: 77040/144360  CE: 3.3570  
[05/07 14:17:54] SuperNet Training INFO: iter: 77160/144360  CE: 3.4876  
[05/07 14:18:56] SuperNet Training INFO: iter: 77280/144360  CE: 3.4744  
[05/07 14:19:56] SuperNet Training INFO: iter: 77400/144360  CE: 3.2775  
[05/07 14:20:57] SuperNet Training INFO: iter: 77520/144360  CE: 3.4820  
[05/07 14:21:59] SuperNet Training INFO: iter: 77640/144360  CE: 3.6106  
[05/07 14:23:01] SuperNet Training INFO: iter: 77760/144360  CE: 3.2763  
[05/07 14:24:02] SuperNet Training INFO: iter: 77880/144360  CE: 3.3466  
[05/07 14:25:04] SuperNet Training INFO: iter: 78000/144360  CE: 3.9380  
[05/07 14:26:06] SuperNet Training INFO: iter: 78120/144360  CE: 3.2471  
[05/07 14:26:43] SuperNet Training INFO: --> epoch:  65/120  avg CE: 3.4214  lr: 0.05216842846679694  
[05/07 14:27:41] SuperNet Training INFO: iter: 78240/144360  CE: 3.2387  
[05/07 14:28:40] SuperNet Training INFO: iter: 78360/144360  CE: 3.3489  
[05/07 14:29:40] SuperNet Training INFO: iter: 78480/144360  CE: 3.0451  
[05/07 14:30:41] SuperNet Training INFO: iter: 78600/144360  CE: 3.5253  
[05/07 14:31:41] SuperNet Training INFO: iter: 78720/144360  CE: 3.2534  
[05/07 14:32:42] SuperNet Training INFO: iter: 78840/144360  CE: 3.4202  
[05/07 14:33:44] SuperNet Training INFO: iter: 78960/144360  CE: 3.4996  
[05/07 14:34:44] SuperNet Training INFO: iter: 79080/144360  CE: 3.3494  
[05/07 14:35:44] SuperNet Training INFO: iter: 79200/144360  CE: 3.5780  
[05/07 14:36:46] SuperNet Training INFO: iter: 79320/144360  CE: 3.7097  
[05/07 14:37:24] SuperNet Training INFO: --> epoch:  66/120  avg CE: 3.3978  lr: 0.05061393209758616  
[05/07 14:38:21] SuperNet Training INFO: iter: 79440/144360  CE: 3.5634  
[05/07 14:39:21] SuperNet Training INFO: iter: 79560/144360  CE: 3.3389  
[05/07 14:40:23] SuperNet Training INFO: iter: 79680/144360  CE: 3.4359  
[05/07 14:41:24] SuperNet Training INFO: iter: 79800/144360  CE: 3.3677  
[05/07 14:42:25] SuperNet Training INFO: iter: 79920/144360  CE: 3.4277  
[05/07 14:43:26] SuperNet Training INFO: iter: 80040/144360  CE: 3.2504  
[05/07 14:44:27] SuperNet Training INFO: iter: 80160/144360  CE: 3.3349  
[05/07 14:45:28] SuperNet Training INFO: iter: 80280/144360  CE: 3.4788  
[05/07 14:46:29] SuperNet Training INFO: iter: 80400/144360  CE: 3.4972  
[05/07 14:47:30] SuperNet Training INFO: iter: 80520/144360  CE: 3.4524  
[05/07 14:48:11] SuperNet Training INFO: --> epoch:  67/120  avg CE: 3.3937  lr: 0.04906586847047105  
[05/07 14:49:07] SuperNet Training INFO: iter: 80640/144360  CE: 3.1493  
[05/07 14:50:06] SuperNet Training INFO: iter: 80760/144360  CE: 3.5673  
[05/07 14:51:06] SuperNet Training INFO: iter: 80880/144360  CE: 3.1592  
[05/07 14:52:06] SuperNet Training INFO: iter: 81000/144360  CE: 3.5703  
[05/07 14:53:07] SuperNet Training INFO: iter: 81120/144360  CE: 3.3193  
[05/07 14:54:07] SuperNet Training INFO: iter: 81240/144360  CE: 3.4477  
[05/07 14:55:08] SuperNet Training INFO: iter: 81360/144360  CE: 3.2031  
[05/07 14:56:10] SuperNet Training INFO: iter: 81480/144360  CE: 3.4785  
[05/07 14:57:11] SuperNet Training INFO: iter: 81600/144360  CE: 3.3073  
[05/07 14:58:11] SuperNet Training INFO: iter: 81720/144360  CE: 3.4255  
[05/07 14:58:51] SuperNet Training INFO: --> epoch:  68/120  avg CE: 3.3768  lr: 0.04752529855093431  
[05/07 14:59:45] SuperNet Training INFO: iter: 81840/144360  CE: 3.1598  
[05/07 15:00:46] SuperNet Training INFO: iter: 81960/144360  CE: 3.2694  
[05/07 15:01:46] SuperNet Training INFO: iter: 82080/144360  CE: 3.4955  
[05/07 15:02:46] SuperNet Training INFO: iter: 82200/144360  CE: 3.2050  
[05/07 15:03:45] SuperNet Training INFO: iter: 82320/144360  CE: 3.3563  
[05/07 15:04:47] SuperNet Training INFO: iter: 82440/144360  CE: 3.5628  
[05/07 15:05:50] SuperNet Training INFO: iter: 82560/144360  CE: 3.4909  
[05/07 15:06:51] SuperNet Training INFO: iter: 82680/144360  CE: 3.1699  
[05/07 15:07:52] SuperNet Training INFO: iter: 82800/144360  CE: 3.4073  
[05/07 15:08:54] SuperNet Training INFO: iter: 82920/144360  CE: 3.3947  
[05/07 15:09:37] SuperNet Training INFO: --> epoch:  69/120  avg CE: 3.3675  lr: 0.04599327816864548  
[05/07 15:10:31] SuperNet Training INFO: iter: 83040/144360  CE: 3.1766  
[05/07 15:11:31] SuperNet Training INFO: iter: 83160/144360  CE: 3.5008  
[05/07 15:12:31] SuperNet Training INFO: iter: 83280/144360  CE: 3.2964  
[05/07 15:13:33] SuperNet Training INFO: iter: 83400/144360  CE: 3.4398  
[05/07 15:14:32] SuperNet Training INFO: iter: 83520/144360  CE: 3.2586  
[05/07 15:15:32] SuperNet Training INFO: iter: 83640/144360  CE: 3.3990  
[05/07 15:16:33] SuperNet Training INFO: iter: 83760/144360  CE: 3.0956  
[05/07 15:17:33] SuperNet Training INFO: iter: 83880/144360  CE: 3.2820  
[05/07 15:18:32] SuperNet Training INFO: iter: 84000/144360  CE: 3.3701  
[05/07 15:19:33] SuperNet Training INFO: iter: 84120/144360  CE: 3.2189  
[05/07 15:20:16] SuperNet Training INFO: --> epoch:  70/120  avg CE: 3.3529  lr: 0.04447085729384866  
[05/07 15:21:09] SuperNet Training INFO: iter: 84240/144360  CE: 3.2869  
[05/07 15:22:10] SuperNet Training INFO: iter: 84360/144360  CE: 3.0662  
[05/07 15:23:11] SuperNet Training INFO: iter: 84480/144360  CE: 3.3517  
[05/07 15:24:12] SuperNet Training INFO: iter: 84600/144360  CE: 3.4754  
[05/07 15:25:12] SuperNet Training INFO: iter: 84720/144360  CE: 3.1132  
[05/07 15:26:12] SuperNet Training INFO: iter: 84840/144360  CE: 3.2072  
[05/07 15:27:13] SuperNet Training INFO: iter: 84960/144360  CE: 3.3814  
[05/07 15:28:15] SuperNet Training INFO: iter: 85080/144360  CE: 3.3134  
[05/07 15:29:17] SuperNet Training INFO: iter: 85200/144360  CE: 3.4472  
[05/07 15:30:18] SuperNet Training INFO: iter: 85320/144360  CE: 3.0356  
[05/07 15:31:03] SuperNet Training INFO: --> epoch:  71/120  avg CE: 3.3428  lr: 0.04295907931776456  
[05/07 15:31:53] SuperNet Training INFO: iter: 85440/144360  CE: 3.3901  
[05/07 15:32:53] SuperNet Training INFO: iter: 85560/144360  CE: 3.4778  
[05/07 15:33:54] SuperNet Training INFO: iter: 85680/144360  CE: 3.1795  
[05/07 15:34:55] SuperNet Training INFO: iter: 85800/144360  CE: 3.1064  
[05/07 15:35:56] SuperNet Training INFO: iter: 85920/144360  CE: 3.3384  
[05/07 15:36:58] SuperNet Training INFO: iter: 86040/144360  CE: 3.3367  
[05/07 15:37:59] SuperNet Training INFO: iter: 86160/144360  CE: 3.2022  
[05/07 15:39:01] SuperNet Training INFO: iter: 86280/144360  CE: 3.4893  
[05/07 15:40:02] SuperNet Training INFO: iter: 86400/144360  CE: 3.2955  
[05/07 15:41:04] SuperNet Training INFO: iter: 86520/144360  CE: 3.3911  
[05/07 15:41:52] SuperNet Training INFO: --> epoch:  72/120  avg CE: 3.3302  lr: 0.04145898033750296  
[05/07 15:42:40] SuperNet Training INFO: iter: 86640/144360  CE: 3.1273  
[05/07 15:43:41] SuperNet Training INFO: iter: 86760/144360  CE: 3.1228  
[05/07 15:44:43] SuperNet Training INFO: iter: 86880/144360  CE: 3.2716  
[05/07 15:45:44] SuperNet Training INFO: iter: 87000/144360  CE: 3.0425  
[05/07 15:46:44] SuperNet Training INFO: iter: 87120/144360  CE: 3.0137  
[05/07 15:47:46] SuperNet Training INFO: iter: 87240/144360  CE: 3.3677  
[05/07 15:48:48] SuperNet Training INFO: iter: 87360/144360  CE: 3.5328  
[05/07 15:49:50] SuperNet Training INFO: iter: 87480/144360  CE: 3.6355  
[05/07 15:50:50] SuperNet Training INFO: iter: 87600/144360  CE: 3.0709  
[05/07 15:51:53] SuperNet Training INFO: iter: 87720/144360  CE: 3.2104  
[05/07 15:52:42] SuperNet Training INFO: --> epoch:  73/120  avg CE: 3.3163  lr: 0.03997158844597365  
[05/07 15:53:29] SuperNet Training INFO: iter: 87840/144360  CE: 3.1699  
[05/07 15:54:30] SuperNet Training INFO: iter: 87960/144360  CE: 3.2887  
[05/07 15:55:32] SuperNet Training INFO: iter: 88080/144360  CE: 3.0673  
[05/07 15:56:33] SuperNet Training INFO: iter: 88200/144360  CE: 3.3915  
[05/07 15:57:36] SuperNet Training INFO: iter: 88320/144360  CE: 3.5092  
[05/07 15:58:36] SuperNet Training INFO: iter: 88440/144360  CE: 3.2615  
[05/07 15:59:36] SuperNet Training INFO: iter: 88560/144360  CE: 3.0899  
[05/07 16:00:36] SuperNet Training INFO: iter: 88680/144360  CE: 2.9970  
[05/07 16:01:36] SuperNet Training INFO: iter: 88800/144360  CE: 2.9972  
[05/07 16:02:36] SuperNet Training INFO: iter: 88920/144360  CE: 3.9887  
[05/07 16:03:27] SuperNet Training INFO: --> epoch:  74/120  avg CE: 3.2992  lr: 0.03849792302728192  
[05/07 16:04:12] SuperNet Training INFO: iter: 89040/144360  CE: 3.3989  
[05/07 16:05:12] SuperNet Training INFO: iter: 89160/144360  CE: 3.0996  
[05/07 16:06:10] SuperNet Training INFO: iter: 89280/144360  CE: 2.9833  
[05/07 16:07:10] SuperNet Training INFO: iter: 89400/144360  CE: 3.2811  
[05/07 16:08:12] SuperNet Training INFO: iter: 89520/144360  CE: 3.2881  
[05/07 16:09:13] SuperNet Training INFO: iter: 89640/144360  CE: 3.3259  
[05/07 16:10:14] SuperNet Training INFO: iter: 89760/144360  CE: 3.4808  
[05/07 16:11:15] SuperNet Training INFO: iter: 89880/144360  CE: 3.2914  
[05/07 16:12:15] SuperNet Training INFO: iter: 90000/144360  CE: 3.2976  
[05/07 16:13:16] SuperNet Training INFO: iter: 90120/144360  CE: 3.0647  
[05/07 16:14:07] SuperNet Training INFO: --> epoch:  75/120  avg CE: 3.2880  lr: 0.03703899405809455  
[05/07 16:14:51] SuperNet Training INFO: iter: 90240/144360  CE: 3.2449  
[05/07 16:15:51] SuperNet Training INFO: iter: 90360/144360  CE: 3.1931  
[05/07 16:16:52] SuperNet Training INFO: iter: 90480/144360  CE: 3.4729  
[05/07 16:17:54] SuperNet Training INFO: iter: 90600/144360  CE: 3.1910  
[05/07 16:18:54] SuperNet Training INFO: iter: 90720/144360  CE: 3.0930  
[05/07 16:19:55] SuperNet Training INFO: iter: 90840/144360  CE: 3.2728  
[05/07 16:20:57] SuperNet Training INFO: iter: 90960/144360  CE: 3.1656  
[05/07 16:21:56] SuperNet Training INFO: iter: 91080/144360  CE: 3.4946  
[05/07 16:22:56] SuperNet Training INFO: iter: 91200/144360  CE: 3.4751  
[05/07 16:23:56] SuperNet Training INFO: iter: 91320/144360  CE: 3.0827  
[05/07 16:24:51] SuperNet Training INFO: --> epoch:  76/120  avg CE: 3.2783  lr: 0.035595801415451815  
[05/07 16:25:33] SuperNet Training INFO: iter: 91440/144360  CE: 3.5423  
[05/07 16:26:32] SuperNet Training INFO: iter: 91560/144360  CE: 3.5786  
[05/07 16:27:32] SuperNet Training INFO: iter: 91680/144360  CE: 3.2966  
[05/07 16:28:34] SuperNet Training INFO: iter: 91800/144360  CE: 3.3484  
[05/07 16:29:35] SuperNet Training INFO: iter: 91920/144360  CE: 2.9315  
[05/07 16:30:34] SuperNet Training INFO: iter: 92040/144360  CE: 3.3196  
[05/07 16:31:36] SuperNet Training INFO: iter: 92160/144360  CE: 3.2793  
[05/07 16:32:36] SuperNet Training INFO: iter: 92280/144360  CE: 3.2195  
[05/07 16:33:38] SuperNet Training INFO: iter: 92400/144360  CE: 3.4696  
[05/07 16:34:39] SuperNet Training INFO: iter: 92520/144360  CE: 3.1035  
[05/07 16:35:35] SuperNet Training INFO: --> epoch:  77/120  avg CE: 3.2794  lr: 0.03416933419150217  
[05/07 16:36:16] SuperNet Training INFO: iter: 92640/144360  CE: 2.9662  
[05/07 16:37:17] SuperNet Training INFO: iter: 92760/144360  CE: 3.5529  
[05/07 16:38:17] SuperNet Training INFO: iter: 92880/144360  CE: 3.4345  
[05/07 16:39:17] SuperNet Training INFO: iter: 93000/144360  CE: 3.5080  
[05/07 16:40:18] SuperNet Training INFO: iter: 93120/144360  CE: 3.2575  
[05/07 16:41:17] SuperNet Training INFO: iter: 93240/144360  CE: 2.9817  
[05/07 16:42:17] SuperNet Training INFO: iter: 93360/144360  CE: 3.3618  
[05/07 16:43:17] SuperNet Training INFO: iter: 93480/144360  CE: 3.3191  
[05/07 16:44:19] SuperNet Training INFO: iter: 93600/144360  CE: 3.3954  
[05/07 16:45:20] SuperNet Training INFO: iter: 93720/144360  CE: 3.2403  
[05/07 16:46:19] SuperNet Training INFO: --> epoch:  78/120  avg CE: 3.2635  lr: 0.03276057001562702  
[05/07 16:46:58] SuperNet Training INFO: iter: 93840/144360  CE: 3.4225  
[05/07 16:47:59] SuperNet Training INFO: iter: 93960/144360  CE: 3.1557  
[05/07 16:49:01] SuperNet Training INFO: iter: 94080/144360  CE: 3.1660  
[05/07 16:50:01] SuperNet Training INFO: iter: 94200/144360  CE: 3.4016  
[05/07 16:51:01] SuperNet Training INFO: iter: 94320/144360  CE: 3.1239  
[05/07 16:52:02] SuperNet Training INFO: iter: 94440/144360  CE: 3.2869  
[05/07 16:53:02] SuperNet Training INFO: iter: 94560/144360  CE: 3.1572  
[05/07 16:54:02] SuperNet Training INFO: iter: 94680/144360  CE: 3.4435  
[05/07 16:55:00] SuperNet Training INFO: iter: 94800/144360  CE: 3.5210  
[05/07 16:56:00] SuperNet Training INFO: iter: 94920/144360  CE: 3.2670  
[05/07 16:56:57] SuperNet Training INFO: --> epoch:  79/120  avg CE: 3.2496  lr: 0.031370474384423336  
[05/07 16:57:35] SuperNet Training INFO: iter: 95040/144360  CE: 3.3280  
[05/07 16:58:37] SuperNet Training INFO: iter: 95160/144360  CE: 3.4402  
[05/07 16:59:38] SuperNet Training INFO: iter: 95280/144360  CE: 3.2961  
[05/07 17:00:39] SuperNet Training INFO: iter: 95400/144360  CE: 3.2613  
[05/07 17:01:40] SuperNet Training INFO: iter: 95520/144360  CE: 3.0874  
[05/07 17:02:41] SuperNet Training INFO: iter: 95640/144360  CE: 3.2125  
[05/07 17:03:43] SuperNet Training INFO: iter: 95760/144360  CE: 3.3815  
[05/07 17:04:44] SuperNet Training INFO: iter: 95880/144360  CE: 3.4600  
[05/07 17:05:45] SuperNet Training INFO: iter: 96000/144360  CE: 3.3020  
[05/07 17:06:45] SuperNet Training INFO: iter: 96120/144360  CE: 3.0350  
[05/07 17:07:44] SuperNet Training INFO: iter: 96240/144360  CE: 2.8212  
[05/07 17:07:44] SuperNet Training INFO: --> epoch:  80/120  avg CE: 3.2313  lr: 0.02999999999999983  
[05/07 17:09:21] SuperNet Training INFO: iter: 96360/144360  CE: 3.1080  
[05/07 17:10:22] SuperNet Training INFO: iter: 96480/144360  CE: 2.9011  
[05/07 17:11:23] SuperNet Training INFO: iter: 96600/144360  CE: 2.9695  
[05/07 17:12:25] SuperNet Training INFO: iter: 96720/144360  CE: 3.3456  
[05/07 17:13:26] SuperNet Training INFO: iter: 96840/144360  CE: 2.9980  
[05/07 17:14:27] SuperNet Training INFO: iter: 96960/144360  CE: 3.3976  
[05/07 17:15:28] SuperNet Training INFO: iter: 97080/144360  CE: 3.0651  
[05/07 17:16:30] SuperNet Training INFO: iter: 97200/144360  CE: 3.4999  
[05/07 17:17:31] SuperNet Training INFO: iter: 97320/144360  CE: 3.5853  
[05/07 17:18:32] SuperNet Training INFO: iter: 97440/144360  CE: 3.2283  
[05/07 17:18:32] SuperNet Training INFO: --> epoch:  81/120  avg CE: 3.2272  lr: 0.028650086117042926  
[05/07 17:20:09] SuperNet Training INFO: iter: 97560/144360  CE: 3.3783  
[05/07 17:21:08] SuperNet Training INFO: iter: 97680/144360  CE: 3.1029  
[05/07 17:22:08] SuperNet Training INFO: iter: 97800/144360  CE: 3.4272  
[05/07 17:23:09] SuperNet Training INFO: iter: 97920/144360  CE: 3.2843  
[05/07 17:24:10] SuperNet Training INFO: iter: 98040/144360  CE: 3.0438  
[05/07 17:25:09] SuperNet Training INFO: iter: 98160/144360  CE: 3.3501  
[05/07 17:26:09] SuperNet Training INFO: iter: 98280/144360  CE: 3.3483  
[05/07 17:27:09] SuperNet Training INFO: iter: 98400/144360  CE: 3.2395  
[05/07 17:28:09] SuperNet Training INFO: iter: 98520/144360  CE: 3.1485  
[05/07 17:29:08] SuperNet Training INFO: iter: 98640/144360  CE: 3.2942  
[05/07 17:29:10] SuperNet Training INFO: --> epoch:  82/120  avg CE: 3.2122  lr: 0.027321657899098132  
[05/07 17:30:44] SuperNet Training INFO: iter: 98760/144360  CE: 2.7654  
[05/07 17:31:45] SuperNet Training INFO: iter: 98880/144360  CE: 3.3216  
[05/07 17:32:46] SuperNet Training INFO: iter: 99000/144360  CE: 3.2684  
[05/07 17:33:49] SuperNet Training INFO: iter: 99120/144360  CE: 3.2488  
[05/07 17:34:49] SuperNet Training INFO: iter: 99240/144360  CE: 3.2366  
[05/07 17:35:50] SuperNet Training INFO: iter: 99360/144360  CE: 3.2907  
[05/07 17:36:51] SuperNet Training INFO: iter: 99480/144360  CE: 3.3080  
[05/07 17:37:52] SuperNet Training INFO: iter: 99600/144360  CE: 3.1298  
[05/07 17:38:54] SuperNet Training INFO: iter: 99720/144360  CE: 2.9481  
[05/07 17:39:56] SuperNet Training INFO: iter: 99840/144360  CE: 3.2833  
[05/07 17:39:59] SuperNet Training INFO: --> epoch:  83/120  avg CE: 3.2086  lr: 0.0260156257845098  
[05/07 17:41:32] SuperNet Training INFO: iter: 99960/144360  CE: 3.2105  
[05/07 17:42:31] SuperNet Training INFO: iter: 100080/144360  CE: 3.1995  
[05/07 17:43:32] SuperNet Training INFO: iter: 100200/144360  CE: 3.4087  
[05/07 17:44:32] SuperNet Training INFO: iter: 100320/144360  CE: 3.2048  
[05/07 17:45:33] SuperNet Training INFO: iter: 100440/144360  CE: 3.3505  
[05/07 17:46:32] SuperNet Training INFO: iter: 100560/144360  CE: 3.1386  
[05/07 17:47:32] SuperNet Training INFO: iter: 100680/144360  CE: 3.1532  
[05/07 17:48:31] SuperNet Training INFO: iter: 100800/144360  CE: 3.1123  
[05/07 17:49:31] SuperNet Training INFO: iter: 100920/144360  CE: 3.2764  
[05/07 17:50:29] SuperNet Training INFO: iter: 101040/144360  CE: 3.3353  
[05/07 17:50:34] SuperNet Training INFO: --> epoch:  84/120  avg CE: 3.1912  lr: 0.02473288486245143  
[05/07 17:52:04] SuperNet Training INFO: iter: 101160/144360  CE: 3.0063  
[05/07 17:53:04] SuperNet Training INFO: iter: 101280/144360  CE: 3.2865  
[05/07 17:54:05] SuperNet Training INFO: iter: 101400/144360  CE: 3.0437  
[05/07 17:55:06] SuperNet Training INFO: iter: 101520/144360  CE: 3.0331  
[05/07 17:56:06] SuperNet Training INFO: iter: 101640/144360  CE: 3.1143  
[05/07 17:57:07] SuperNet Training INFO: iter: 101760/144360  CE: 3.2600  
[05/07 17:58:06] SuperNet Training INFO: iter: 101880/144360  CE: 3.2399  
[05/07 17:59:06] SuperNet Training INFO: iter: 102000/144360  CE: 3.1630  
[05/07 18:00:05] SuperNet Training INFO: iter: 102120/144360  CE: 3.3781  
[05/07 18:01:06] SuperNet Training INFO: iter: 102240/144360  CE: 3.1342  
[05/07 18:01:12] SuperNet Training INFO: --> epoch:  85/120  avg CE: 3.1840  lr: 0.023474314259476523  
[05/07 18:02:40] SuperNet Training INFO: iter: 102360/144360  CE: 3.2300  
[05/07 18:03:40] SuperNet Training INFO: iter: 102480/144360  CE: 3.0422  
[05/07 18:04:42] SuperNet Training INFO: iter: 102600/144360  CE: 3.4197  
[05/07 18:05:42] SuperNet Training INFO: iter: 102720/144360  CE: 3.0244  
[05/07 18:06:42] SuperNet Training INFO: iter: 102840/144360  CE: 3.3723  
[05/07 18:07:43] SuperNet Training INFO: iter: 102960/144360  CE: 3.1565  
[05/07 18:08:42] SuperNet Training INFO: iter: 103080/144360  CE: 3.1602  
[05/07 18:09:42] SuperNet Training INFO: iter: 103200/144360  CE: 3.0723  
[05/07 18:10:41] SuperNet Training INFO: iter: 103320/144360  CE: 3.1218  
[05/07 18:11:41] SuperNet Training INFO: iter: 103440/144360  CE: 3.2562  
[05/07 18:11:49] SuperNet Training INFO: --> epoch:  86/120  avg CE: 3.1748  lr: 0.02224077653700959  
[05/07 18:13:18] SuperNet Training INFO: iter: 103560/144360  CE: 3.5796  
[05/07 18:14:18] SuperNet Training INFO: iter: 103680/144360  CE: 2.8160  
[05/07 18:15:18] SuperNet Training INFO: iter: 103800/144360  CE: 2.9577  
[05/07 18:16:20] SuperNet Training INFO: iter: 103920/144360  CE: 3.2338  
[05/07 18:17:21] SuperNet Training INFO: iter: 104040/144360  CE: 3.2898  
[05/07 18:18:23] SuperNet Training INFO: iter: 104160/144360  CE: 3.2339  
[05/07 18:19:25] SuperNet Training INFO: iter: 104280/144360  CE: 3.1203  
[05/07 18:20:26] SuperNet Training INFO: iter: 104400/144360  CE: 3.0254  
[05/07 18:21:27] SuperNet Training INFO: iter: 104520/144360  CE: 3.1508  
[05/07 18:22:29] SuperNet Training INFO: iter: 104640/144360  CE: 3.0324  
[05/07 18:22:38] SuperNet Training INFO: --> epoch:  87/120  avg CE: 3.1561  lr: 0.02103311710018879  
[05/07 18:24:04] SuperNet Training INFO: iter: 104760/144360  CE: 3.3852  
[05/07 18:25:05] SuperNet Training INFO: iter: 104880/144360  CE: 3.0335  
[05/07 18:26:06] SuperNet Training INFO: iter: 105000/144360  CE: 3.1963  
[05/07 18:27:06] SuperNet Training INFO: iter: 105120/144360  CE: 3.2872  
[05/07 18:28:06] SuperNet Training INFO: iter: 105240/144360  CE: 3.4661  
[05/07 18:29:07] SuperNet Training INFO: iter: 105360/144360  CE: 3.0015  
[05/07 18:30:07] SuperNet Training INFO: iter: 105480/144360  CE: 3.3385  
[05/07 18:31:09] SuperNet Training INFO: iter: 105600/144360  CE: 3.6400  
[05/07 18:32:10] SuperNet Training INFO: iter: 105720/144360  CE: 2.9022  
[05/07 18:33:11] SuperNet Training INFO: iter: 105840/144360  CE: 3.1746  
[05/07 18:33:22] SuperNet Training INFO: --> epoch:  88/120  avg CE: 3.1565  lr: 0.019852163618468272  
[05/07 18:34:46] SuperNet Training INFO: iter: 105960/144360  CE: 3.2587  
[05/07 18:35:46] SuperNet Training INFO: iter: 106080/144360  CE: 3.0296  
[05/07 18:36:46] SuperNet Training INFO: iter: 106200/144360  CE: 2.7831  
[05/07 18:37:45] SuperNet Training INFO: iter: 106320/144360  CE: 2.9785  
[05/07 18:38:45] SuperNet Training INFO: iter: 106440/144360  CE: 2.9112  
[05/07 18:39:45] SuperNet Training INFO: iter: 106560/144360  CE: 3.2068  
[05/07 18:40:45] SuperNet Training INFO: iter: 106680/144360  CE: 2.9930  
[05/07 18:41:45] SuperNet Training INFO: iter: 106800/144360  CE: 3.2205  
[05/07 18:42:45] SuperNet Training INFO: iter: 106920/144360  CE: 3.2919  
[05/07 18:43:46] SuperNet Training INFO: iter: 107040/144360  CE: 3.3408  
[05/07 18:43:59] SuperNet Training INFO: --> epoch:  89/120  avg CE: 3.1481  lr: 0.018698725458374543  
[05/07 18:45:22] SuperNet Training INFO: iter: 107160/144360  CE: 3.1047  
[05/07 18:46:25] SuperNet Training INFO: iter: 107280/144360  CE: 3.0813  
[05/07 18:47:25] SuperNet Training INFO: iter: 107400/144360  CE: 3.0001  
[05/07 18:48:24] SuperNet Training INFO: iter: 107520/144360  CE: 2.9588  
[05/07 18:49:25] SuperNet Training INFO: iter: 107640/144360  CE: 3.0359  
[05/07 18:50:27] SuperNet Training INFO: iter: 107760/144360  CE: 2.9041  
[05/07 18:51:28] SuperNet Training INFO: iter: 107880/144360  CE: 3.1180  
[05/07 18:52:30] SuperNet Training INFO: iter: 108000/144360  CE: 2.9110  
[05/07 18:53:30] SuperNet Training INFO: iter: 108120/144360  CE: 3.1600  
[05/07 18:54:31] SuperNet Training INFO: iter: 108240/144360  CE: 3.1420  
[05/07 18:54:44] SuperNet Training INFO: --> epoch:  90/120  avg CE: 3.1291  lr: 0.01757359312880703  
[05/07 18:56:06] SuperNet Training INFO: iter: 108360/144360  CE: 3.1920  
[05/07 18:57:06] SuperNet Training INFO: iter: 108480/144360  CE: 3.0644  
[05/07 18:58:07] SuperNet Training INFO: iter: 108600/144360  CE: 3.2116  
[05/07 18:59:07] SuperNet Training INFO: iter: 108720/144360  CE: 3.1326  
[05/07 19:00:07] SuperNet Training INFO: iter: 108840/144360  CE: 3.1648  
[05/07 19:01:07] SuperNet Training INFO: iter: 108960/144360  CE: 3.1163  
[05/07 19:02:08] SuperNet Training INFO: iter: 109080/144360  CE: 3.1374  
[05/07 19:03:10] SuperNet Training INFO: iter: 109200/144360  CE: 2.9517  
[05/07 19:04:10] SuperNet Training INFO: iter: 109320/144360  CE: 3.0770  
[05/07 19:05:10] SuperNet Training INFO: iter: 109440/144360  CE: 2.9350  
[05/07 19:05:25] SuperNet Training INFO: --> epoch:  91/120  avg CE: 3.1196  lr: 0.016477537739262627  
[05/07 19:06:45] SuperNet Training INFO: iter: 109560/144360  CE: 2.9094  
[05/07 19:07:46] SuperNet Training INFO: iter: 109680/144360  CE: 3.0336  
[05/07 19:08:46] SuperNet Training INFO: iter: 109800/144360  CE: 3.0809  
[05/07 19:09:48] SuperNet Training INFO: iter: 109920/144360  CE: 3.0705  
[05/07 19:10:48] SuperNet Training INFO: iter: 110040/144360  CE: 2.9504  
[05/07 19:11:49] SuperNet Training INFO: iter: 110160/144360  CE: 2.8664  
[05/07 19:12:49] SuperNet Training INFO: iter: 110280/144360  CE: 3.0583  
[05/07 19:13:50] SuperNet Training INFO: iter: 110400/144360  CE: 3.3448  
[05/07 19:14:49] SuperNet Training INFO: iter: 110520/144360  CE: 3.1374  
[05/07 19:15:49] SuperNet Training INFO: iter: 110640/144360  CE: 3.2602  
[05/07 19:16:06] SuperNet Training INFO: --> epoch:  92/120  avg CE: 3.1163  lr: 0.015411310471356233  
[05/07 19:17:24] SuperNet Training INFO: iter: 110760/144360  CE: 3.1692  
[05/07 19:18:24] SuperNet Training INFO: iter: 110880/144360  CE: 3.1568  
[05/07 19:19:23] SuperNet Training INFO: iter: 111000/144360  CE: 2.7942  
[05/07 19:20:25] SuperNet Training INFO: iter: 111120/144360  CE: 3.2543  
[05/07 19:21:25] SuperNet Training INFO: iter: 111240/144360  CE: 3.2527  
[05/07 19:22:26] SuperNet Training INFO: iter: 111360/144360  CE: 2.7717  
[05/07 19:23:25] SuperNet Training INFO: iter: 111480/144360  CE: 3.0505  
[05/07 19:24:26] SuperNet Training INFO: iter: 111600/144360  CE: 2.9143  
[05/07 19:25:27] SuperNet Training INFO: iter: 111720/144360  CE: 3.1297  
[05/07 19:26:28] SuperNet Training INFO: iter: 111840/144360  CE: 3.1315  
[05/07 19:26:46] SuperNet Training INFO: --> epoch:  93/120  avg CE: 3.0958  lr: 0.014375642063998082  
[05/07 19:28:04] SuperNet Training INFO: iter: 111960/144360  CE: 3.5425  
[05/07 19:29:03] SuperNet Training INFO: iter: 112080/144360  CE: 3.2432  
[05/07 19:30:04] SuperNet Training INFO: iter: 112200/144360  CE: 2.8947  
[05/07 19:31:04] SuperNet Training INFO: iter: 112320/144360  CE: 3.1209  
[05/07 19:32:06] SuperNet Training INFO: iter: 112440/144360  CE: 3.1185  
[05/07 19:33:06] SuperNet Training INFO: iter: 112560/144360  CE: 3.1065  
[05/07 19:34:07] SuperNet Training INFO: iter: 112680/144360  CE: 3.0018  
[05/07 19:35:08] SuperNet Training INFO: iter: 112800/144360  CE: 3.0335  
[05/07 19:36:10] SuperNet Training INFO: iter: 112920/144360  CE: 2.9539  
[05/07 19:37:10] SuperNet Training INFO: iter: 113040/144360  CE: 2.7388  
[05/07 19:37:30] SuperNet Training INFO: --> epoch:  94/120  avg CE: 3.0879  lr: 0.01337124231258167  
[05/07 19:38:45] SuperNet Training INFO: iter: 113160/144360  CE: 2.9814  
[05/07 19:39:46] SuperNet Training INFO: iter: 113280/144360  CE: 3.1316  
[05/07 19:40:47] SuperNet Training INFO: iter: 113400/144360  CE: 3.0772  
[05/07 19:41:47] SuperNet Training INFO: iter: 113520/144360  CE: 3.3957  
[05/07 19:42:48] SuperNet Training INFO: iter: 113640/144360  CE: 3.1627  
[05/07 19:43:50] SuperNet Training INFO: iter: 113760/144360  CE: 2.9192  
[05/07 19:44:51] SuperNet Training INFO: iter: 113880/144360  CE: 2.9684  
[05/07 19:45:52] SuperNet Training INFO: iter: 114000/144360  CE: 2.9490  
[05/07 19:46:52] SuperNet Training INFO: iter: 114120/144360  CE: 2.8994  
[05/07 19:47:55] SuperNet Training INFO: iter: 114240/144360  CE: 2.9630  
[05/07 19:48:16] SuperNet Training INFO: --> epoch:  95/120  avg CE: 3.0891  lr: 0.012398799582525794  
[05/07 19:49:31] SuperNet Training INFO: iter: 114360/144360  CE: 2.9979  
[05/07 19:50:31] SuperNet Training INFO: iter: 114480/144360  CE: 2.8928  
[05/07 19:51:33] SuperNet Training INFO: iter: 114600/144360  CE: 2.9059  
[05/07 19:52:33] SuperNet Training INFO: iter: 114720/144360  CE: 3.1463  
[05/07 19:53:33] SuperNet Training INFO: iter: 114840/144360  CE: 3.3099  
[05/07 19:54:34] SuperNet Training INFO: iter: 114960/144360  CE: 3.1025  
[05/07 19:55:34] SuperNet Training INFO: iter: 115080/144360  CE: 2.9098  
[05/07 19:56:34] SuperNet Training INFO: iter: 115200/144360  CE: 3.2050  
[05/07 19:57:35] SuperNet Training INFO: iter: 115320/144360  CE: 3.1126  
[05/07 19:58:34] SuperNet Training INFO: iter: 115440/144360  CE: 2.8832  
[05/07 19:58:58] SuperNet Training INFO: --> epoch:  96/120  avg CE: 3.0799  lr: 0.01145898033750309  
[05/07 20:00:11] SuperNet Training INFO: iter: 115560/144360  CE: 3.0136  
[05/07 20:01:12] SuperNet Training INFO: iter: 115680/144360  CE: 2.8758  
[05/07 20:02:14] SuperNet Training INFO: iter: 115800/144360  CE: 3.2392  
[05/07 20:03:14] SuperNet Training INFO: iter: 115920/144360  CE: 3.1285  
[05/07 20:04:15] SuperNet Training INFO: iter: 116040/144360  CE: 2.9583  
[05/07 20:05:17] SuperNet Training INFO: iter: 116160/144360  CE: 2.8789  
[05/07 20:06:17] SuperNet Training INFO: iter: 116280/144360  CE: 2.8624  
[05/07 20:07:18] SuperNet Training INFO: iter: 116400/144360  CE: 2.9117  
[05/07 20:08:19] SuperNet Training INFO: iter: 116520/144360  CE: 3.0240  
[05/07 20:09:21] SuperNet Training INFO: iter: 116640/144360  CE: 3.0686  
[05/07 20:09:45] SuperNet Training INFO: --> epoch:  97/120  avg CE: 3.0655  lr: 0.01055242868267905  
[05/07 20:10:56] SuperNet Training INFO: iter: 116760/144360  CE: 3.0796  
[05/07 20:11:57] SuperNet Training INFO: iter: 116880/144360  CE: 2.7862  
[05/07 20:12:59] SuperNet Training INFO: iter: 117000/144360  CE: 3.0014  
[05/07 20:14:01] SuperNet Training INFO: iter: 117120/144360  CE: 2.9934  
[05/07 20:15:03] SuperNet Training INFO: iter: 117240/144360  CE: 3.1452  
[05/07 20:16:04] SuperNet Training INFO: iter: 117360/144360  CE: 3.0912  
[05/07 20:17:06] SuperNet Training INFO: iter: 117480/144360  CE: 2.9161  
[05/07 20:18:08] SuperNet Training INFO: iter: 117600/144360  CE: 3.1135  
[05/07 20:19:10] SuperNet Training INFO: iter: 117720/144360  CE: 3.1464  
[05/07 20:20:12] SuperNet Training INFO: iter: 117840/144360  CE: 3.0361  
[05/07 20:20:38] SuperNet Training INFO: --> epoch:  98/120  avg CE: 3.0494  lr: 0.009679765923274538  
[05/07 20:21:49] SuperNet Training INFO: iter: 117960/144360  CE: 3.0992  
[05/07 20:22:50] SuperNet Training INFO: iter: 118080/144360  CE: 2.9636  
[05/07 20:23:52] SuperNet Training INFO: iter: 118200/144360  CE: 3.4413  
[05/07 20:24:52] SuperNet Training INFO: iter: 118320/144360  CE: 2.9048  
[05/07 20:25:55] SuperNet Training INFO: iter: 118440/144360  CE: 3.0926  
[05/07 20:26:56] SuperNet Training INFO: iter: 118560/144360  CE: 2.5527  
[05/07 20:27:57] SuperNet Training INFO: iter: 118680/144360  CE: 3.0730  
[05/07 20:28:56] SuperNet Training INFO: iter: 118800/144360  CE: 2.8762  
[05/07 20:29:56] SuperNet Training INFO: iter: 118920/144360  CE: 3.0172  
[05/07 20:30:55] SuperNet Training INFO: iter: 119040/144360  CE: 3.0494  
[05/07 20:31:22] SuperNet Training INFO: --> epoch:  99/120  avg CE: 3.0417  lr: 0.008841590138754444  
[05/07 20:32:29] SuperNet Training INFO: iter: 119160/144360  CE: 3.0566  
[05/07 20:33:31] SuperNet Training INFO: iter: 119280/144360  CE: 3.1427  
[05/07 20:34:32] SuperNet Training INFO: iter: 119400/144360  CE: 3.1134  
[05/07 20:35:34] SuperNet Training INFO: iter: 119520/144360  CE: 2.8553  
[05/07 20:36:35] SuperNet Training INFO: iter: 119640/144360  CE: 2.8815  
[05/07 20:37:36] SuperNet Training INFO: iter: 119760/144360  CE: 3.1170  
[05/07 20:38:36] SuperNet Training INFO: iter: 119880/144360  CE: 3.0287  
[05/07 20:39:37] SuperNet Training INFO: iter: 120000/144360  CE: 2.9377  
[05/07 20:40:38] SuperNet Training INFO: iter: 120120/144360  CE: 3.1752  
[05/07 20:41:36] SuperNet Training INFO: iter: 120240/144360  CE: 2.9294  
[05/07 20:42:05] SuperNet Training INFO: --> epoch: 100/120  avg CE: 3.0322  lr: 0.00803847577293368  
[05/07 20:43:13] SuperNet Training INFO: iter: 120360/144360  CE: 2.8664  
[05/07 20:44:14] SuperNet Training INFO: iter: 120480/144360  CE: 2.6512  
[05/07 20:45:15] SuperNet Training INFO: iter: 120600/144360  CE: 3.1630  
[05/07 20:46:16] SuperNet Training INFO: iter: 120720/144360  CE: 3.3817  
[05/07 20:47:18] SuperNet Training INFO: iter: 120840/144360  CE: 3.2298  
[05/07 20:48:18] SuperNet Training INFO: iter: 120960/144360  CE: 2.8632  
[05/07 20:49:19] SuperNet Training INFO: iter: 121080/144360  CE: 3.3043  
[05/07 20:50:21] SuperNet Training INFO: iter: 121200/144360  CE: 3.0498  
[05/07 20:51:22] SuperNet Training INFO: iter: 121320/144360  CE: 3.1538  
[05/07 20:52:23] SuperNet Training INFO: iter: 121440/144360  CE: 2.9658  
[05/07 20:52:54] SuperNet Training INFO: --> epoch: 101/120  avg CE: 3.0350  lr: 0.007270973240282054  
[05/07 20:53:59] SuperNet Training INFO: iter: 121560/144360  CE: 3.2049  
[05/07 20:54:59] SuperNet Training INFO: iter: 121680/144360  CE: 2.9307  
[05/07 20:55:59] SuperNet Training INFO: iter: 121800/144360  CE: 2.9745  
[05/07 20:56:59] SuperNet Training INFO: iter: 121920/144360  CE: 2.9007  
[05/07 20:57:59] SuperNet Training INFO: iter: 122040/144360  CE: 2.9882  
[05/07 20:59:00] SuperNet Training INFO: iter: 122160/144360  CE: 3.3766  
[05/07 21:00:00] SuperNet Training INFO: iter: 122280/144360  CE: 2.7950  
[05/07 21:01:00] SuperNet Training INFO: iter: 122400/144360  CE: 2.9955  
[05/07 21:02:00] SuperNet Training INFO: iter: 122520/144360  CE: 3.1150  
[05/07 21:02:58] SuperNet Training INFO: iter: 122640/144360  CE: 3.0711  
[05/07 21:03:29] SuperNet Training INFO: --> epoch: 102/120  avg CE: 3.0229  lr: 0.006539608548697928  
[05/07 21:04:34] SuperNet Training INFO: iter: 122760/144360  CE: 3.2239  
[05/07 21:05:36] SuperNet Training INFO: iter: 122880/144360  CE: 2.8201  
[05/07 21:06:37] SuperNet Training INFO: iter: 123000/144360  CE: 3.0376  
[05/07 21:07:38] SuperNet Training INFO: iter: 123120/144360  CE: 2.7666  
[05/07 21:08:37] SuperNet Training INFO: iter: 123240/144360  CE: 3.0511  
[05/07 21:09:37] SuperNet Training INFO: iter: 123360/144360  CE: 2.9380  
[05/07 21:10:38] SuperNet Training INFO: iter: 123480/144360  CE: 2.9832  
[05/07 21:11:37] SuperNet Training INFO: iter: 123600/144360  CE: 3.3250  
[05/07 21:12:37] SuperNet Training INFO: iter: 123720/144360  CE: 3.1417  
[05/07 21:13:37] SuperNet Training INFO: iter: 123840/144360  CE: 3.0221  
[05/07 21:14:11] SuperNet Training INFO: --> epoch: 103/120  avg CE: 3.0280  lr: 0.00584488293900834  
[05/07 21:15:14] SuperNet Training INFO: iter: 123960/144360  CE: 3.1113  
[05/07 21:16:16] SuperNet Training INFO: iter: 124080/144360  CE: 3.1061  
[05/07 21:17:18] SuperNet Training INFO: iter: 124200/144360  CE: 3.0063  
[05/07 21:18:19] SuperNet Training INFO: iter: 124320/144360  CE: 2.9690  
[05/07 21:19:20] SuperNet Training INFO: iter: 124440/144360  CE: 3.0868  
[05/07 21:20:21] SuperNet Training INFO: iter: 124560/144360  CE: 2.8241  
[05/07 21:21:23] SuperNet Training INFO: iter: 124680/144360  CE: 3.1410  
[05/07 21:22:24] SuperNet Training INFO: iter: 124800/144360  CE: 3.0078  
[05/07 21:23:23] SuperNet Training INFO: iter: 124920/144360  CE: 3.1352  
[05/07 21:24:23] SuperNet Training INFO: iter: 125040/144360  CE: 3.0527  
[05/07 21:24:58] SuperNet Training INFO: --> epoch: 104/120  avg CE: 3.0062  lr: 0.005187272541443939  
[05/07 21:25:58] SuperNet Training INFO: iter: 125160/144360  CE: 3.1419  
[05/07 21:26:59] SuperNet Training INFO: iter: 125280/144360  CE: 3.2090  
[05/07 21:28:00] SuperNet Training INFO: iter: 125400/144360  CE: 3.0191  
[05/07 21:29:01] SuperNet Training INFO: iter: 125520/144360  CE: 2.9984  
[05/07 21:30:03] SuperNet Training INFO: iter: 125640/144360  CE: 3.4901  
[05/07 21:31:05] SuperNet Training INFO: iter: 125760/144360  CE: 2.8016  
[05/07 21:32:06] SuperNet Training INFO: iter: 125880/144360  CE: 2.9625  
[05/07 21:33:08] SuperNet Training INFO: iter: 126000/144360  CE: 2.7302  
[05/07 21:34:09] SuperNet Training INFO: iter: 126120/144360  CE: 2.7857  
[05/07 21:35:10] SuperNet Training INFO: iter: 126240/144360  CE: 2.8989  
[05/07 21:35:48] SuperNet Training INFO: --> epoch: 105/120  avg CE: 3.0040  lr: 0.00456722804932279  
[05/07 21:36:46] SuperNet Training INFO: iter: 126360/144360  CE: 2.7853  
[05/07 21:37:47] SuperNet Training INFO: iter: 126480/144360  CE: 2.7855  
[05/07 21:38:48] SuperNet Training INFO: iter: 126600/144360  CE: 3.0335  
[05/07 21:39:49] SuperNet Training INFO: iter: 126720/144360  CE: 3.0689  
[05/07 21:40:50] SuperNet Training INFO: iter: 126840/144360  CE: 3.0342  
[05/07 21:41:50] SuperNet Training INFO: iter: 126960/144360  CE: 2.9391  
[05/07 21:42:50] SuperNet Training INFO: iter: 127080/144360  CE: 2.9239  
[05/07 21:43:51] SuperNet Training INFO: iter: 127200/144360  CE: 3.0239  
[05/07 21:44:49] SuperNet Training INFO: iter: 127320/144360  CE: 3.0344  
[05/07 21:45:49] SuperNet Training INFO: iter: 127440/144360  CE: 2.8134  
[05/07 21:46:26] SuperNet Training INFO: --> epoch: 106/120  avg CE: 2.9919  lr: 0.003985174410167894  
[05/07 21:47:23] SuperNet Training INFO: iter: 127560/144360  CE: 2.8871  
[05/07 21:48:24] SuperNet Training INFO: iter: 127680/144360  CE: 2.9778  
[05/07 21:49:25] SuperNet Training INFO: iter: 127800/144360  CE: 3.0035  
[05/07 21:50:26] SuperNet Training INFO: iter: 127920/144360  CE: 2.9647  
[05/07 21:51:28] SuperNet Training INFO: iter: 128040/144360  CE: 2.7493  
[05/07 21:52:29] SuperNet Training INFO: iter: 128160/144360  CE: 3.2054  
[05/07 21:53:30] SuperNet Training INFO: iter: 128280/144360  CE: 2.8966  
[05/07 21:54:31] SuperNet Training INFO: iter: 128400/144360  CE: 2.8080  
[05/07 21:55:32] SuperNet Training INFO: iter: 128520/144360  CE: 2.8624  
[05/07 21:56:32] SuperNet Training INFO: iter: 128640/144360  CE: 3.0064  
[05/07 21:57:12] SuperNet Training INFO: --> epoch: 107/120  avg CE: 2.9959  lr: 0.003441510534469298  
[05/07 21:58:08] SuperNet Training INFO: iter: 128760/144360  CE: 3.0257  
[05/07 21:59:09] SuperNet Training INFO: iter: 128880/144360  CE: 2.6045  
[05/07 22:00:09] SuperNet Training INFO: iter: 129000/144360  CE: 2.8783  
[05/07 22:01:10] SuperNet Training INFO: iter: 129120/144360  CE: 3.1453  
[05/07 22:02:11] SuperNet Training INFO: iter: 129240/144360  CE: 3.2598  
[05/07 22:03:12] SuperNet Training INFO: iter: 129360/144360  CE: 3.0582  
[05/07 22:04:13] SuperNet Training INFO: iter: 129480/144360  CE: 3.0820  
[05/07 22:05:14] SuperNet Training INFO: iter: 129600/144360  CE: 3.1507  
[05/07 22:06:16] SuperNet Training INFO: iter: 129720/144360  CE: 3.0159  
[05/07 22:07:17] SuperNet Training INFO: iter: 129840/144360  CE: 3.2914  
[05/07 22:08:00] SuperNet Training INFO: --> epoch: 108/120  avg CE: 2.9878  lr: 0.002936609022290792  
[05/07 22:08:54] SuperNet Training INFO: iter: 129960/144360  CE: 2.7568  
[05/07 22:09:56] SuperNet Training INFO: iter: 130080/144360  CE: 2.9909  
[05/07 22:10:58] SuperNet Training INFO: iter: 130200/144360  CE: 3.0585  
[05/07 22:12:01] SuperNet Training INFO: iter: 130320/144360  CE: 2.7654  
[05/07 22:13:02] SuperNet Training INFO: iter: 130440/144360  CE: 2.9447  
[05/07 22:14:03] SuperNet Training INFO: iter: 130560/144360  CE: 3.0180  
[05/07 22:15:05] SuperNet Training INFO: iter: 130680/144360  CE: 3.2109  
[05/07 22:16:07] SuperNet Training INFO: iter: 130800/144360  CE: 2.9990  
[05/07 22:17:08] SuperNet Training INFO: iter: 130920/144360  CE: 3.3137  
[05/07 22:18:09] SuperNet Training INFO: iter: 131040/144360  CE: 3.0300  
[05/07 22:18:52] SuperNet Training INFO: --> epoch: 109/120  avg CE: 2.9843  lr: 0.0024708159079084185  
[05/07 22:19:45] SuperNet Training INFO: iter: 131160/144360  CE: 3.0598  
[05/07 22:20:46] SuperNet Training INFO: iter: 131280/144360  CE: 2.9576  
[05/07 22:21:47] SuperNet Training INFO: iter: 131400/144360  CE: 2.5912  
[05/07 22:22:48] SuperNet Training INFO: iter: 131520/144360  CE: 2.9682  
[05/07 22:23:48] SuperNet Training INFO: iter: 131640/144360  CE: 2.9424  
[05/07 22:24:48] SuperNet Training INFO: iter: 131760/144360  CE: 2.9120  
[05/07 22:25:48] SuperNet Training INFO: iter: 131880/144360  CE: 3.1753  
[05/07 22:26:48] SuperNet Training INFO: iter: 132000/144360  CE: 3.1669  
[05/07 22:27:48] SuperNet Training INFO: iter: 132120/144360  CE: 3.1552  
[05/07 22:28:49] SuperNet Training INFO: iter: 132240/144360  CE: 2.8645  
[05/07 22:29:33] SuperNet Training INFO: --> epoch: 110/120  avg CE: 2.9865  lr: 0.0020444504226559065  
[05/07 22:30:24] SuperNet Training INFO: iter: 132360/144360  CE: 3.2048  
[05/07 22:31:24] SuperNet Training INFO: iter: 132480/144360  CE: 3.5653  
[05/07 22:32:25] SuperNet Training INFO: iter: 132600/144360  CE: 3.0652  
[05/07 22:33:26] SuperNet Training INFO: iter: 132720/144360  CE: 3.0268  
[05/07 22:34:26] SuperNet Training INFO: iter: 132840/144360  CE: 3.2404  
[05/07 22:35:27] SuperNet Training INFO: iter: 132960/144360  CE: 2.9042  
[05/07 22:36:28] SuperNet Training INFO: iter: 133080/144360  CE: 3.1423  
[05/07 22:37:28] SuperNet Training INFO: iter: 133200/144360  CE: 2.7328  
[05/07 22:38:30] SuperNet Training INFO: iter: 133320/144360  CE: 3.0749  
[05/07 22:39:29] SuperNet Training INFO: iter: 133440/144360  CE: 3.1579  
[05/07 22:40:15] SuperNet Training INFO: --> epoch: 111/120  avg CE: 2.9747  lr: 0.0016578047761394107  
[05/07 22:41:04] SuperNet Training INFO: iter: 133560/144360  CE: 2.9100  
[05/07 22:42:05] SuperNet Training INFO: iter: 133680/144360  CE: 3.1065  
[05/07 22:43:07] SuperNet Training INFO: iter: 133800/144360  CE: 3.2410  
[05/07 22:44:08] SuperNet Training INFO: iter: 133920/144360  CE: 3.3491  
[05/07 22:45:09] SuperNet Training INFO: iter: 134040/144360  CE: 2.8941  
[05/07 22:46:11] SuperNet Training INFO: iter: 134160/144360  CE: 3.0031  
[05/07 22:47:11] SuperNet Training INFO: iter: 134280/144360  CE: 3.1293  
[05/07 22:48:11] SuperNet Training INFO: iter: 134400/144360  CE: 2.6852  
[05/07 22:49:11] SuperNet Training INFO: iter: 134520/144360  CE: 2.8026  
[05/07 22:50:12] SuperNet Training INFO: iter: 134640/144360  CE: 2.8198  
[05/07 22:51:00] SuperNet Training INFO: --> epoch: 112/120  avg CE: 2.9835  lr: 0.0013111439559716617  
[05/07 22:51:48] SuperNet Training INFO: iter: 134760/144360  CE: 2.9004  
[05/07 22:52:48] SuperNet Training INFO: iter: 134880/144360  CE: 2.9871  
[05/07 22:53:50] SuperNet Training INFO: iter: 135000/144360  CE: 3.0336  
[05/07 22:54:50] SuperNet Training INFO: iter: 135120/144360  CE: 2.7615  
[05/07 22:55:51] SuperNet Training INFO: iter: 135240/144360  CE: 3.0291  
[05/07 22:56:52] SuperNet Training INFO: iter: 135360/144360  CE: 2.9955  
[05/07 22:57:52] SuperNet Training INFO: iter: 135480/144360  CE: 3.0882  
[05/07 22:58:52] SuperNet Training INFO: iter: 135600/144360  CE: 3.0522  
[05/07 22:59:52] SuperNet Training INFO: iter: 135720/144360  CE: 2.8819  
[05/07 23:00:52] SuperNet Training INFO: iter: 135840/144360  CE: 2.7126  
[05/07 23:01:41] SuperNet Training INFO: --> epoch: 113/120  avg CE: 2.9773  lr: 0.0010047055461627253  
[05/07 23:02:27] SuperNet Training INFO: iter: 135960/144360  CE: 3.2673  
[05/07 23:03:27] SuperNet Training INFO: iter: 136080/144360  CE: 3.1912  
[05/07 23:04:28] SuperNet Training INFO: iter: 136200/144360  CE: 2.7055  
[05/07 23:05:29] SuperNet Training INFO: iter: 136320/144360  CE: 2.9500  
[05/07 23:06:29] SuperNet Training INFO: iter: 136440/144360  CE: 3.0503  
[05/07 23:07:31] SuperNet Training INFO: iter: 136560/144360  CE: 2.8736  
[05/07 23:08:31] SuperNet Training INFO: iter: 136680/144360  CE: 3.0817  
[05/07 23:09:31] SuperNet Training INFO: iter: 136800/144360  CE: 3.0434  
[05/07 23:10:32] SuperNet Training INFO: iter: 136920/144360  CE: 2.8393  
[05/07 23:11:33] SuperNet Training INFO: iter: 137040/144360  CE: 3.2756  
[05/07 23:12:23] SuperNet Training INFO: --> epoch: 114/120  avg CE: 2.9685  lr: 0.000738699564291742  
[05/07 23:13:08] SuperNet Training INFO: iter: 137160/144360  CE: 2.8324  
[05/07 23:14:09] SuperNet Training INFO: iter: 137280/144360  CE: 2.6518  
[05/07 23:15:10] SuperNet Training INFO: iter: 137400/144360  CE: 2.7940  
[05/07 23:16:11] SuperNet Training INFO: iter: 137520/144360  CE: 2.8701  
[05/07 23:17:13] SuperNet Training INFO: iter: 137640/144360  CE: 2.6565  
[05/07 23:18:14] SuperNet Training INFO: iter: 137760/144360  CE: 3.1821  
[05/07 23:19:14] SuperNet Training INFO: iter: 137880/144360  CE: 2.8328  
[05/07 23:20:14] SuperNet Training INFO: iter: 138000/144360  CE: 3.0637  
[05/07 23:21:14] SuperNet Training INFO: iter: 138120/144360  CE: 2.7760  
[05/07 23:22:15] SuperNet Training INFO: iter: 138240/144360  CE: 3.1510  
[05/07 23:23:07] SuperNet Training INFO: --> epoch: 115/120  avg CE: 2.9684  lr: 0.0005133083175713779  
[05/07 23:23:51] SuperNet Training INFO: iter: 138360/144360  CE: 2.8454  
[05/07 23:24:51] SuperNet Training INFO: iter: 138480/144360  CE: 3.3668  
[05/07 23:25:52] SuperNet Training INFO: iter: 138600/144360  CE: 2.9511  
[05/07 23:26:53] SuperNet Training INFO: iter: 138720/144360  CE: 2.9262  
[05/07 23:27:53] SuperNet Training INFO: iter: 138840/144360  CE: 2.9924  
[05/07 23:28:53] SuperNet Training INFO: iter: 138960/144360  CE: 3.0476  
[05/07 23:29:55] SuperNet Training INFO: iter: 139080/144360  CE: 3.1562  
[05/07 23:30:56] SuperNet Training INFO: iter: 139200/144360  CE: 2.9753  
[05/07 23:31:56] SuperNet Training INFO: iter: 139320/144360  CE: 2.9355  
[05/07 23:32:56] SuperNet Training INFO: iter: 139440/144360  CE: 3.0418  
[05/07 23:33:50] SuperNet Training INFO: --> epoch: 116/120  avg CE: 2.9659  lr: 0.00032868627790359544  
[05/07 23:34:32] SuperNet Training INFO: iter: 139560/144360  CE: 2.8649  
[05/07 23:35:32] SuperNet Training INFO: iter: 139680/144360  CE: 2.7711  
[05/07 23:36:31] SuperNet Training INFO: iter: 139800/144360  CE: 2.8727  
[05/07 23:37:32] SuperNet Training INFO: iter: 139920/144360  CE: 2.9042  
[05/07 23:38:33] SuperNet Training INFO: iter: 140040/144360  CE: 2.9302  
[05/07 23:39:35] SuperNet Training INFO: iter: 140160/144360  CE: 3.0445  
[05/07 23:40:36] SuperNet Training INFO: iter: 140280/144360  CE: 2.8390  
[05/07 23:41:36] SuperNet Training INFO: iter: 140400/144360  CE: 3.0381  
[05/07 23:42:37] SuperNet Training INFO: iter: 140520/144360  CE: 3.0498  
[05/07 23:43:37] SuperNet Training INFO: iter: 140640/144360  CE: 3.1633  
[05/07 23:44:31] SuperNet Training INFO: --> epoch: 117/120  avg CE: 2.9651  lr: 0.00018495997601232129  
[05/07 23:45:12] SuperNet Training INFO: iter: 140760/144360  CE: 2.8703  
[05/07 23:46:13] SuperNet Training INFO: iter: 140880/144360  CE: 3.1654  
[05/07 23:47:14] SuperNet Training INFO: iter: 141000/144360  CE: 3.1636  
[05/07 23:48:15] SuperNet Training INFO: iter: 141120/144360  CE: 3.0522  
[05/07 23:49:14] SuperNet Training INFO: iter: 141240/144360  CE: 3.1696  
[05/07 23:50:15] SuperNet Training INFO: iter: 141360/144360  CE: 2.7463  
[05/07 23:51:17] SuperNet Training INFO: iter: 141480/144360  CE: 2.8740  
[05/07 23:52:17] SuperNet Training INFO: iter: 141600/144360  CE: 2.8490  
[05/07 23:53:18] SuperNet Training INFO: iter: 141720/144360  CE: 2.9048  
[05/07 23:54:18] SuperNet Training INFO: iter: 141840/144360  CE: 2.7859  
[05/07 23:55:15] SuperNet Training INFO: --> epoch: 118/120  avg CE: 2.9584  lr: 8.222791472556962e-05  
[05/07 23:55:54] SuperNet Training INFO: iter: 141960/144360  CE: 2.7700  
[05/07 23:56:54] SuperNet Training INFO: iter: 142080/144360  CE: 2.7625  
[05/07 23:57:54] SuperNet Training INFO: iter: 142200/144360  CE: 3.1546  
[05/07 23:58:54] SuperNet Training INFO: iter: 142320/144360  CE: 3.0553  
[05/07 23:59:54] SuperNet Training INFO: iter: 142440/144360  CE: 2.9446  
[05/08 00:00:54] SuperNet Training INFO: iter: 142560/144360  CE: 3.0014  
[05/08 00:01:55] SuperNet Training INFO: iter: 142680/144360  CE: 3.0006  
[05/08 00:02:55] SuperNet Training INFO: iter: 142800/144360  CE: 2.9815  
[05/08 00:03:55] SuperNet Training INFO: iter: 142920/144360  CE: 2.9949  
[05/08 00:04:54] SuperNet Training INFO: iter: 143040/144360  CE: 2.8385  
[05/08 00:05:52] SuperNet Training INFO: --> epoch: 119/120  avg CE: 2.9509  lr: 2.0560501466564365e-05  
[05/08 00:06:29] SuperNet Training INFO: iter: 143160/144360  CE: 2.9323  
[05/08 00:07:29] SuperNet Training INFO: iter: 143280/144360  CE: 2.9296  
[05/08 00:08:30] SuperNet Training INFO: iter: 143400/144360  CE: 2.6513  
[05/08 00:09:30] SuperNet Training INFO: iter: 143520/144360  CE: 2.6888  
[05/08 00:10:32] SuperNet Training INFO: iter: 143640/144360  CE: 3.0663  
[05/08 00:11:33] SuperNet Training INFO: iter: 143760/144360  CE: 3.1143  
[05/08 00:12:34] SuperNet Training INFO: iter: 143880/144360  CE: 3.0111  
[05/08 00:13:35] SuperNet Training INFO: iter: 144000/144360  CE: 3.1885  
[05/08 00:14:37] SuperNet Training INFO: iter: 144120/144360  CE: 2.9474  
[05/08 00:15:37] SuperNet Training INFO: iter: 144240/144360  CE: 2.9339  
[05/08 00:16:37] SuperNet Training INFO: iter: 144360/144360  CE: 2.8540  
[05/08 00:16:37] SuperNet Training INFO: --> epoch: 120/120  avg CE: 2.9611  lr: 0.0  
[05/08 00:16:37] SuperNet Training INFO: --> END mobile0-tbs3-seed-0
[05/08 00:16:37] SuperNet Training INFO: {0: 72180, 1: 72180}
[05/08 00:16:43] SuperNet Training INFO: ELAPSED TIME: 77454.4(s) = 21(h) 30(m)
