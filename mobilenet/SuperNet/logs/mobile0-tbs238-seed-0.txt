[05/05 07:00:46] SuperNet Training INFO: tag                 : mobile0-tbs238
[05/05 07:00:46] SuperNet Training INFO: seed                : 0
[05/05 07:00:46] SuperNet Training INFO: thresholds          : [38]
[05/05 07:00:46] SuperNet Training INFO: data_path           : ../../../dataset/ILSVRC2012
[05/05 07:00:46] SuperNet Training INFO: save_path           : ./SuperNet
[05/05 07:00:46] SuperNet Training INFO: search_space        : proxyless
[05/05 07:00:46] SuperNet Training INFO: valid_size          : 50000
[05/05 07:00:46] SuperNet Training INFO: num_gpus            : 8
[05/05 07:00:46] SuperNet Training INFO: workers             : 4
[05/05 07:00:46] SuperNet Training INFO: interval_ep_eval    : 8
[05/05 07:00:46] SuperNet Training INFO: train_batch_size    : 1024
[05/05 07:00:46] SuperNet Training INFO: test_batch_size     : 256
[05/05 07:00:46] SuperNet Training INFO: max_epoch           : 120
[05/05 07:00:46] SuperNet Training INFO: learning_rate       : 0.12
[05/05 07:00:46] SuperNet Training INFO: momentum            : 0.9
[05/05 07:00:46] SuperNet Training INFO: weight_decay        : 4e-05
[05/05 07:00:46] SuperNet Training INFO: nesterov            : True
[05/05 07:00:46] SuperNet Training INFO: lr_schedule_type    : cosine
[05/05 07:00:46] SuperNet Training INFO: warmup              : False
[05/05 07:00:46] SuperNet Training INFO: label_smooth        : 0.1
[05/05 07:00:46] SuperNet Training INFO: rank                : 0
[05/05 07:00:46] SuperNet Training INFO: gpu                 : 0
[05/05 07:00:46] SuperNet Training INFO: save_name           : mobile0-tbs238-seed-0
[05/05 07:00:46] SuperNet Training INFO: log_path            : ./SuperNet/logs/mobile0-tbs238-seed-0.txt
[05/05 07:00:46] SuperNet Training INFO: ckpt_path           : ./SuperNet/checkpoint/mobile0-tbs238-seed-0.pt
[05/05 07:00:46] SuperNet Training INFO: dist_url            : tcp://127.0.0.1:23456
[05/05 07:00:46] SuperNet Training INFO: world_size          : 8
[05/05 07:00:46] SuperNet Training INFO: distributed         : True
[05/05 07:00:46] SuperNet Training INFO: ['3x3_MBConv3', '3x3_MBConv6', '5x5_MBConv3', '5x5_MBConv6', '7x7_MBConv3', '7x7_MBConv6', 'Identity']
[05/05 07:01:23] SuperNet Training INFO: DistributedDataParallel(
  (module): SuperNet(
    (first_conv): Sequential(
      (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
      (2): ReLU6(inplace=True)
    )
    (first_block): InvertedResidual(
      (depth_conv): Sequential(
        (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)
        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
        (2): ReLU6(inplace=True)
      )
      (point_linear): Sequential(
        (0): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
      )
    )
    (blocks): ModuleList(
      (0): ModuleList(
        (0): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(16, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(48, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=48, bias=False)
            (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(48, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (1): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)
            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(96, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (2): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(16, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(48, 48, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=48, bias=False)
            (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(48, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (3): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(96, 96, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=96, bias=False)
            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(96, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (4): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(16, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(48, 48, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), groups=48, bias=False)
            (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(48, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (5): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(96, 96, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), groups=96, bias=False)
            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(96, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
      )
      (1): ModuleList(
        (0): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(32, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(96, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (1): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (2): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(32, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96, bias=False)
            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(96, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (3): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(192, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=192, bias=False)
            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (4): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(32, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96, bias=False)
            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(96, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (5): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192, bias=False)
            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (6): Identity()
      )
      (2): ModuleList(
        (0): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(32, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(96, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (1): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (2): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(32, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96, bias=False)
            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(96, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (3): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(192, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=192, bias=False)
            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (4): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(32, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96, bias=False)
            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(96, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (5): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192, bias=False)
            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (6): Identity()
      )
      (3): ModuleList(
        (0): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(32, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(96, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (1): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (2): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(32, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96, bias=False)
            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(96, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (3): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(192, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=192, bias=False)
            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (4): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(32, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96, bias=False)
            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(96, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (5): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192, bias=False)
            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (6): Identity()
      )
      (4): ModuleList(
        (0): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(32, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)
            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(96, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (1): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)
            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(192, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (2): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(32, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(96, 96, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=96, bias=False)
            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(96, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (3): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(192, 192, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=192, bias=False)
            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(192, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (4): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(32, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(96, 96, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), groups=96, bias=False)
            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(96, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (5): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(192, 192, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), groups=192, bias=False)
            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(192, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
      )
      (5): ModuleList(
        (0): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(120, 120, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=120, bias=False)
            (1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(120, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (1): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(240, 240, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=240, bias=False)
            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (2): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(120, 120, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=120, bias=False)
            (1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(120, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (3): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(240, 240, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=240, bias=False)
            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (4): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(120, 120, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=120, bias=False)
            (1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(120, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (5): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(240, 240, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=240, bias=False)
            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (6): Identity()
      )
      (6): ModuleList(
        (0): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(120, 120, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=120, bias=False)
            (1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(120, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (1): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(240, 240, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=240, bias=False)
            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (2): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(120, 120, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=120, bias=False)
            (1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(120, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (3): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(240, 240, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=240, bias=False)
            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (4): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(120, 120, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=120, bias=False)
            (1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(120, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (5): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(240, 240, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=240, bias=False)
            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (6): Identity()
      )
      (7): ModuleList(
        (0): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(120, 120, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=120, bias=False)
            (1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(120, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (1): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(240, 240, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=240, bias=False)
            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (2): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(120, 120, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=120, bias=False)
            (1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(120, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (3): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(240, 240, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=240, bias=False)
            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (4): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(120, 120, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=120, bias=False)
            (1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(120, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (5): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(240, 240, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=240, bias=False)
            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (6): Identity()
      )
      (8): ModuleList(
        (0): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(120, 120, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=120, bias=False)
            (1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(120, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (1): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(240, 240, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=240, bias=False)
            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (2): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(120, 120, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=120, bias=False)
            (1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(120, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (3): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(240, 240, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=240, bias=False)
            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (4): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(120, 120, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), groups=120, bias=False)
            (1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(120, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (5): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(240, 240, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), groups=240, bias=False)
            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
      )
      (9): ModuleList(
        (0): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(80, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(240, 240, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=240, bias=False)
            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (1): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)
            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (2): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(80, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(240, 240, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=240, bias=False)
            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (3): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(480, 480, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=480, bias=False)
            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (4): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(80, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(240, 240, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=240, bias=False)
            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (5): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(480, 480, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=480, bias=False)
            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (6): Identity()
      )
      (10): ModuleList(
        (0): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(80, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(240, 240, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=240, bias=False)
            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (1): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)
            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (2): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(80, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(240, 240, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=240, bias=False)
            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (3): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(480, 480, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=480, bias=False)
            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (4): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(80, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(240, 240, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=240, bias=False)
            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (5): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(480, 480, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=480, bias=False)
            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (6): Identity()
      )
      (11): ModuleList(
        (0): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(80, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(240, 240, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=240, bias=False)
            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (1): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)
            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (2): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(80, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(240, 240, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=240, bias=False)
            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (3): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(480, 480, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=480, bias=False)
            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (4): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(80, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(240, 240, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=240, bias=False)
            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (5): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(480, 480, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=480, bias=False)
            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (6): Identity()
      )
      (12): ModuleList(
        (0): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(80, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(240, 240, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=240, bias=False)
            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(240, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (1): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)
            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(480, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (2): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(80, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(240, 240, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=240, bias=False)
            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(240, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (3): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(480, 480, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=480, bias=False)
            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(480, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (4): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(80, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(240, 240, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=240, bias=False)
            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(240, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (5): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(480, 480, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=480, bias=False)
            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(480, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
      )
      (13): ModuleList(
        (0): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(96, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
            (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(288, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (1): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)
            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (2): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(96, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(288, 288, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=288, bias=False)
            (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(288, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (3): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(576, 576, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=576, bias=False)
            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (4): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(96, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(288, 288, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=288, bias=False)
            (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(288, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (5): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(576, 576, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=576, bias=False)
            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (6): Identity()
      )
      (14): ModuleList(
        (0): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(96, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
            (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(288, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (1): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)
            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (2): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(96, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(288, 288, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=288, bias=False)
            (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(288, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (3): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(576, 576, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=576, bias=False)
            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (4): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(96, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(288, 288, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=288, bias=False)
            (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(288, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (5): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(576, 576, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=576, bias=False)
            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (6): Identity()
      )
      (15): ModuleList(
        (0): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(96, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
            (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(288, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (1): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)
            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (2): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(96, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(288, 288, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=288, bias=False)
            (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(288, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (3): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(576, 576, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=576, bias=False)
            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (4): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(96, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(288, 288, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=288, bias=False)
            (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(288, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (5): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(576, 576, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=576, bias=False)
            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (6): Identity()
      )
      (16): ModuleList(
        (0): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(96, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(288, 288, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=288, bias=False)
            (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(288, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (1): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=576, bias=False)
            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(576, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (2): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(96, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(288, 288, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=288, bias=False)
            (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(288, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (3): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(576, 576, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=576, bias=False)
            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(576, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (4): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(96, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(288, 288, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), groups=288, bias=False)
            (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(288, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (5): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(576, 576, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), groups=576, bias=False)
            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(576, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
      )
      (17): ModuleList(
        (0): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(192, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)
            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(576, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (1): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152, bias=False)
            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (2): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(192, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(576, 576, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=576, bias=False)
            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(576, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (3): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)
            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (4): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(192, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(576, 576, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=576, bias=False)
            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(576, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (5): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(1152, 1152, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1152, bias=False)
            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (6): Identity()
      )
      (18): ModuleList(
        (0): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(192, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)
            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(576, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (1): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152, bias=False)
            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (2): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(192, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(576, 576, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=576, bias=False)
            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(576, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (3): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)
            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (4): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(192, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(576, 576, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=576, bias=False)
            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(576, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (5): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(1152, 1152, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1152, bias=False)
            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (6): Identity()
      )
      (19): ModuleList(
        (0): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(192, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)
            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(576, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (1): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152, bias=False)
            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (2): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(192, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(576, 576, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=576, bias=False)
            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(576, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (3): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)
            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (4): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(192, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(576, 576, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=576, bias=False)
            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(576, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (5): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(1152, 1152, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1152, bias=False)
            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (6): Identity()
      )
      (20): ModuleList(
        (0): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(192, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)
            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(576, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (1): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152, bias=False)
            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(1152, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (2): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(192, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(576, 576, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=576, bias=False)
            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(576, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (3): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)
            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(1152, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (4): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(192, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(576, 576, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=576, bias=False)
            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(576, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (5): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(1152, 1152, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1152, bias=False)
            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(1152, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
      )
    )
    (feature_mix_layer): Sequential(
      (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
      (2): ReLU6(inplace=True)
    )
    (avgpool): AvgPool2d(kernel_size=7, stride=7, padding=0)
    (classifier): Sequential(
      (0): Linear(in_features=1280, out_features=1000, bias=True)
    )
  )
)
[05/05 07:01:49] SuperNet Training INFO: Trainset Size: 1231167
[05/05 07:01:49] SuperNet Training INFO: Validset Size:   50000
[05/05 07:01:49] SuperNet Training INFO: Compose(
    RandomResizedCrop(size=(224, 224), scale=(0.08, 1.0), ratio=(0.75, 1.3333), interpolation=bilinear)
    RandomHorizontalFlip(p=0.5)
    ToTensor()
    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
)
[05/05 07:01:49] SuperNet Training INFO: --> START mobile0-tbs238-seed-0
[05/05 07:01:49] SuperNet Training INFO: {0: 0, 1: 0}
[05/05 07:04:18] SuperNet Training INFO: iter:   120/144360  CE: 6.9461  
[05/05 07:05:19] SuperNet Training INFO: iter:   240/144360  CE: 6.8401  
[05/05 07:06:19] SuperNet Training INFO: iter:   360/144360  CE: 6.8659  
[05/05 07:07:19] SuperNet Training INFO: iter:   480/144360  CE: 6.9421  
[05/05 07:08:20] SuperNet Training INFO: iter:   600/144360  CE: 6.8670  
[05/05 07:09:20] SuperNet Training INFO: iter:   720/144360  CE: 6.9370  
[05/05 07:10:21] SuperNet Training INFO: iter:   840/144360  CE: 6.8666  
[05/05 07:11:22] SuperNet Training INFO: iter:   960/144360  CE: 6.8843  
[05/05 07:12:23] SuperNet Training INFO: iter:  1080/144360  CE: 6.8584  
[05/05 07:13:23] SuperNet Training INFO: iter:  1200/144360  CE: 6.9270  
[05/05 07:13:23] SuperNet Training INFO: --> epoch:   1/120  avg CE: 6.8997  lr: 0.11997943949853311  
[05/05 07:14:58] SuperNet Training INFO: iter:  1320/144360  CE: 6.8322  
[05/05 07:15:57] SuperNet Training INFO: iter:  1440/144360  CE: 6.7561  
[05/05 07:16:56] SuperNet Training INFO: iter:  1560/144360  CE: 6.7818  
[05/05 07:17:55] SuperNet Training INFO: iter:  1680/144360  CE: 6.7138  
[05/05 07:18:54] SuperNet Training INFO: iter:  1800/144360  CE: 6.8288  
[05/05 07:19:52] SuperNet Training INFO: iter:  1920/144360  CE: 6.7444  
[05/05 07:20:50] SuperNet Training INFO: iter:  2040/144360  CE: 6.8449  
[05/05 07:21:48] SuperNet Training INFO: iter:  2160/144360  CE: 6.6942  
[05/05 07:22:47] SuperNet Training INFO: iter:  2280/144360  CE: 6.6209  
[05/05 07:23:45] SuperNet Training INFO: iter:  2400/144360  CE: 6.6393  
[05/05 07:23:47] SuperNet Training INFO: --> epoch:   2/120  avg CE: 6.7726  lr: 0.11991777208527424  
[05/05 07:25:18] SuperNet Training INFO: iter:  2520/144360  CE: 6.6911  
[05/05 07:26:18] SuperNet Training INFO: iter:  2640/144360  CE: 6.7321  
[05/05 07:27:19] SuperNet Training INFO: iter:  2760/144360  CE: 6.6340  
[05/05 07:28:21] SuperNet Training INFO: iter:  2880/144360  CE: 6.6032  
[05/05 07:29:21] SuperNet Training INFO: iter:  3000/144360  CE: 6.7729  
[05/05 07:30:22] SuperNet Training INFO: iter:  3120/144360  CE: 6.6198  
[05/05 07:31:22] SuperNet Training INFO: iter:  3240/144360  CE: 6.6079  
[05/05 07:32:20] SuperNet Training INFO: iter:  3360/144360  CE: 6.6740  
[05/05 07:33:20] SuperNet Training INFO: iter:  3480/144360  CE: 6.5461  
[05/05 07:34:19] SuperNet Training INFO: iter:  3600/144360  CE: 6.5889  
[05/05 07:34:22] SuperNet Training INFO: --> epoch:   3/120  avg CE: 6.6368  lr: 0.11981504002398749  
[05/05 07:35:53] SuperNet Training INFO: iter:  3720/144360  CE: 6.5265  
[05/05 07:36:53] SuperNet Training INFO: iter:  3840/144360  CE: 6.6510  
[05/05 07:37:52] SuperNet Training INFO: iter:  3960/144360  CE: 6.5304  
[05/05 07:38:51] SuperNet Training INFO: iter:  4080/144360  CE: 6.5162  
[05/05 07:39:53] SuperNet Training INFO: iter:  4200/144360  CE: 6.5104  
[05/05 07:40:52] SuperNet Training INFO: iter:  4320/144360  CE: 6.2888  
[05/05 07:41:52] SuperNet Training INFO: iter:  4440/144360  CE: 6.4602  
[05/05 07:42:53] SuperNet Training INFO: iter:  4560/144360  CE: 6.3192  
[05/05 07:43:54] SuperNet Training INFO: iter:  4680/144360  CE: 6.2216  
[05/05 07:44:55] SuperNet Training INFO: iter:  4800/144360  CE: 6.3880  
[05/05 07:45:00] SuperNet Training INFO: --> epoch:   4/120  avg CE: 6.4764  lr: 0.11967131372209595  
[05/05 07:46:29] SuperNet Training INFO: iter:  4920/144360  CE: 6.4581  
[05/05 07:47:31] SuperNet Training INFO: iter:  5040/144360  CE: 6.2346  
[05/05 07:48:29] SuperNet Training INFO: iter:  5160/144360  CE: 6.2682  
[05/05 07:49:28] SuperNet Training INFO: iter:  5280/144360  CE: 6.2631  
[05/05 07:50:27] SuperNet Training INFO: iter:  5400/144360  CE: 6.2380  
[05/05 07:51:25] SuperNet Training INFO: iter:  5520/144360  CE: 6.4357  
[05/05 07:52:23] SuperNet Training INFO: iter:  5640/144360  CE: 6.2877  
[05/05 07:53:20] SuperNet Training INFO: iter:  5760/144360  CE: 6.3515  
[05/05 07:54:19] SuperNet Training INFO: iter:  5880/144360  CE: 6.1646  
[05/05 07:55:16] SuperNet Training INFO: iter:  6000/144360  CE: 6.0728  
[05/05 07:55:22] SuperNet Training INFO: --> epoch:   5/120  avg CE: 6.2881  lr: 0.11948669168242801  
[05/05 07:56:49] SuperNet Training INFO: iter:  6120/144360  CE: 6.1567  
[05/05 07:57:48] SuperNet Training INFO: iter:  6240/144360  CE: 6.2142  
[05/05 07:58:47] SuperNet Training INFO: iter:  6360/144360  CE: 5.9879  
[05/05 07:59:48] SuperNet Training INFO: iter:  6480/144360  CE: 6.0872  
[05/05 08:00:48] SuperNet Training INFO: iter:  6600/144360  CE: 6.0054  
[05/05 08:01:47] SuperNet Training INFO: iter:  6720/144360  CE: 5.9251  
[05/05 08:02:46] SuperNet Training INFO: iter:  6840/144360  CE: 6.2192  
[05/05 08:03:46] SuperNet Training INFO: iter:  6960/144360  CE: 6.0854  
[05/05 08:04:43] SuperNet Training INFO: iter:  7080/144360  CE: 6.0531  
[05/05 08:05:40] SuperNet Training INFO: iter:  7200/144360  CE: 6.1120  
[05/05 08:05:48] SuperNet Training INFO: --> epoch:   6/120  avg CE: 6.0960  lr: 0.1192613004357081  
[05/05 08:07:16] SuperNet Training INFO: iter:  7320/144360  CE: 6.0436  
[05/05 08:08:15] SuperNet Training INFO: iter:  7440/144360  CE: 5.9830  
[05/05 08:09:15] SuperNet Training INFO: iter:  7560/144360  CE: 5.9468  
[05/05 08:10:16] SuperNet Training INFO: iter:  7680/144360  CE: 5.9462  
[05/05 08:11:16] SuperNet Training INFO: iter:  7800/144360  CE: 5.8943  
[05/05 08:12:16] SuperNet Training INFO: iter:  7920/144360  CE: 6.0231  
[05/05 08:13:15] SuperNet Training INFO: iter:  8040/144360  CE: 6.0851  
[05/05 08:14:14] SuperNet Training INFO: iter:  8160/144360  CE: 6.0682  
[05/05 08:15:13] SuperNet Training INFO: iter:  8280/144360  CE: 5.9298  
[05/05 08:16:13] SuperNet Training INFO: iter:  8400/144360  CE: 5.8921  
[05/05 08:16:22] SuperNet Training INFO: --> epoch:   7/120  avg CE: 5.9341  lr: 0.11899529445383715  
[05/05 08:17:46] SuperNet Training INFO: iter:  8520/144360  CE: 5.9134  
[05/05 08:18:45] SuperNet Training INFO: iter:  8640/144360  CE: 5.7275  
[05/05 08:19:44] SuperNet Training INFO: iter:  8760/144360  CE: 5.7962  
[05/05 08:20:42] SuperNet Training INFO: iter:  8880/144360  CE: 5.9799  
[05/05 08:21:41] SuperNet Training INFO: iter:  9000/144360  CE: 5.6648  
[05/05 08:22:39] SuperNet Training INFO: iter:  9120/144360  CE: 5.6247  
[05/05 08:23:38] SuperNet Training INFO: iter:  9240/144360  CE: 5.7318  
[05/05 08:24:37] SuperNet Training INFO: iter:  9360/144360  CE: 5.6767  
[05/05 08:25:35] SuperNet Training INFO: iter:  9480/144360  CE: 5.9032  
[05/05 08:26:34] SuperNet Training INFO: iter:  9600/144360  CE: 5.7844  
[05/05 08:26:44] SuperNet Training INFO: --> epoch:   8/120  avg CE: 5.7892  lr: 0.11868885604402826  
[05/05 08:28:07] SuperNet Training INFO: iter:  9720/144360  CE: 5.9117  
[05/05 08:29:07] SuperNet Training INFO: iter:  9840/144360  CE: 5.8196  
[05/05 08:30:07] SuperNet Training INFO: iter:  9960/144360  CE: 5.6586  
[05/05 08:31:06] SuperNet Training INFO: iter: 10080/144360  CE: 5.6247  
[05/05 08:32:06] SuperNet Training INFO: iter: 10200/144360  CE: 5.7037  
[05/05 08:33:06] SuperNet Training INFO: iter: 10320/144360  CE: 5.5424  
[05/05 08:34:05] SuperNet Training INFO: iter: 10440/144360  CE: 5.6987  
[05/05 08:35:06] SuperNet Training INFO: iter: 10560/144360  CE: 5.5790  
[05/05 08:36:06] SuperNet Training INFO: iter: 10680/144360  CE: 5.7863  
[05/05 08:37:05] SuperNet Training INFO: iter: 10800/144360  CE: 5.3987  
[05/05 08:37:17] SuperNet Training INFO: --> epoch:   9/120  avg CE: 5.6679  lr: 0.11834219522386061  
[05/05 08:38:40] SuperNet Training INFO: iter: 10920/144360  CE: 5.5245  
[05/05 08:39:41] SuperNet Training INFO: iter: 11040/144360  CE: 5.3755  
[05/05 08:40:42] SuperNet Training INFO: iter: 11160/144360  CE: 5.5314  
[05/05 08:41:40] SuperNet Training INFO: iter: 11280/144360  CE: 5.4629  
[05/05 08:42:39] SuperNet Training INFO: iter: 11400/144360  CE: 5.4428  
[05/05 08:43:38] SuperNet Training INFO: iter: 11520/144360  CE: 5.6790  
[05/05 08:44:38] SuperNet Training INFO: iter: 11640/144360  CE: 5.5390  
[05/05 08:45:36] SuperNet Training INFO: iter: 11760/144360  CE: 5.3434  
[05/05 08:46:35] SuperNet Training INFO: iter: 11880/144360  CE: 5.3040  
[05/05 08:47:34] SuperNet Training INFO: iter: 12000/144360  CE: 5.5337  
[05/05 08:47:48] SuperNet Training INFO: --> epoch:  10/120  avg CE: 5.5399  lr: 0.1179555495773443  
[05/05 08:49:09] SuperNet Training INFO: iter: 12120/144360  CE: 5.6026  
[05/05 08:50:08] SuperNet Training INFO: iter: 12240/144360  CE: 5.2717  
[05/05 08:51:09] SuperNet Training INFO: iter: 12360/144360  CE: 5.2319  
[05/05 08:52:09] SuperNet Training INFO: iter: 12480/144360  CE: 5.6268  
[05/05 08:53:12] SuperNet Training INFO: iter: 12600/144360  CE: 5.2638  
[05/05 08:54:12] SuperNet Training INFO: iter: 12720/144360  CE: 5.7310  
[05/05 08:55:11] SuperNet Training INFO: iter: 12840/144360  CE: 5.3283  
[05/05 08:56:10] SuperNet Training INFO: iter: 12960/144360  CE: 5.1551  
[05/05 08:57:09] SuperNet Training INFO: iter: 13080/144360  CE: 5.1334  
[05/05 08:58:08] SuperNet Training INFO: iter: 13200/144360  CE: 5.4429  
[05/05 08:58:22] SuperNet Training INFO: --> epoch:  11/120  avg CE: 5.4136  lr: 0.11752918409209158  
[05/05 08:59:41] SuperNet Training INFO: iter: 13320/144360  CE: 5.1713  
[05/05 09:00:41] SuperNet Training INFO: iter: 13440/144360  CE: 4.9833  
[05/05 09:01:41] SuperNet Training INFO: iter: 13560/144360  CE: 5.3983  
[05/05 09:02:40] SuperNet Training INFO: iter: 13680/144360  CE: 5.2205  
[05/05 09:03:37] SuperNet Training INFO: iter: 13800/144360  CE: 5.0758  
[05/05 09:04:36] SuperNet Training INFO: iter: 13920/144360  CE: 5.3413  
[05/05 09:05:35] SuperNet Training INFO: iter: 14040/144360  CE: 5.2978  
[05/05 09:06:34] SuperNet Training INFO: iter: 14160/144360  CE: 5.1034  
[05/05 09:07:34] SuperNet Training INFO: iter: 14280/144360  CE: 5.1153  
[05/05 09:08:32] SuperNet Training INFO: iter: 14400/144360  CE: 5.1833  
[05/05 09:08:49] SuperNet Training INFO: --> epoch:  12/120  avg CE: 5.2835  lr: 0.11706339097770935  
[05/05 09:10:08] SuperNet Training INFO: iter: 14520/144360  CE: 4.8364  
[05/05 09:11:10] SuperNet Training INFO: iter: 14640/144360  CE: 5.2012  
[05/05 09:12:11] SuperNet Training INFO: iter: 14760/144360  CE: 5.0901  
[05/05 09:13:11] SuperNet Training INFO: iter: 14880/144360  CE: 5.2145  
[05/05 09:14:10] SuperNet Training INFO: iter: 15000/144360  CE: 5.2830  
[05/05 09:15:10] SuperNet Training INFO: iter: 15120/144360  CE: 5.1156  
[05/05 09:16:10] SuperNet Training INFO: iter: 15240/144360  CE: 5.0614  
[05/05 09:17:09] SuperNet Training INFO: iter: 15360/144360  CE: 5.0767  
[05/05 09:18:10] SuperNet Training INFO: iter: 15480/144360  CE: 4.9669  
[05/05 09:19:09] SuperNet Training INFO: iter: 15600/144360  CE: 5.0423  
[05/05 09:19:28] SuperNet Training INFO: --> epoch:  13/120  avg CE: 5.1635  lr: 0.11655848946553125  
[05/05 09:20:44] SuperNet Training INFO: iter: 15720/144360  CE: 5.1246  
[05/05 09:21:43] SuperNet Training INFO: iter: 15840/144360  CE: 4.9547  
[05/05 09:22:42] SuperNet Training INFO: iter: 15960/144360  CE: 4.8981  
[05/05 09:23:42] SuperNet Training INFO: iter: 16080/144360  CE: 5.1273  
[05/05 09:24:42] SuperNet Training INFO: iter: 16200/144360  CE: 5.0224  
[05/05 09:25:42] SuperNet Training INFO: iter: 16320/144360  CE: 4.9885  
[05/05 09:26:42] SuperNet Training INFO: iter: 16440/144360  CE: 5.0201  
[05/05 09:27:40] SuperNet Training INFO: iter: 16560/144360  CE: 5.0225  
[05/05 09:28:39] SuperNet Training INFO: iter: 16680/144360  CE: 5.0295  
[05/05 09:29:39] SuperNet Training INFO: iter: 16800/144360  CE: 5.0717  
[05/05 09:29:59] SuperNet Training INFO: --> epoch:  14/120  avg CE: 5.0507  lr: 0.11601482558983225  
[05/05 09:31:12] SuperNet Training INFO: iter: 16920/144360  CE: 5.0021  
[05/05 09:32:12] SuperNet Training INFO: iter: 17040/144360  CE: 5.1020  
[05/05 09:33:12] SuperNet Training INFO: iter: 17160/144360  CE: 4.7559  
[05/05 09:34:13] SuperNet Training INFO: iter: 17280/144360  CE: 4.6851  
[05/05 09:35:12] SuperNet Training INFO: iter: 17400/144360  CE: 5.2548  
[05/05 09:36:13] SuperNet Training INFO: iter: 17520/144360  CE: 4.9833  
[05/05 09:37:13] SuperNet Training INFO: iter: 17640/144360  CE: 4.9439  
[05/05 09:38:12] SuperNet Training INFO: iter: 17760/144360  CE: 4.7642  
[05/05 09:39:11] SuperNet Training INFO: iter: 17880/144360  CE: 4.8958  
[05/05 09:40:10] SuperNet Training INFO: iter: 18000/144360  CE: 4.9429  
[05/05 09:40:32] SuperNet Training INFO: --> epoch:  15/120  avg CE: 4.9540  lr: 0.11543277195067722  
[05/05 09:41:46] SuperNet Training INFO: iter: 18120/144360  CE: 4.8802  
[05/05 09:42:46] SuperNet Training INFO: iter: 18240/144360  CE: 4.7456  
[05/05 09:43:47] SuperNet Training INFO: iter: 18360/144360  CE: 4.8065  
[05/05 09:44:48] SuperNet Training INFO: iter: 18480/144360  CE: 4.9487  
[05/05 09:45:47] SuperNet Training INFO: iter: 18600/144360  CE: 4.8568  
[05/05 09:46:48] SuperNet Training INFO: iter: 18720/144360  CE: 4.6431  
[05/05 09:47:47] SuperNet Training INFO: iter: 18840/144360  CE: 4.8277  
[05/05 09:48:47] SuperNet Training INFO: iter: 18960/144360  CE: 4.6812  
[05/05 09:49:47] SuperNet Training INFO: iter: 19080/144360  CE: 5.0024  
[05/05 09:50:45] SuperNet Training INFO: iter: 19200/144360  CE: 4.6222  
[05/05 09:51:07] SuperNet Training INFO: --> epoch:  16/120  avg CE: 4.8591  lr: 0.1148127274585561  
[05/05 09:52:17] SuperNet Training INFO: iter: 19320/144360  CE: 4.6808  
[05/05 09:53:17] SuperNet Training INFO: iter: 19440/144360  CE: 4.9340  
[05/05 09:54:16] SuperNet Training INFO: iter: 19560/144360  CE: 4.9584  
[05/05 09:55:16] SuperNet Training INFO: iter: 19680/144360  CE: 4.8747  
[05/05 09:56:16] SuperNet Training INFO: iter: 19800/144360  CE: 4.6128  
[05/05 09:57:15] SuperNet Training INFO: iter: 19920/144360  CE: 4.5655  
[05/05 09:58:15] SuperNet Training INFO: iter: 20040/144360  CE: 4.8694  
[05/05 09:59:13] SuperNet Training INFO: iter: 20160/144360  CE: 4.7137  
[05/05 10:00:12] SuperNet Training INFO: iter: 20280/144360  CE: 4.8622  
[05/05 10:01:12] SuperNet Training INFO: iter: 20400/144360  CE: 4.6260  
[05/05 10:01:36] SuperNet Training INFO: --> epoch:  17/120  avg CE: 4.7800  lr: 0.11415511706099139  
[05/05 10:02:47] SuperNet Training INFO: iter: 20520/144360  CE: 4.9114  
[05/05 10:03:47] SuperNet Training INFO: iter: 20640/144360  CE: 4.4312  
[05/05 10:04:48] SuperNet Training INFO: iter: 20760/144360  CE: 4.7649  
[05/05 10:05:49] SuperNet Training INFO: iter: 20880/144360  CE: 4.8050  
[05/05 10:06:49] SuperNet Training INFO: iter: 21000/144360  CE: 4.6114  
[05/05 10:07:50] SuperNet Training INFO: iter: 21120/144360  CE: 4.9081  
[05/05 10:08:49] SuperNet Training INFO: iter: 21240/144360  CE: 4.9081  
[05/05 10:09:49] SuperNet Training INFO: iter: 21360/144360  CE: 4.6837  
[05/05 10:10:49] SuperNet Training INFO: iter: 21480/144360  CE: 4.8044  
[05/05 10:11:49] SuperNet Training INFO: iter: 21600/144360  CE: 4.8068  
[05/05 10:12:15] SuperNet Training INFO: --> epoch:  18/120  avg CE: 4.6969  lr: 0.11346039145130195  
[05/05 10:13:22] SuperNet Training INFO: iter: 21720/144360  CE: 4.4100  
[05/05 10:14:23] SuperNet Training INFO: iter: 21840/144360  CE: 4.7845  
[05/05 10:15:23] SuperNet Training INFO: iter: 21960/144360  CE: 4.5909  
[05/05 10:16:22] SuperNet Training INFO: iter: 22080/144360  CE: 4.5450  
[05/05 10:17:22] SuperNet Training INFO: iter: 22200/144360  CE: 4.6846  
[05/05 10:18:23] SuperNet Training INFO: iter: 22320/144360  CE: 4.5300  
[05/05 10:19:23] SuperNet Training INFO: iter: 22440/144360  CE: 4.9422  
[05/05 10:20:23] SuperNet Training INFO: iter: 22560/144360  CE: 4.6332  
[05/05 10:21:22] SuperNet Training INFO: iter: 22680/144360  CE: 4.4442  
[05/05 10:22:21] SuperNet Training INFO: iter: 22800/144360  CE: 4.6538  
[05/05 10:22:48] SuperNet Training INFO: --> epoch:  19/120  avg CE: 4.6315  lr: 0.11272902675971772  
[05/05 10:23:55] SuperNet Training INFO: iter: 22920/144360  CE: 4.5683  
[05/05 10:24:56] SuperNet Training INFO: iter: 23040/144360  CE: 4.3525  
[05/05 10:25:57] SuperNet Training INFO: iter: 23160/144360  CE: 4.8141  
[05/05 10:26:57] SuperNet Training INFO: iter: 23280/144360  CE: 4.6679  
[05/05 10:27:58] SuperNet Training INFO: iter: 23400/144360  CE: 4.6493  
[05/05 10:28:59] SuperNet Training INFO: iter: 23520/144360  CE: 4.5696  
[05/05 10:29:59] SuperNet Training INFO: iter: 23640/144360  CE: 4.7121  
[05/05 10:31:00] SuperNet Training INFO: iter: 23760/144360  CE: 4.6650  
[05/05 10:32:00] SuperNet Training INFO: iter: 23880/144360  CE: 4.6081  
[05/05 10:32:59] SuperNet Training INFO: iter: 24000/144360  CE: 4.7471  
[05/05 10:33:28] SuperNet Training INFO: --> epoch:  20/120  avg CE: 4.5549  lr: 0.11196152422706572  
[05/05 10:34:33] SuperNet Training INFO: iter: 24120/144360  CE: 4.5629  
[05/05 10:35:34] SuperNet Training INFO: iter: 24240/144360  CE: 4.9247  
[05/05 10:36:35] SuperNet Training INFO: iter: 24360/144360  CE: 4.6332  
[05/05 10:37:35] SuperNet Training INFO: iter: 24480/144360  CE: 4.0006  
[05/05 10:38:35] SuperNet Training INFO: iter: 24600/144360  CE: 4.5485  
[05/05 10:39:35] SuperNet Training INFO: iter: 24720/144360  CE: 4.1182  
[05/05 10:40:35] SuperNet Training INFO: iter: 24840/144360  CE: 4.4628  
[05/05 10:41:36] SuperNet Training INFO: iter: 24960/144360  CE: 4.6539  
[05/05 10:42:35] SuperNet Training INFO: iter: 25080/144360  CE: 4.3634  
[05/05 10:43:34] SuperNet Training INFO: iter: 25200/144360  CE: 4.3032  
[05/05 10:44:05] SuperNet Training INFO: --> epoch:  21/120  avg CE: 4.4842  lr: 0.11115840986124514  
[05/05 10:45:08] SuperNet Training INFO: iter: 25320/144360  CE: 4.6015  
[05/05 10:46:10] SuperNet Training INFO: iter: 25440/144360  CE: 4.6088  
[05/05 10:47:09] SuperNet Training INFO: iter: 25560/144360  CE: 4.7835  
[05/05 10:48:09] SuperNet Training INFO: iter: 25680/144360  CE: 4.2824  
[05/05 10:49:09] SuperNet Training INFO: iter: 25800/144360  CE: 4.2914  
[05/05 10:50:08] SuperNet Training INFO: iter: 25920/144360  CE: 4.4660  
[05/05 10:51:07] SuperNet Training INFO: iter: 26040/144360  CE: 4.3576  
[05/05 10:52:05] SuperNet Training INFO: iter: 26160/144360  CE: 4.4577  
[05/05 10:53:03] SuperNet Training INFO: iter: 26280/144360  CE: 4.5391  
[05/05 10:54:01] SuperNet Training INFO: iter: 26400/144360  CE: 4.1569  
[05/05 10:54:32] SuperNet Training INFO: --> epoch:  22/120  avg CE: 4.4358  lr: 0.11032023407672516  
[05/05 10:55:34] SuperNet Training INFO: iter: 26520/144360  CE: 4.3623  
[05/05 10:56:34] SuperNet Training INFO: iter: 26640/144360  CE: 4.3072  
[05/05 10:57:34] SuperNet Training INFO: iter: 26760/144360  CE: 4.2534  
[05/05 10:58:34] SuperNet Training INFO: iter: 26880/144360  CE: 4.3654  
[05/05 10:59:35] SuperNet Training INFO: iter: 27000/144360  CE: 4.1678  
[05/05 11:00:35] SuperNet Training INFO: iter: 27120/144360  CE: 4.6080  
[05/05 11:01:36] SuperNet Training INFO: iter: 27240/144360  CE: 4.1874  
[05/05 11:02:36] SuperNet Training INFO: iter: 27360/144360  CE: 4.3331  
[05/05 11:03:38] SuperNet Training INFO: iter: 27480/144360  CE: 4.3845  
[05/05 11:04:38] SuperNet Training INFO: iter: 27600/144360  CE: 4.6521  
[05/05 11:05:12] SuperNet Training INFO: --> epoch:  23/120  avg CE: 4.3811  lr: 0.10944757131732062  
[05/05 11:06:12] SuperNet Training INFO: iter: 27720/144360  CE: 4.3745  
[05/05 11:07:11] SuperNet Training INFO: iter: 27840/144360  CE: 4.3686  
[05/05 11:08:10] SuperNet Training INFO: iter: 27960/144360  CE: 4.4911  
[05/05 11:09:09] SuperNet Training INFO: iter: 28080/144360  CE: 4.1198  
[05/05 11:10:08] SuperNet Training INFO: iter: 28200/144360  CE: 4.1937  
[05/05 11:11:08] SuperNet Training INFO: iter: 28320/144360  CE: 4.2549  
[05/05 11:12:08] SuperNet Training INFO: iter: 28440/144360  CE: 4.3563  
[05/05 11:13:06] SuperNet Training INFO: iter: 28560/144360  CE: 4.1944  
[05/05 11:14:05] SuperNet Training INFO: iter: 28680/144360  CE: 4.3267  
[05/05 11:15:03] SuperNet Training INFO: iter: 28800/144360  CE: 4.0897  
[05/05 11:15:38] SuperNet Training INFO: --> epoch:  24/120  avg CE: 4.3331  lr: 0.10854101966249682  
[05/05 11:16:38] SuperNet Training INFO: iter: 28920/144360  CE: 4.4363  
[05/05 11:17:37] SuperNet Training INFO: iter: 29040/144360  CE: 4.0135  
[05/05 11:18:37] SuperNet Training INFO: iter: 29160/144360  CE: 4.3715  
[05/05 11:19:35] SuperNet Training INFO: iter: 29280/144360  CE: 4.2728  
[05/05 11:20:36] SuperNet Training INFO: iter: 29400/144360  CE: 4.2682  
[05/05 11:21:35] SuperNet Training INFO: iter: 29520/144360  CE: 4.0051  
[05/05 11:22:35] SuperNet Training INFO: iter: 29640/144360  CE: 4.2109  
[05/05 11:23:35] SuperNet Training INFO: iter: 29760/144360  CE: 4.4071  
[05/05 11:24:34] SuperNet Training INFO: iter: 29880/144360  CE: 4.3351  
[05/05 11:25:35] SuperNet Training INFO: iter: 30000/144360  CE: 4.2045  
[05/05 11:26:12] SuperNet Training INFO: --> epoch:  25/120  avg CE: 4.2902  lr: 0.10760120041747454  
[05/05 11:27:09] SuperNet Training INFO: iter: 30120/144360  CE: 4.2806  
[05/05 11:28:09] SuperNet Training INFO: iter: 30240/144360  CE: 4.2430  
[05/05 11:29:10] SuperNet Training INFO: iter: 30360/144360  CE: 4.1342  
[05/05 11:30:10] SuperNet Training INFO: iter: 30480/144360  CE: 4.2041  
[05/05 11:31:10] SuperNet Training INFO: iter: 30600/144360  CE: 4.2271  
[05/05 11:32:10] SuperNet Training INFO: iter: 30720/144360  CE: 4.1179  
[05/05 11:33:10] SuperNet Training INFO: iter: 30840/144360  CE: 4.1573  
[05/05 11:34:09] SuperNet Training INFO: iter: 30960/144360  CE: 4.2362  
[05/05 11:35:09] SuperNet Training INFO: iter: 31080/144360  CE: 4.0293  
[05/05 11:36:09] SuperNet Training INFO: iter: 31200/144360  CE: 4.1475  
[05/05 11:36:46] SuperNet Training INFO: --> epoch:  26/120  avg CE: 4.2405  lr: 0.10662875768741867  
[05/05 11:37:43] SuperNet Training INFO: iter: 31320/144360  CE: 4.0157  
[05/05 11:38:43] SuperNet Training INFO: iter: 31440/144360  CE: 4.4633  
[05/05 11:39:42] SuperNet Training INFO: iter: 31560/144360  CE: 4.2247  
[05/05 11:40:40] SuperNet Training INFO: iter: 31680/144360  CE: 4.1559  
[05/05 11:41:39] SuperNet Training INFO: iter: 31800/144360  CE: 4.0792  
[05/05 11:42:37] SuperNet Training INFO: iter: 31920/144360  CE: 4.2299  
[05/05 11:43:37] SuperNet Training INFO: iter: 32040/144360  CE: 4.2196  
[05/05 11:44:36] SuperNet Training INFO: iter: 32160/144360  CE: 4.1543  
[05/05 11:45:35] SuperNet Training INFO: iter: 32280/144360  CE: 4.3394  
[05/05 11:46:35] SuperNet Training INFO: iter: 32400/144360  CE: 4.2879  
[05/05 11:47:13] SuperNet Training INFO: --> epoch:  27/120  avg CE: 4.2042  lr: 0.10562435793600224  
[05/05 11:48:07] SuperNet Training INFO: iter: 32520/144360  CE: 4.2864  
[05/05 11:49:06] SuperNet Training INFO: iter: 32640/144360  CE: 4.3083  
[05/05 11:50:05] SuperNet Training INFO: iter: 32760/144360  CE: 4.0919  
[05/05 11:51:04] SuperNet Training INFO: iter: 32880/144360  CE: 4.0792  
[05/05 11:52:03] SuperNet Training INFO: iter: 33000/144360  CE: 3.9665  
[05/05 11:53:02] SuperNet Training INFO: iter: 33120/144360  CE: 4.0252  
[05/05 11:54:03] SuperNet Training INFO: iter: 33240/144360  CE: 4.3914  
[05/05 11:55:03] SuperNet Training INFO: iter: 33360/144360  CE: 4.4040  
[05/05 11:56:02] SuperNet Training INFO: iter: 33480/144360  CE: 3.9462  
[05/05 11:57:02] SuperNet Training INFO: iter: 33600/144360  CE: 3.7342  
[05/05 11:57:44] SuperNet Training INFO: --> epoch:  28/120  avg CE: 4.1596  lr: 0.10458868952864393  
[05/05 11:58:37] SuperNet Training INFO: iter: 33720/144360  CE: 3.9016  
[05/05 11:59:39] SuperNet Training INFO: iter: 33840/144360  CE: 4.1239  
[05/05 12:00:39] SuperNet Training INFO: iter: 33960/144360  CE: 3.8053  
[05/05 12:01:38] SuperNet Training INFO: iter: 34080/144360  CE: 4.5346  
[05/05 12:02:40] SuperNet Training INFO: iter: 34200/144360  CE: 4.1000  
[05/05 12:03:40] SuperNet Training INFO: iter: 34320/144360  CE: 4.0754  
[05/05 12:04:40] SuperNet Training INFO: iter: 34440/144360  CE: 4.0219  
[05/05 12:05:40] SuperNet Training INFO: iter: 34560/144360  CE: 4.2573  
[05/05 12:06:40] SuperNet Training INFO: iter: 34680/144360  CE: 3.9598  
[05/05 12:07:39] SuperNet Training INFO: iter: 34800/144360  CE: 3.8203  
[05/05 12:08:21] SuperNet Training INFO: --> epoch:  29/120  avg CE: 4.1211  lr: 0.10352246226073762  
[05/05 12:09:13] SuperNet Training INFO: iter: 34920/144360  CE: 4.0689  
[05/05 12:10:14] SuperNet Training INFO: iter: 35040/144360  CE: 3.9163  
[05/05 12:11:14] SuperNet Training INFO: iter: 35160/144360  CE: 4.4530  
[05/05 12:12:13] SuperNet Training INFO: iter: 35280/144360  CE: 4.0276  
[05/05 12:13:11] SuperNet Training INFO: iter: 35400/144360  CE: 4.0171  
[05/05 12:14:11] SuperNet Training INFO: iter: 35520/144360  CE: 4.3526  
[05/05 12:15:09] SuperNet Training INFO: iter: 35640/144360  CE: 4.4943  
[05/05 12:16:10] SuperNet Training INFO: iter: 35760/144360  CE: 3.9310  
[05/05 12:17:09] SuperNet Training INFO: iter: 35880/144360  CE: 4.1290  
[05/05 12:18:09] SuperNet Training INFO: iter: 36000/144360  CE: 4.0259  
[05/05 12:18:53] SuperNet Training INFO: --> epoch:  30/120  avg CE: 4.0899  lr: 0.10242640687119343  
[05/05 12:19:44] SuperNet Training INFO: iter: 36120/144360  CE: 4.0346  
[05/05 12:20:45] SuperNet Training INFO: iter: 36240/144360  CE: 4.2626  
[05/05 12:21:46] SuperNet Training INFO: iter: 36360/144360  CE: 4.0005  
[05/05 12:22:45] SuperNet Training INFO: iter: 36480/144360  CE: 4.2816  
[05/05 12:23:44] SuperNet Training INFO: iter: 36600/144360  CE: 4.1433  
[05/05 12:24:41] SuperNet Training INFO: iter: 36720/144360  CE: 3.9173  
[05/05 12:25:38] SuperNet Training INFO: iter: 36840/144360  CE: 3.8966  
[05/05 12:26:37] SuperNet Training INFO: iter: 36960/144360  CE: 4.1165  
[05/05 12:27:35] SuperNet Training INFO: iter: 37080/144360  CE: 4.0612  
[05/05 12:28:33] SuperNet Training INFO: iter: 37200/144360  CE: 3.7861  
[05/05 12:29:17] SuperNet Training INFO: --> epoch:  31/120  avg CE: 4.0561  lr: 0.10130127454162571  
[05/05 12:30:06] SuperNet Training INFO: iter: 37320/144360  CE: 4.2113  
[05/05 12:31:04] SuperNet Training INFO: iter: 37440/144360  CE: 3.9388  
[05/05 12:32:03] SuperNet Training INFO: iter: 37560/144360  CE: 3.8947  
[05/05 12:33:03] SuperNet Training INFO: iter: 37680/144360  CE: 4.0217  
[05/05 12:34:03] SuperNet Training INFO: iter: 37800/144360  CE: 4.0142  
[05/05 12:35:02] SuperNet Training INFO: iter: 37920/144360  CE: 4.0015  
[05/05 12:36:01] SuperNet Training INFO: iter: 38040/144360  CE: 4.1536  
[05/05 12:37:02] SuperNet Training INFO: iter: 38160/144360  CE: 3.9223  
[05/05 12:38:02] SuperNet Training INFO: iter: 38280/144360  CE: 4.1009  
[05/05 12:39:03] SuperNet Training INFO: iter: 38400/144360  CE: 3.8029  
[05/05 12:39:50] SuperNet Training INFO: --> epoch:  32/120  avg CE: 4.0331  lr: 0.10014783638153192  
[05/05 12:40:37] SuperNet Training INFO: iter: 38520/144360  CE: 3.8974  
[05/05 12:41:36] SuperNet Training INFO: iter: 38640/144360  CE: 4.2967  
[05/05 12:42:37] SuperNet Training INFO: iter: 38760/144360  CE: 3.8645  
[05/05 12:43:38] SuperNet Training INFO: iter: 38880/144360  CE: 4.3085  
[05/05 12:44:38] SuperNet Training INFO: iter: 39000/144360  CE: 3.8180  
[05/05 12:45:39] SuperNet Training INFO: iter: 39120/144360  CE: 3.8400  
[05/05 12:46:38] SuperNet Training INFO: iter: 39240/144360  CE: 3.7999  
[05/05 12:47:38] SuperNet Training INFO: iter: 39360/144360  CE: 4.0669  
[05/05 12:48:40] SuperNet Training INFO: iter: 39480/144360  CE: 3.7489  
[05/05 12:49:40] SuperNet Training INFO: iter: 39600/144360  CE: 4.0657  
[05/05 12:50:29] SuperNet Training INFO: --> epoch:  33/120  avg CE: 3.9988  lr: 0.09896688289981138  
[05/05 12:51:15] SuperNet Training INFO: iter: 39720/144360  CE: 3.9227  
[05/05 12:52:16] SuperNet Training INFO: iter: 39840/144360  CE: 3.9816  
[05/05 12:53:15] SuperNet Training INFO: iter: 39960/144360  CE: 4.1749  
[05/05 12:54:15] SuperNet Training INFO: iter: 40080/144360  CE: 3.7653  
[05/05 12:55:16] SuperNet Training INFO: iter: 40200/144360  CE: 4.3096  
[05/05 12:56:19] SuperNet Training INFO: iter: 40320/144360  CE: 3.8849  
[05/05 12:57:19] SuperNet Training INFO: iter: 40440/144360  CE: 3.9707  
[05/05 12:58:20] SuperNet Training INFO: iter: 40560/144360  CE: 3.8398  
[05/05 12:59:20] SuperNet Training INFO: iter: 40680/144360  CE: 3.9640  
[05/05 13:00:20] SuperNet Training INFO: iter: 40800/144360  CE: 4.1198  
[05/05 13:01:09] SuperNet Training INFO: --> epoch:  34/120  avg CE: 3.9586  lr: 0.09775922346299062  
[05/05 13:01:54] SuperNet Training INFO: iter: 40920/144360  CE: 4.1753  
[05/05 13:02:54] SuperNet Training INFO: iter: 41040/144360  CE: 3.6406  
[05/05 13:03:55] SuperNet Training INFO: iter: 41160/144360  CE: 3.9148  
[05/05 13:04:55] SuperNet Training INFO: iter: 41280/144360  CE: 3.8864  
[05/05 13:05:55] SuperNet Training INFO: iter: 41400/144360  CE: 4.2318  
[05/05 13:06:54] SuperNet Training INFO: iter: 41520/144360  CE: 3.7460  
[05/05 13:07:52] SuperNet Training INFO: iter: 41640/144360  CE: 3.6907  
[05/05 13:08:51] SuperNet Training INFO: iter: 41760/144360  CE: 3.8665  
[05/05 13:09:48] SuperNet Training INFO: iter: 41880/144360  CE: 4.1899  
[05/05 13:10:46] SuperNet Training INFO: iter: 42000/144360  CE: 4.1165  
[05/05 13:11:35] SuperNet Training INFO: --> epoch:  35/120  avg CE: 3.9309  lr: 0.09652568574052359  
[05/05 13:12:18] SuperNet Training INFO: iter: 42120/144360  CE: 3.8892  
[05/05 13:13:18] SuperNet Training INFO: iter: 42240/144360  CE: 3.9830  
[05/05 13:14:18] SuperNet Training INFO: iter: 42360/144360  CE: 4.0089  
[05/05 13:15:17] SuperNet Training INFO: iter: 42480/144360  CE: 4.1377  
[05/05 13:16:17] SuperNet Training INFO: iter: 42600/144360  CE: 3.9267  
[05/05 13:17:18] SuperNet Training INFO: iter: 42720/144360  CE: 3.9652  
[05/05 13:18:17] SuperNet Training INFO: iter: 42840/144360  CE: 3.7118  
[05/05 13:19:17] SuperNet Training INFO: iter: 42960/144360  CE: 3.9401  
[05/05 13:20:15] SuperNet Training INFO: iter: 43080/144360  CE: 3.8844  
[05/05 13:21:14] SuperNet Training INFO: iter: 43200/144360  CE: 3.8049  
[05/05 13:22:06] SuperNet Training INFO: --> epoch:  36/120  avg CE: 3.9103  lr: 0.09526711513754865  
[05/05 13:22:48] SuperNet Training INFO: iter: 43320/144360  CE: 3.6218  
[05/05 13:23:48] SuperNet Training INFO: iter: 43440/144360  CE: 4.0307  
[05/05 13:24:48] SuperNet Training INFO: iter: 43560/144360  CE: 3.8807  
[05/05 13:25:46] SuperNet Training INFO: iter: 43680/144360  CE: 3.9856  
[05/05 13:26:46] SuperNet Training INFO: iter: 43800/144360  CE: 3.6658  
[05/05 13:27:45] SuperNet Training INFO: iter: 43920/144360  CE: 4.1967  
[05/05 13:28:45] SuperNet Training INFO: iter: 44040/144360  CE: 3.8702  
[05/05 13:29:44] SuperNet Training INFO: iter: 44160/144360  CE: 3.6215  
[05/05 13:30:44] SuperNet Training INFO: iter: 44280/144360  CE: 3.7457  
[05/05 13:31:45] SuperNet Training INFO: iter: 44400/144360  CE: 4.0388  
[05/05 13:32:39] SuperNet Training INFO: --> epoch:  37/120  avg CE: 3.8694  lr: 0.0939843742154901  
[05/05 13:33:18] SuperNet Training INFO: iter: 44520/144360  CE: 3.8215  
[05/05 13:34:18] SuperNet Training INFO: iter: 44640/144360  CE: 3.8249  
[05/05 13:35:17] SuperNet Training INFO: iter: 44760/144360  CE: 3.9841  
[05/05 13:36:15] SuperNet Training INFO: iter: 44880/144360  CE: 3.7044  
[05/05 13:37:14] SuperNet Training INFO: iter: 45000/144360  CE: 3.9411  
[05/05 13:38:13] SuperNet Training INFO: iter: 45120/144360  CE: 3.8463  
[05/05 13:39:12] SuperNet Training INFO: iter: 45240/144360  CE: 4.0455  
[05/05 13:40:11] SuperNet Training INFO: iter: 45360/144360  CE: 3.8975  
[05/05 13:41:10] SuperNet Training INFO: iter: 45480/144360  CE: 4.1210  
[05/05 13:42:10] SuperNet Training INFO: iter: 45600/144360  CE: 3.8097  
[05/05 13:43:07] SuperNet Training INFO: --> epoch:  38/120  avg CE: 3.8522  lr: 0.0926783421009017  
[05/05 13:43:45] SuperNet Training INFO: iter: 45720/144360  CE: 3.7958  
[05/05 13:44:45] SuperNet Training INFO: iter: 45840/144360  CE: 3.8441  
[05/05 13:45:46] SuperNet Training INFO: iter: 45960/144360  CE: 3.8112  
[05/05 13:46:46] SuperNet Training INFO: iter: 46080/144360  CE: 3.7463  
[05/05 13:47:46] SuperNet Training INFO: iter: 46200/144360  CE: 3.9847  
[05/05 13:48:46] SuperNet Training INFO: iter: 46320/144360  CE: 3.5212  
[05/05 13:49:45] SuperNet Training INFO: iter: 46440/144360  CE: 3.3603  
[05/05 13:50:44] SuperNet Training INFO: iter: 46560/144360  CE: 4.0661  
[05/05 13:51:44] SuperNet Training INFO: iter: 46680/144360  CE: 4.0762  
[05/05 13:52:44] SuperNet Training INFO: iter: 46800/144360  CE: 3.7391  
[05/05 13:53:42] SuperNet Training INFO: --> epoch:  39/120  avg CE: 3.8301  lr: 0.09134991388295689  
[05/05 13:54:19] SuperNet Training INFO: iter: 46920/144360  CE: 3.8043  
[05/05 13:55:18] SuperNet Training INFO: iter: 47040/144360  CE: 3.7101  
[05/05 13:56:17] SuperNet Training INFO: iter: 47160/144360  CE: 4.0439  
[05/05 13:57:16] SuperNet Training INFO: iter: 47280/144360  CE: 3.7638  
[05/05 13:58:16] SuperNet Training INFO: iter: 47400/144360  CE: 4.1175  
[05/05 13:59:15] SuperNet Training INFO: iter: 47520/144360  CE: 3.7162  
[05/05 14:00:14] SuperNet Training INFO: iter: 47640/144360  CE: 3.9121  
[05/05 14:01:13] SuperNet Training INFO: iter: 47760/144360  CE: 3.3989  
[05/05 14:02:13] SuperNet Training INFO: iter: 47880/144360  CE: 3.7468  
[05/05 14:03:13] SuperNet Training INFO: iter: 48000/144360  CE: 3.7939  
[05/05 14:04:10] SuperNet Training INFO: iter: 48120/144360  CE: 4.1505  
[05/05 14:04:10] SuperNet Training INFO: --> epoch:  40/120  avg CE: 3.7992  lr: 0.0899999999999998  
[05/05 14:05:46] SuperNet Training INFO: iter: 48240/144360  CE: 3.4412  
[05/05 14:06:45] SuperNet Training INFO: iter: 48360/144360  CE: 3.7067  
[05/05 14:07:46] SuperNet Training INFO: iter: 48480/144360  CE: 3.6882  
[05/05 14:08:46] SuperNet Training INFO: iter: 48600/144360  CE: 3.4743  
[05/05 14:09:45] SuperNet Training INFO: iter: 48720/144360  CE: 3.9177  
[05/05 14:10:44] SuperNet Training INFO: iter: 48840/144360  CE: 3.8715  
[05/05 14:11:44] SuperNet Training INFO: iter: 48960/144360  CE: 3.5076  
[05/05 14:12:44] SuperNet Training INFO: iter: 49080/144360  CE: 3.6379  
[05/05 14:13:43] SuperNet Training INFO: iter: 49200/144360  CE: 3.6227  
[05/05 14:14:42] SuperNet Training INFO: iter: 49320/144360  CE: 3.5934  
[05/05 14:14:43] SuperNet Training INFO: --> epoch:  41/120  avg CE: 3.7753  lr: 0.08862952561557644  
[05/05 14:16:17] SuperNet Training INFO: iter: 49440/144360  CE: 3.6096  
[05/05 14:17:17] SuperNet Training INFO: iter: 49560/144360  CE: 3.4104  
[05/05 14:18:18] SuperNet Training INFO: iter: 49680/144360  CE: 3.6833  
[05/05 14:19:19] SuperNet Training INFO: iter: 49800/144360  CE: 3.6625  
[05/05 14:20:20] SuperNet Training INFO: iter: 49920/144360  CE: 3.9071  
[05/05 14:21:20] SuperNet Training INFO: iter: 50040/144360  CE: 3.7801  
[05/05 14:22:21] SuperNet Training INFO: iter: 50160/144360  CE: 3.7628  
[05/05 14:23:20] SuperNet Training INFO: iter: 50280/144360  CE: 3.6142  
[05/05 14:24:20] SuperNet Training INFO: iter: 50400/144360  CE: 3.8369  
[05/05 14:25:19] SuperNet Training INFO: iter: 50520/144360  CE: 3.9194  
[05/05 14:25:21] SuperNet Training INFO: --> epoch:  42/120  avg CE: 3.7500  lr: 0.08723942998437267  
[05/05 14:26:55] SuperNet Training INFO: iter: 50640/144360  CE: 3.7275  
[05/05 14:27:56] SuperNet Training INFO: iter: 50760/144360  CE: 3.7798  
[05/05 14:28:57] SuperNet Training INFO: iter: 50880/144360  CE: 3.7863  
[05/05 14:29:58] SuperNet Training INFO: iter: 51000/144360  CE: 3.9536  
[05/05 14:30:57] SuperNet Training INFO: iter: 51120/144360  CE: 3.7250  
[05/05 14:31:56] SuperNet Training INFO: iter: 51240/144360  CE: 3.6768  
[05/05 14:32:57] SuperNet Training INFO: iter: 51360/144360  CE: 3.4814  
[05/05 14:33:57] SuperNet Training INFO: iter: 51480/144360  CE: 3.6331  
[05/05 14:34:55] SuperNet Training INFO: iter: 51600/144360  CE: 3.2550  
[05/05 14:35:55] SuperNet Training INFO: iter: 51720/144360  CE: 3.4204  
[05/05 14:35:59] SuperNet Training INFO: --> epoch:  43/120  avg CE: 3.7437  lr: 0.08583066580849745  
[05/05 14:37:30] SuperNet Training INFO: iter: 51840/144360  CE: 4.1016  
[05/05 14:38:32] SuperNet Training INFO: iter: 51960/144360  CE: 3.4971  
[05/05 14:39:33] SuperNet Training INFO: iter: 52080/144360  CE: 3.6033  
[05/05 14:40:34] SuperNet Training INFO: iter: 52200/144360  CE: 3.5578  
[05/05 14:41:34] SuperNet Training INFO: iter: 52320/144360  CE: 3.7356  
[05/05 14:42:33] SuperNet Training INFO: iter: 52440/144360  CE: 4.0546  
[05/05 14:43:32] SuperNet Training INFO: iter: 52560/144360  CE: 3.6064  
[05/05 14:44:32] SuperNet Training INFO: iter: 52680/144360  CE: 3.4690  
[05/05 14:45:31] SuperNet Training INFO: iter: 52800/144360  CE: 3.6555  
[05/05 14:46:29] SuperNet Training INFO: iter: 52920/144360  CE: 3.8015  
[05/05 14:46:34] SuperNet Training INFO: --> epoch:  44/120  avg CE: 3.7120  lr: 0.08440419858454766  
[05/05 14:48:03] SuperNet Training INFO: iter: 53040/144360  CE: 3.7090  
[05/05 14:49:02] SuperNet Training INFO: iter: 53160/144360  CE: 3.8641  
[05/05 14:50:02] SuperNet Training INFO: iter: 53280/144360  CE: 3.8531  
[05/05 14:51:01] SuperNet Training INFO: iter: 53400/144360  CE: 3.7099  
[05/05 14:52:00] SuperNet Training INFO: iter: 53520/144360  CE: 3.3205  
[05/05 14:52:58] SuperNet Training INFO: iter: 53640/144360  CE: 3.8392  
[05/05 14:53:57] SuperNet Training INFO: iter: 53760/144360  CE: 3.7007  
[05/05 14:54:56] SuperNet Training INFO: iter: 53880/144360  CE: 3.6676  
[05/05 14:55:55] SuperNet Training INFO: iter: 54000/144360  CE: 3.4439  
[05/05 14:56:54] SuperNet Training INFO: iter: 54120/144360  CE: 3.8365  
[05/05 14:57:00] SuperNet Training INFO: --> epoch:  45/120  avg CE: 3.7010  lr: 0.0829610059419049  
[05/05 14:58:28] SuperNet Training INFO: iter: 54240/144360  CE: 3.6185  
[05/05 14:59:29] SuperNet Training INFO: iter: 54360/144360  CE: 3.4990  
[05/05 15:00:31] SuperNet Training INFO: iter: 54480/144360  CE: 3.5441  
[05/05 15:01:32] SuperNet Training INFO: iter: 54600/144360  CE: 3.9187  
[05/05 15:02:33] SuperNet Training INFO: iter: 54720/144360  CE: 3.2345  
[05/05 15:03:34] SuperNet Training INFO: iter: 54840/144360  CE: 3.7632  
[05/05 15:04:34] SuperNet Training INFO: iter: 54960/144360  CE: 3.6056  
[05/05 15:05:34] SuperNet Training INFO: iter: 55080/144360  CE: 3.6926  
[05/05 15:06:33] SuperNet Training INFO: iter: 55200/144360  CE: 3.6298  
[05/05 15:07:33] SuperNet Training INFO: iter: 55320/144360  CE: 3.5367  
[05/05 15:07:41] SuperNet Training INFO: --> epoch:  46/120  avg CE: 3.6848  lr: 0.08150207697271764  
[05/05 15:09:07] SuperNet Training INFO: iter: 55440/144360  CE: 3.6733  
[05/05 15:10:07] SuperNet Training INFO: iter: 55560/144360  CE: 3.3457  
[05/05 15:11:07] SuperNet Training INFO: iter: 55680/144360  CE: 3.4164  
[05/05 15:12:07] SuperNet Training INFO: iter: 55800/144360  CE: 3.6527  
[05/05 15:13:07] SuperNet Training INFO: iter: 55920/144360  CE: 3.6082  
[05/05 15:14:07] SuperNet Training INFO: iter: 56040/144360  CE: 3.7810  
[05/05 15:15:07] SuperNet Training INFO: iter: 56160/144360  CE: 3.3477  
[05/05 15:16:06] SuperNet Training INFO: iter: 56280/144360  CE: 3.1435  
[05/05 15:17:06] SuperNet Training INFO: iter: 56400/144360  CE: 3.3303  
[05/05 15:18:07] SuperNet Training INFO: iter: 56520/144360  CE: 3.6957  
[05/05 15:18:16] SuperNet Training INFO: --> epoch:  47/120  avg CE: 3.6498  lr: 0.08002841155402596  
[05/05 15:19:42] SuperNet Training INFO: iter: 56640/144360  CE: 3.3432  
[05/05 15:20:43] SuperNet Training INFO: iter: 56760/144360  CE: 3.6964  
[05/05 15:21:41] SuperNet Training INFO: iter: 56880/144360  CE: 3.8227  
[05/05 15:22:40] SuperNet Training INFO: iter: 57000/144360  CE: 3.6316  
[05/05 15:23:39] SuperNet Training INFO: iter: 57120/144360  CE: 3.7683  
[05/05 15:24:37] SuperNet Training INFO: iter: 57240/144360  CE: 3.6322  
[05/05 15:25:36] SuperNet Training INFO: iter: 57360/144360  CE: 3.7368  
[05/05 15:26:34] SuperNet Training INFO: iter: 57480/144360  CE: 3.5768  
[05/05 15:27:33] SuperNet Training INFO: iter: 57600/144360  CE: 3.7256  
[05/05 15:28:32] SuperNet Training INFO: iter: 57720/144360  CE: 3.7552  
[05/05 15:28:43] SuperNet Training INFO: --> epoch:  48/120  avg CE: 3.6437  lr: 0.07854101966249659  
[05/05 15:30:05] SuperNet Training INFO: iter: 57840/144360  CE: 3.2917  
[05/05 15:31:03] SuperNet Training INFO: iter: 57960/144360  CE: 3.6134  
[05/05 15:32:03] SuperNet Training INFO: iter: 58080/144360  CE: 3.4847  
[05/05 15:33:02] SuperNet Training INFO: iter: 58200/144360  CE: 3.5551  
[05/05 15:34:02] SuperNet Training INFO: iter: 58320/144360  CE: 3.5966  
[05/05 15:35:01] SuperNet Training INFO: iter: 58440/144360  CE: 3.6037  
[05/05 15:36:01] SuperNet Training INFO: iter: 58560/144360  CE: 3.8315  
[05/05 15:36:59] SuperNet Training INFO: iter: 58680/144360  CE: 3.6672  
[05/05 15:37:59] SuperNet Training INFO: iter: 58800/144360  CE: 3.1460  
[05/05 15:38:57] SuperNet Training INFO: iter: 58920/144360  CE: 3.6820  
[05/05 15:39:09] SuperNet Training INFO: --> epoch:  49/120  avg CE: 3.6220  lr: 0.07704092068223518  
[05/05 15:40:30] SuperNet Training INFO: iter: 59040/144360  CE: 3.5333  
[05/05 15:41:28] SuperNet Training INFO: iter: 59160/144360  CE: 3.4696  
[05/05 15:42:27] SuperNet Training INFO: iter: 59280/144360  CE: 3.5424  
[05/05 15:43:25] SuperNet Training INFO: iter: 59400/144360  CE: 3.5816  
[05/05 15:44:23] SuperNet Training INFO: iter: 59520/144360  CE: 3.6404  
[05/05 15:45:22] SuperNet Training INFO: iter: 59640/144360  CE: 3.5855  
[05/05 15:46:20] SuperNet Training INFO: iter: 59760/144360  CE: 3.5248  
[05/05 15:47:18] SuperNet Training INFO: iter: 59880/144360  CE: 3.5652  
[05/05 15:48:18] SuperNet Training INFO: iter: 60000/144360  CE: 3.6507  
[05/05 15:49:18] SuperNet Training INFO: iter: 60120/144360  CE: 3.4757  
[05/05 15:49:31] SuperNet Training INFO: --> epoch:  50/120  avg CE: 3.6026  lr: 0.07552914270615126  
[05/05 15:50:52] SuperNet Training INFO: iter: 60240/144360  CE: 3.6448  
[05/05 15:51:52] SuperNet Training INFO: iter: 60360/144360  CE: 3.4987  
[05/05 15:52:52] SuperNet Training INFO: iter: 60480/144360  CE: 3.6178  
[05/05 15:53:52] SuperNet Training INFO: iter: 60600/144360  CE: 3.1500  
[05/05 15:54:51] SuperNet Training INFO: iter: 60720/144360  CE: 3.7600  
[05/05 15:55:51] SuperNet Training INFO: iter: 60840/144360  CE: 3.5462  
[05/05 15:56:52] SuperNet Training INFO: iter: 60960/144360  CE: 3.7269  
[05/05 15:57:52] SuperNet Training INFO: iter: 61080/144360  CE: 3.4363  
[05/05 15:58:51] SuperNet Training INFO: iter: 61200/144360  CE: 3.6848  
[05/05 15:59:50] SuperNet Training INFO: iter: 61320/144360  CE: 3.6306  
[05/05 16:00:05] SuperNet Training INFO: --> epoch:  51/120  avg CE: 3.5934  lr: 0.0740067218313545  
[05/05 16:01:24] SuperNet Training INFO: iter: 61440/144360  CE: 3.7370  
[05/05 16:02:25] SuperNet Training INFO: iter: 61560/144360  CE: 3.4649  
[05/05 16:03:26] SuperNet Training INFO: iter: 61680/144360  CE: 3.5914  
[05/05 16:04:27] SuperNet Training INFO: iter: 61800/144360  CE: 3.7454  
[05/05 16:05:26] SuperNet Training INFO: iter: 61920/144360  CE: 3.9695  
[05/05 16:06:25] SuperNet Training INFO: iter: 62040/144360  CE: 3.6798  
[05/05 16:07:25] SuperNet Training INFO: iter: 62160/144360  CE: 3.6846  
[05/05 16:08:24] SuperNet Training INFO: iter: 62280/144360  CE: 3.3203  
[05/05 16:09:23] SuperNet Training INFO: iter: 62400/144360  CE: 3.7355  
[05/05 16:10:21] SuperNet Training INFO: iter: 62520/144360  CE: 3.6662  
[05/05 16:10:38] SuperNet Training INFO: --> epoch:  52/120  avg CE: 3.5695  lr: 0.07247470144906537  
[05/05 16:11:56] SuperNet Training INFO: iter: 62640/144360  CE: 3.8217  
[05/05 16:12:55] SuperNet Training INFO: iter: 62760/144360  CE: 3.6223  
[05/05 16:13:55] SuperNet Training INFO: iter: 62880/144360  CE: 3.5059  
[05/05 16:14:55] SuperNet Training INFO: iter: 63000/144360  CE: 3.6586  
[05/05 16:15:54] SuperNet Training INFO: iter: 63120/144360  CE: 3.3466  
[05/05 16:16:54] SuperNet Training INFO: iter: 63240/144360  CE: 3.5865  
[05/05 16:17:53] SuperNet Training INFO: iter: 63360/144360  CE: 3.3321  
[05/05 16:18:52] SuperNet Training INFO: iter: 63480/144360  CE: 3.2501  
[05/05 16:19:53] SuperNet Training INFO: iter: 63600/144360  CE: 3.5461  
[05/05 16:20:52] SuperNet Training INFO: iter: 63720/144360  CE: 3.3655  
[05/05 16:21:10] SuperNet Training INFO: --> epoch:  53/120  avg CE: 3.5484  lr: 0.07093413152952865  
[05/05 16:22:26] SuperNet Training INFO: iter: 63840/144360  CE: 3.5179  
[05/05 16:23:27] SuperNet Training INFO: iter: 63960/144360  CE: 3.6255  
[05/05 16:24:27] SuperNet Training INFO: iter: 64080/144360  CE: 3.2910  
[05/05 16:25:26] SuperNet Training INFO: iter: 64200/144360  CE: 3.8008  
[05/05 16:26:25] SuperNet Training INFO: iter: 64320/144360  CE: 3.4826  
[05/05 16:27:23] SuperNet Training INFO: iter: 64440/144360  CE: 3.5862  
[05/05 16:28:21] SuperNet Training INFO: iter: 64560/144360  CE: 3.4206  
[05/05 16:29:20] SuperNet Training INFO: iter: 64680/144360  CE: 3.4226  
[05/05 16:30:19] SuperNet Training INFO: iter: 64800/144360  CE: 3.4377  
[05/05 16:31:17] SuperNet Training INFO: iter: 64920/144360  CE: 3.5035  
[05/05 16:31:37] SuperNet Training INFO: --> epoch:  54/120  avg CE: 3.5426  lr: 0.06938606790241343  
[05/05 16:32:52] SuperNet Training INFO: iter: 65040/144360  CE: 3.8631  
[05/05 16:33:53] SuperNet Training INFO: iter: 65160/144360  CE: 3.4316  
[05/05 16:34:53] SuperNet Training INFO: iter: 65280/144360  CE: 3.6831  
[05/05 16:35:53] SuperNet Training INFO: iter: 65400/144360  CE: 3.3861  
[05/05 16:36:52] SuperNet Training INFO: iter: 65520/144360  CE: 3.5591  
[05/05 16:37:51] SuperNet Training INFO: iter: 65640/144360  CE: 3.5494  
[05/05 16:38:51] SuperNet Training INFO: iter: 65760/144360  CE: 3.4269  
[05/05 16:39:51] SuperNet Training INFO: iter: 65880/144360  CE: 3.7778  
[05/05 16:40:52] SuperNet Training INFO: iter: 66000/144360  CE: 3.2590  
[05/05 16:41:50] SuperNet Training INFO: iter: 66120/144360  CE: 3.5522  
[05/05 16:42:12] SuperNet Training INFO: --> epoch:  55/120  avg CE: 3.5198  lr: 0.06783157153320266  
[05/05 16:43:24] SuperNet Training INFO: iter: 66240/144360  CE: 3.4208  
[05/05 16:44:25] SuperNet Training INFO: iter: 66360/144360  CE: 3.2630  
[05/05 16:45:26] SuperNet Training INFO: iter: 66480/144360  CE: 3.5588  
[05/05 16:46:26] SuperNet Training INFO: iter: 66600/144360  CE: 3.5689  
[05/05 16:47:26] SuperNet Training INFO: iter: 66720/144360  CE: 3.3613  
[05/05 16:48:26] SuperNet Training INFO: iter: 66840/144360  CE: 3.0831  
[05/05 16:49:24] SuperNet Training INFO: iter: 66960/144360  CE: 3.4565  
[05/05 16:50:23] SuperNet Training INFO: iter: 67080/144360  CE: 3.6396  
[05/05 16:51:22] SuperNet Training INFO: iter: 67200/144360  CE: 3.5305  
[05/05 16:52:20] SuperNet Training INFO: iter: 67320/144360  CE: 3.6345  
[05/05 16:52:43] SuperNet Training INFO: --> epoch:  56/120  avg CE: 3.5095  lr: 0.06627170779605904  
[05/05 16:53:54] SuperNet Training INFO: iter: 67440/144360  CE: 3.4120  
[05/05 16:54:53] SuperNet Training INFO: iter: 67560/144360  CE: 3.4295  
[05/05 16:55:51] SuperNet Training INFO: iter: 67680/144360  CE: 3.4889  
[05/05 16:56:50] SuperNet Training INFO: iter: 67800/144360  CE: 3.6645  
[05/05 16:57:50] SuperNet Training INFO: iter: 67920/144360  CE: 3.3145  
[05/05 16:58:50] SuperNet Training INFO: iter: 68040/144360  CE: 3.7972  
[05/05 16:59:49] SuperNet Training INFO: iter: 68160/144360  CE: 3.2191  
[05/05 17:00:49] SuperNet Training INFO: iter: 68280/144360  CE: 3.1467  
[05/05 17:01:48] SuperNet Training INFO: iter: 68400/144360  CE: 3.3999  
[05/05 17:02:48] SuperNet Training INFO: iter: 68520/144360  CE: 3.5394  
[05/05 17:03:12] SuperNet Training INFO: --> epoch:  57/120  avg CE: 3.4972  lr: 0.06470754574367053  
[05/05 17:04:23] SuperNet Training INFO: iter: 68640/144360  CE: 3.5523  
[05/05 17:05:23] SuperNet Training INFO: iter: 68760/144360  CE: 3.4757  
[05/05 17:06:24] SuperNet Training INFO: iter: 68880/144360  CE: 3.4864  
[05/05 17:07:24] SuperNet Training INFO: iter: 69000/144360  CE: 3.6655  
[05/05 17:08:24] SuperNet Training INFO: iter: 69120/144360  CE: 3.4146  
[05/05 17:09:23] SuperNet Training INFO: iter: 69240/144360  CE: 3.4237  
[05/05 17:10:21] SuperNet Training INFO: iter: 69360/144360  CE: 3.6859  
[05/05 17:11:21] SuperNet Training INFO: iter: 69480/144360  CE: 3.1701  
[05/05 17:12:18] SuperNet Training INFO: iter: 69600/144360  CE: 3.4522  
[05/05 17:13:17] SuperNet Training INFO: iter: 69720/144360  CE: 3.2528  
[05/05 17:13:42] SuperNet Training INFO: --> epoch:  58/120  avg CE: 3.4830  lr: 0.06314015737457618  
[05/05 17:14:49] SuperNet Training INFO: iter: 69840/144360  CE: 3.6092  
[05/05 17:15:49] SuperNet Training INFO: iter: 69960/144360  CE: 3.8051  
[05/05 17:16:49] SuperNet Training INFO: iter: 70080/144360  CE: 3.5897  
[05/05 17:17:49] SuperNet Training INFO: iter: 70200/144360  CE: 3.2597  
[05/05 17:18:48] SuperNet Training INFO: iter: 70320/144360  CE: 3.6626  
[05/05 17:19:49] SuperNet Training INFO: iter: 70440/144360  CE: 3.2834  
[05/05 17:20:49] SuperNet Training INFO: iter: 70560/144360  CE: 3.9111  
[05/05 17:21:51] SuperNet Training INFO: iter: 70680/144360  CE: 3.4134  
[05/05 17:22:50] SuperNet Training INFO: iter: 70800/144360  CE: 3.3219  
[05/05 17:23:50] SuperNet Training INFO: iter: 70920/144360  CE: 3.1357  
[05/05 17:24:16] SuperNet Training INFO: --> epoch:  59/120  avg CE: 3.4608  lr: 0.061570616898472125  
[05/05 17:25:23] SuperNet Training INFO: iter: 71040/144360  CE: 3.3317  
[05/05 17:26:23] SuperNet Training INFO: iter: 71160/144360  CE: 3.1060  
[05/05 17:27:23] SuperNet Training INFO: iter: 71280/144360  CE: 3.4660  
[05/05 17:28:23] SuperNet Training INFO: iter: 71400/144360  CE: 3.5155  
[05/05 17:29:23] SuperNet Training INFO: iter: 71520/144360  CE: 3.5193  
[05/05 17:30:23] SuperNet Training INFO: iter: 71640/144360  CE: 3.4982  
[05/05 17:31:22] SuperNet Training INFO: iter: 71760/144360  CE: 3.4353  
[05/05 17:32:21] SuperNet Training INFO: iter: 71880/144360  CE: 3.3758  
[05/05 17:33:19] SuperNet Training INFO: iter: 72000/144360  CE: 3.2227  
[05/05 17:34:19] SuperNet Training INFO: iter: 72120/144360  CE: 3.3617  
[05/05 17:34:47] SuperNet Training INFO: --> epoch:  60/120  avg CE: 3.4588  lr: 0.05999999999999976  
[05/05 17:35:51] SuperNet Training INFO: iter: 72240/144360  CE: 3.3964  
[05/05 17:36:50] SuperNet Training INFO: iter: 72360/144360  CE: 3.6218  
[05/05 17:37:51] SuperNet Training INFO: iter: 72480/144360  CE: 3.3514  
[05/05 17:38:50] SuperNet Training INFO: iter: 72600/144360  CE: 3.4888  
[05/05 17:39:49] SuperNet Training INFO: iter: 72720/144360  CE: 3.1782  
[05/05 17:40:48] SuperNet Training INFO: iter: 72840/144360  CE: 3.1799  
[05/05 17:41:48] SuperNet Training INFO: iter: 72960/144360  CE: 3.1512  
[05/05 17:42:48] SuperNet Training INFO: iter: 73080/144360  CE: 3.5108  
[05/05 17:43:48] SuperNet Training INFO: iter: 73200/144360  CE: 3.4947  
[05/05 17:44:48] SuperNet Training INFO: iter: 73320/144360  CE: 3.6382  
[05/05 17:45:20] SuperNet Training INFO: --> epoch:  61/120  avg CE: 3.4381  lr: 0.058429383101527455  
[05/05 17:46:24] SuperNet Training INFO: iter: 73440/144360  CE: 3.4100  
[05/05 17:47:24] SuperNet Training INFO: iter: 73560/144360  CE: 3.4015  
[05/05 17:48:25] SuperNet Training INFO: iter: 73680/144360  CE: 3.2983  
[05/05 17:49:25] SuperNet Training INFO: iter: 73800/144360  CE: 3.1335  
[05/05 17:50:25] SuperNet Training INFO: iter: 73920/144360  CE: 3.6342  
[05/05 17:51:24] SuperNet Training INFO: iter: 74040/144360  CE: 3.7617  
[05/05 17:52:22] SuperNet Training INFO: iter: 74160/144360  CE: 3.4706  
[05/05 17:53:22] SuperNet Training INFO: iter: 74280/144360  CE: 3.4761  
[05/05 17:54:19] SuperNet Training INFO: iter: 74400/144360  CE: 3.3022  
[05/05 17:55:17] SuperNet Training INFO: iter: 74520/144360  CE: 3.2965  
[05/05 17:55:47] SuperNet Training INFO: --> epoch:  62/120  avg CE: 3.4196  lr: 0.05685984262542327  
[05/05 17:56:48] SuperNet Training INFO: iter: 74640/144360  CE: 3.8042  
[05/05 17:57:47] SuperNet Training INFO: iter: 74760/144360  CE: 3.3675  
[05/05 17:58:46] SuperNet Training INFO: iter: 74880/144360  CE: 3.6941  
[05/05 17:59:45] SuperNet Training INFO: iter: 75000/144360  CE: 3.4852  
[05/05 18:00:45] SuperNet Training INFO: iter: 75120/144360  CE: 3.0499  
[05/05 18:01:46] SuperNet Training INFO: iter: 75240/144360  CE: 3.3260  
[05/05 18:02:47] SuperNet Training INFO: iter: 75360/144360  CE: 3.5159  
[05/05 18:03:47] SuperNet Training INFO: iter: 75480/144360  CE: 3.6039  
[05/05 18:04:47] SuperNet Training INFO: iter: 75600/144360  CE: 3.4749  
[05/05 18:05:48] SuperNet Training INFO: iter: 75720/144360  CE: 3.1699  
[05/05 18:06:21] SuperNet Training INFO: --> epoch:  63/120  avg CE: 3.4110  lr: 0.05529245425632924  
[05/05 18:07:22] SuperNet Training INFO: iter: 75840/144360  CE: 3.4707  
[05/05 18:08:21] SuperNet Training INFO: iter: 75960/144360  CE: 3.4011  
[05/05 18:09:21] SuperNet Training INFO: iter: 76080/144360  CE: 3.6034  
[05/05 18:10:21] SuperNet Training INFO: iter: 76200/144360  CE: 3.2522  
[05/05 18:11:20] SuperNet Training INFO: iter: 76320/144360  CE: 3.2828  
[05/05 18:12:20] SuperNet Training INFO: iter: 76440/144360  CE: 3.3229  
[05/05 18:13:20] SuperNet Training INFO: iter: 76560/144360  CE: 3.2985  
[05/05 18:14:18] SuperNet Training INFO: iter: 76680/144360  CE: 3.3714  
[05/05 18:15:18] SuperNet Training INFO: iter: 76800/144360  CE: 3.5735  
[05/05 18:16:17] SuperNet Training INFO: iter: 76920/144360  CE: 3.3058  
[05/05 18:16:52] SuperNet Training INFO: --> epoch:  64/120  avg CE: 3.3866  lr: 0.05372829220394078  
[05/05 18:17:52] SuperNet Training INFO: iter: 77040/144360  CE: 3.2407  
[05/05 18:18:52] SuperNet Training INFO: iter: 77160/144360  CE: 3.3890  
[05/05 18:19:53] SuperNet Training INFO: iter: 77280/144360  CE: 3.3825  
[05/05 18:20:53] SuperNet Training INFO: iter: 77400/144360  CE: 3.2286  
[05/05 18:21:53] SuperNet Training INFO: iter: 77520/144360  CE: 3.5953  
[05/05 18:22:53] SuperNet Training INFO: iter: 77640/144360  CE: 3.5647  
[05/05 18:23:51] SuperNet Training INFO: iter: 77760/144360  CE: 3.0896  
[05/05 18:24:49] SuperNet Training INFO: iter: 77880/144360  CE: 3.5417  
[05/05 18:25:48] SuperNet Training INFO: iter: 78000/144360  CE: 3.5774  
[05/05 18:26:45] SuperNet Training INFO: iter: 78120/144360  CE: 3.3322  
[05/05 18:27:20] SuperNet Training INFO: --> epoch:  65/120  avg CE: 3.3776  lr: 0.05216842846679694  
[05/05 18:28:18] SuperNet Training INFO: iter: 78240/144360  CE: 3.3735  
[05/05 18:29:18] SuperNet Training INFO: iter: 78360/144360  CE: 3.3783  
[05/05 18:30:18] SuperNet Training INFO: iter: 78480/144360  CE: 3.2084  
[05/05 18:31:20] SuperNet Training INFO: iter: 78600/144360  CE: 3.6908  
[05/05 18:32:21] SuperNet Training INFO: iter: 78720/144360  CE: 3.3293  
[05/05 18:33:20] SuperNet Training INFO: iter: 78840/144360  CE: 3.3750  
[05/05 18:34:20] SuperNet Training INFO: iter: 78960/144360  CE: 3.4755  
[05/05 18:35:18] SuperNet Training INFO: iter: 79080/144360  CE: 3.2206  
[05/05 18:36:17] SuperNet Training INFO: iter: 79200/144360  CE: 3.1404  
[05/05 18:37:16] SuperNet Training INFO: iter: 79320/144360  CE: 3.5574  
[05/05 18:37:54] SuperNet Training INFO: --> epoch:  66/120  avg CE: 3.3621  lr: 0.05061393209758616  
[05/05 18:38:50] SuperNet Training INFO: iter: 79440/144360  CE: 3.5887  
[05/05 18:39:52] SuperNet Training INFO: iter: 79560/144360  CE: 3.2123  
[05/05 18:40:53] SuperNet Training INFO: iter: 79680/144360  CE: 3.4599  
[05/05 18:41:53] SuperNet Training INFO: iter: 79800/144360  CE: 3.2900  
[05/05 18:42:54] SuperNet Training INFO: iter: 79920/144360  CE: 3.2271  
[05/05 18:43:54] SuperNet Training INFO: iter: 80040/144360  CE: 3.2858  
[05/05 18:44:53] SuperNet Training INFO: iter: 80160/144360  CE: 3.1675  
[05/05 18:45:52] SuperNet Training INFO: iter: 80280/144360  CE: 3.7376  
[05/05 18:46:50] SuperNet Training INFO: iter: 80400/144360  CE: 3.4319  
[05/05 18:47:47] SuperNet Training INFO: iter: 80520/144360  CE: 3.4457  
[05/05 18:48:26] SuperNet Training INFO: --> epoch:  67/120  avg CE: 3.3462  lr: 0.04906586847047105  
[05/05 18:49:21] SuperNet Training INFO: iter: 80640/144360  CE: 2.9820  
[05/05 18:50:21] SuperNet Training INFO: iter: 80760/144360  CE: 3.4315  
[05/05 18:51:20] SuperNet Training INFO: iter: 80880/144360  CE: 3.0123  
[05/05 18:52:19] SuperNet Training INFO: iter: 81000/144360  CE: 3.6571  
[05/05 18:53:18] SuperNet Training INFO: iter: 81120/144360  CE: 3.2801  
[05/05 18:54:17] SuperNet Training INFO: iter: 81240/144360  CE: 3.5420  
[05/05 18:55:18] SuperNet Training INFO: iter: 81360/144360  CE: 3.2230  
[05/05 18:56:17] SuperNet Training INFO: iter: 81480/144360  CE: 3.1471  
[05/05 18:57:16] SuperNet Training INFO: iter: 81600/144360  CE: 3.1957  
[05/05 18:58:15] SuperNet Training INFO: iter: 81720/144360  CE: 3.4133  
[05/05 18:58:56] SuperNet Training INFO: --> epoch:  68/120  avg CE: 3.3370  lr: 0.04752529855093431  
[05/05 18:59:49] SuperNet Training INFO: iter: 81840/144360  CE: 2.9176  
[05/05 19:00:49] SuperNet Training INFO: iter: 81960/144360  CE: 3.3292  
[05/05 19:01:48] SuperNet Training INFO: iter: 82080/144360  CE: 3.4902  
[05/05 19:02:47] SuperNet Training INFO: iter: 82200/144360  CE: 3.3116  
[05/05 19:03:44] SuperNet Training INFO: iter: 82320/144360  CE: 3.2836  
[05/05 19:04:42] SuperNet Training INFO: iter: 82440/144360  CE: 3.6450  
[05/05 19:05:40] SuperNet Training INFO: iter: 82560/144360  CE: 3.1355  
[05/05 19:06:38] SuperNet Training INFO: iter: 82680/144360  CE: 3.2532  
[05/05 19:07:36] SuperNet Training INFO: iter: 82800/144360  CE: 3.4664  
[05/05 19:08:34] SuperNet Training INFO: iter: 82920/144360  CE: 3.2293  
[05/05 19:09:15] SuperNet Training INFO: --> epoch:  69/120  avg CE: 3.3265  lr: 0.04599327816864548  
[05/05 19:10:08] SuperNet Training INFO: iter: 83040/144360  CE: 3.3362  
[05/05 19:11:08] SuperNet Training INFO: iter: 83160/144360  CE: 3.3572  
[05/05 19:12:08] SuperNet Training INFO: iter: 83280/144360  CE: 3.3517  
[05/05 19:13:08] SuperNet Training INFO: iter: 83400/144360  CE: 3.2657  
[05/05 19:14:08] SuperNet Training INFO: iter: 83520/144360  CE: 3.1571  
[05/05 19:15:08] SuperNet Training INFO: iter: 83640/144360  CE: 3.1295  
[05/05 19:16:08] SuperNet Training INFO: iter: 83760/144360  CE: 3.1888  
[05/05 19:17:09] SuperNet Training INFO: iter: 83880/144360  CE: 3.2855  
[05/05 19:18:09] SuperNet Training INFO: iter: 84000/144360  CE: 3.4707  
[05/05 19:19:07] SuperNet Training INFO: iter: 84120/144360  CE: 2.8833  
[05/05 19:19:52] SuperNet Training INFO: --> epoch:  70/120  avg CE: 3.3192  lr: 0.04447085729384866  
[05/05 19:20:41] SuperNet Training INFO: iter: 84240/144360  CE: 3.6350  
[05/05 19:21:42] SuperNet Training INFO: iter: 84360/144360  CE: 2.9005  
[05/05 19:22:41] SuperNet Training INFO: iter: 84480/144360  CE: 3.2051  
[05/05 19:23:42] SuperNet Training INFO: iter: 84600/144360  CE: 3.6178  
[05/05 19:24:42] SuperNet Training INFO: iter: 84720/144360  CE: 3.0090  
[05/05 19:25:41] SuperNet Training INFO: iter: 84840/144360  CE: 3.0921  
[05/05 19:26:40] SuperNet Training INFO: iter: 84960/144360  CE: 3.2435  
[05/05 19:27:39] SuperNet Training INFO: iter: 85080/144360  CE: 3.2704  
[05/05 19:28:39] SuperNet Training INFO: iter: 85200/144360  CE: 3.2521  
[05/05 19:29:38] SuperNet Training INFO: iter: 85320/144360  CE: 3.1210  
[05/05 19:30:23] SuperNet Training INFO: --> epoch:  71/120  avg CE: 3.3043  lr: 0.04295907931776456  
[05/05 19:31:11] SuperNet Training INFO: iter: 85440/144360  CE: 3.1706  
[05/05 19:32:10] SuperNet Training INFO: iter: 85560/144360  CE: 3.4245  
[05/05 19:33:10] SuperNet Training INFO: iter: 85680/144360  CE: 3.0412  
[05/05 19:34:10] SuperNet Training INFO: iter: 85800/144360  CE: 3.1758  
[05/05 19:35:11] SuperNet Training INFO: iter: 85920/144360  CE: 3.1260  
[05/05 19:36:10] SuperNet Training INFO: iter: 86040/144360  CE: 3.3577  
[05/05 19:37:11] SuperNet Training INFO: iter: 86160/144360  CE: 3.4303  
[05/05 19:38:11] SuperNet Training INFO: iter: 86280/144360  CE: 3.1993  
[05/05 19:39:11] SuperNet Training INFO: iter: 86400/144360  CE: 3.3001  
[05/05 19:40:12] SuperNet Training INFO: iter: 86520/144360  CE: 3.3749  
[05/05 19:40:58] SuperNet Training INFO: --> epoch:  72/120  avg CE: 3.2906  lr: 0.04145898033750296  
[05/05 19:41:45] SuperNet Training INFO: iter: 86640/144360  CE: 3.2304  
[05/05 19:42:46] SuperNet Training INFO: iter: 86760/144360  CE: 3.2280  
[05/05 19:43:45] SuperNet Training INFO: iter: 86880/144360  CE: 3.2533  
[05/05 19:44:44] SuperNet Training INFO: iter: 87000/144360  CE: 3.2329  
[05/05 19:45:44] SuperNet Training INFO: iter: 87120/144360  CE: 3.2121  
[05/05 19:46:43] SuperNet Training INFO: iter: 87240/144360  CE: 3.2168  
[05/05 19:47:42] SuperNet Training INFO: iter: 87360/144360  CE: 3.1553  
[05/05 19:48:41] SuperNet Training INFO: iter: 87480/144360  CE: 3.7319  
[05/05 19:49:41] SuperNet Training INFO: iter: 87600/144360  CE: 3.1587  
[05/05 19:50:41] SuperNet Training INFO: iter: 87720/144360  CE: 3.3978  
[05/05 19:51:29] SuperNet Training INFO: --> epoch:  73/120  avg CE: 3.2751  lr: 0.03997158844597365  
[05/05 19:52:15] SuperNet Training INFO: iter: 87840/144360  CE: 3.2253  
[05/05 19:53:15] SuperNet Training INFO: iter: 87960/144360  CE: 3.4138  
[05/05 19:54:14] SuperNet Training INFO: iter: 88080/144360  CE: 3.1716  
[05/05 19:55:13] SuperNet Training INFO: iter: 88200/144360  CE: 3.2240  
[05/05 19:56:11] SuperNet Training INFO: iter: 88320/144360  CE: 3.2731  
[05/05 19:57:10] SuperNet Training INFO: iter: 88440/144360  CE: 3.1516  
[05/05 19:58:08] SuperNet Training INFO: iter: 88560/144360  CE: 2.9395  
[05/05 19:59:07] SuperNet Training INFO: iter: 88680/144360  CE: 3.0649  
[05/05 20:00:05] SuperNet Training INFO: iter: 88800/144360  CE: 3.2378  
[05/05 20:01:03] SuperNet Training INFO: iter: 88920/144360  CE: 3.4757  
[05/05 20:01:52] SuperNet Training INFO: --> epoch:  74/120  avg CE: 3.2616  lr: 0.03849792302728192  
[05/05 20:02:36] SuperNet Training INFO: iter: 89040/144360  CE: 3.2167  
[05/05 20:03:36] SuperNet Training INFO: iter: 89160/144360  CE: 3.2859  
[05/05 20:04:37] SuperNet Training INFO: iter: 89280/144360  CE: 3.0104  
[05/05 20:05:37] SuperNet Training INFO: iter: 89400/144360  CE: 3.0825  
[05/05 20:06:36] SuperNet Training INFO: iter: 89520/144360  CE: 3.0332  
[05/05 20:07:36] SuperNet Training INFO: iter: 89640/144360  CE: 3.3001  
[05/05 20:08:35] SuperNet Training INFO: iter: 89760/144360  CE: 3.5861  
[05/05 20:09:35] SuperNet Training INFO: iter: 89880/144360  CE: 3.1703  
[05/05 20:10:35] SuperNet Training INFO: iter: 90000/144360  CE: 3.2176  
[05/05 20:11:32] SuperNet Training INFO: iter: 90120/144360  CE: 3.2387  
[05/05 20:12:21] SuperNet Training INFO: --> epoch:  75/120  avg CE: 3.2557  lr: 0.03703899405809455  
[05/05 20:13:04] SuperNet Training INFO: iter: 90240/144360  CE: 3.2202  
[05/05 20:14:05] SuperNet Training INFO: iter: 90360/144360  CE: 3.0375  
[05/05 20:15:06] SuperNet Training INFO: iter: 90480/144360  CE: 3.3771  
[05/05 20:16:07] SuperNet Training INFO: iter: 90600/144360  CE: 3.0168  
[05/05 20:17:07] SuperNet Training INFO: iter: 90720/144360  CE: 3.1131  
[05/05 20:18:06] SuperNet Training INFO: iter: 90840/144360  CE: 3.2890  
[05/05 20:19:06] SuperNet Training INFO: iter: 90960/144360  CE: 3.2927  
[05/05 20:20:06] SuperNet Training INFO: iter: 91080/144360  CE: 3.4472  
[05/05 20:21:05] SuperNet Training INFO: iter: 91200/144360  CE: 3.3994  
[05/05 20:22:03] SuperNet Training INFO: iter: 91320/144360  CE: 3.0410  
[05/05 20:22:55] SuperNet Training INFO: --> epoch:  76/120  avg CE: 3.2444  lr: 0.035595801415451815  
[05/05 20:23:37] SuperNet Training INFO: iter: 91440/144360  CE: 3.3809  
[05/05 20:24:36] SuperNet Training INFO: iter: 91560/144360  CE: 3.5770  
[05/05 20:25:37] SuperNet Training INFO: iter: 91680/144360  CE: 3.1761  
[05/05 20:26:37] SuperNet Training INFO: iter: 91800/144360  CE: 2.9392  
[05/05 20:27:35] SuperNet Training INFO: iter: 91920/144360  CE: 2.8280  
[05/05 20:28:35] SuperNet Training INFO: iter: 92040/144360  CE: 3.2990  
[05/05 20:29:35] SuperNet Training INFO: iter: 92160/144360  CE: 3.2855  
[05/05 20:30:33] SuperNet Training INFO: iter: 92280/144360  CE: 3.1672  
[05/05 20:31:32] SuperNet Training INFO: iter: 92400/144360  CE: 3.4497  
[05/05 20:32:30] SuperNet Training INFO: iter: 92520/144360  CE: 3.0640  
[05/05 20:33:23] SuperNet Training INFO: --> epoch:  77/120  avg CE: 3.2400  lr: 0.03416933419150217  
[05/05 20:34:02] SuperNet Training INFO: iter: 92640/144360  CE: 3.1175  
[05/05 20:35:02] SuperNet Training INFO: iter: 92760/144360  CE: 3.1449  
[05/05 20:36:01] SuperNet Training INFO: iter: 92880/144360  CE: 3.3781  
[05/05 20:37:00] SuperNet Training INFO: iter: 93000/144360  CE: 3.2237  
[05/05 20:38:00] SuperNet Training INFO: iter: 93120/144360  CE: 3.1750  
[05/05 20:39:00] SuperNet Training INFO: iter: 93240/144360  CE: 3.1794  
[05/05 20:39:59] SuperNet Training INFO: iter: 93360/144360  CE: 3.2493  
[05/05 20:40:58] SuperNet Training INFO: iter: 93480/144360  CE: 3.2642  
[05/05 20:41:58] SuperNet Training INFO: iter: 93600/144360  CE: 3.2068  
[05/05 20:42:58] SuperNet Training INFO: iter: 93720/144360  CE: 3.2712  
[05/05 20:43:55] SuperNet Training INFO: --> epoch:  78/120  avg CE: 3.2320  lr: 0.03276057001562702  
[05/05 20:44:34] SuperNet Training INFO: iter: 93840/144360  CE: 3.3935  
[05/05 20:45:34] SuperNet Training INFO: iter: 93960/144360  CE: 3.3276  
[05/05 20:46:35] SuperNet Training INFO: iter: 94080/144360  CE: 3.1127  
[05/05 20:47:34] SuperNet Training INFO: iter: 94200/144360  CE: 3.2066  
[05/05 20:48:34] SuperNet Training INFO: iter: 94320/144360  CE: 2.9287  
[05/05 20:49:35] SuperNet Training INFO: iter: 94440/144360  CE: 3.3386  
[05/05 20:50:34] SuperNet Training INFO: iter: 94560/144360  CE: 3.1724  
[05/05 20:51:33] SuperNet Training INFO: iter: 94680/144360  CE: 3.5018  
[05/05 20:52:34] SuperNet Training INFO: iter: 94800/144360  CE: 3.5031  
[05/05 20:53:33] SuperNet Training INFO: iter: 94920/144360  CE: 3.3797  
[05/05 20:54:29] SuperNet Training INFO: --> epoch:  79/120  avg CE: 3.2095  lr: 0.031370474384423336  
[05/05 20:55:06] SuperNet Training INFO: iter: 95040/144360  CE: 3.1493  
[05/05 20:56:06] SuperNet Training INFO: iter: 95160/144360  CE: 3.3307  
[05/05 20:57:05] SuperNet Training INFO: iter: 95280/144360  CE: 3.2507  
[05/05 20:58:03] SuperNet Training INFO: iter: 95400/144360  CE: 3.1911  
[05/05 20:59:01] SuperNet Training INFO: iter: 95520/144360  CE: 3.1161  
[05/05 20:59:58] SuperNet Training INFO: iter: 95640/144360  CE: 3.1494  
[05/05 21:00:56] SuperNet Training INFO: iter: 95760/144360  CE: 3.1222  
[05/05 21:01:54] SuperNet Training INFO: iter: 95880/144360  CE: 3.3562  
[05/05 21:02:53] SuperNet Training INFO: iter: 96000/144360  CE: 3.3577  
[05/05 21:03:51] SuperNet Training INFO: iter: 96120/144360  CE: 2.8691  
[05/05 21:04:47] SuperNet Training INFO: iter: 96240/144360  CE: 3.1200  
[05/05 21:04:47] SuperNet Training INFO: --> epoch:  80/120  avg CE: 3.1939  lr: 0.02999999999999983  
[05/05 21:06:21] SuperNet Training INFO: iter: 96360/144360  CE: 3.0803  
[05/05 21:07:23] SuperNet Training INFO: iter: 96480/144360  CE: 2.8135  
[05/05 21:08:22] SuperNet Training INFO: iter: 96600/144360  CE: 2.9696  
[05/05 21:09:22] SuperNet Training INFO: iter: 96720/144360  CE: 3.0954  
[05/05 21:10:23] SuperNet Training INFO: iter: 96840/144360  CE: 3.1165  
[05/05 21:11:25] SuperNet Training INFO: iter: 96960/144360  CE: 3.2160  
[05/05 21:12:25] SuperNet Training INFO: iter: 97080/144360  CE: 3.0881  
[05/05 21:13:25] SuperNet Training INFO: iter: 97200/144360  CE: 3.4631  
[05/05 21:14:25] SuperNet Training INFO: iter: 97320/144360  CE: 3.5174  
[05/05 21:15:24] SuperNet Training INFO: iter: 97440/144360  CE: 3.3470  
[05/05 21:15:25] SuperNet Training INFO: --> epoch:  81/120  avg CE: 3.1877  lr: 0.028650086117042926  
[05/05 21:16:57] SuperNet Training INFO: iter: 97560/144360  CE: 3.5388  
[05/05 21:17:55] SuperNet Training INFO: iter: 97680/144360  CE: 3.0241  
[05/05 21:18:55] SuperNet Training INFO: iter: 97800/144360  CE: 3.1043  
[05/05 21:19:54] SuperNet Training INFO: iter: 97920/144360  CE: 3.1012  
[05/05 21:20:53] SuperNet Training INFO: iter: 98040/144360  CE: 3.0966  
[05/05 21:21:52] SuperNet Training INFO: iter: 98160/144360  CE: 3.4083  
[05/05 21:22:51] SuperNet Training INFO: iter: 98280/144360  CE: 3.1149  
[05/05 21:23:49] SuperNet Training INFO: iter: 98400/144360  CE: 3.1621  
[05/05 21:24:50] SuperNet Training INFO: iter: 98520/144360  CE: 3.2860  
[05/05 21:25:49] SuperNet Training INFO: iter: 98640/144360  CE: 3.3876  
[05/05 21:25:51] SuperNet Training INFO: --> epoch:  82/120  avg CE: 3.1761  lr: 0.027321657899098132  
[05/05 21:27:21] SuperNet Training INFO: iter: 98760/144360  CE: 2.9632  
[05/05 21:28:21] SuperNet Training INFO: iter: 98880/144360  CE: 2.9725  
[05/05 21:29:19] SuperNet Training INFO: iter: 99000/144360  CE: 2.9991  
[05/05 21:30:19] SuperNet Training INFO: iter: 99120/144360  CE: 3.1796  
[05/05 21:31:19] SuperNet Training INFO: iter: 99240/144360  CE: 3.1142  
[05/05 21:32:20] SuperNet Training INFO: iter: 99360/144360  CE: 3.2840  
[05/05 21:33:20] SuperNet Training INFO: iter: 99480/144360  CE: 2.9994  
[05/05 21:34:19] SuperNet Training INFO: iter: 99600/144360  CE: 3.2960  
[05/05 21:35:18] SuperNet Training INFO: iter: 99720/144360  CE: 3.1942  
[05/05 21:36:17] SuperNet Training INFO: iter: 99840/144360  CE: 3.2841  
[05/05 21:36:21] SuperNet Training INFO: --> epoch:  83/120  avg CE: 3.1672  lr: 0.0260156257845098  
[05/05 21:37:54] SuperNet Training INFO: iter: 99960/144360  CE: 3.3317  
[05/05 21:38:56] SuperNet Training INFO: iter: 100080/144360  CE: 3.1502  
[05/05 21:39:56] SuperNet Training INFO: iter: 100200/144360  CE: 3.4552  
[05/05 21:40:57] SuperNet Training INFO: iter: 100320/144360  CE: 3.1955  
[05/05 21:41:57] SuperNet Training INFO: iter: 100440/144360  CE: 3.0770  
[05/05 21:42:56] SuperNet Training INFO: iter: 100560/144360  CE: 3.1423  
[05/05 21:43:55] SuperNet Training INFO: iter: 100680/144360  CE: 2.9740  
[05/05 21:44:53] SuperNet Training INFO: iter: 100800/144360  CE: 3.2195  
[05/05 21:45:52] SuperNet Training INFO: iter: 100920/144360  CE: 3.1832  
[05/05 21:46:50] SuperNet Training INFO: iter: 101040/144360  CE: 3.2705  
[05/05 21:46:54] SuperNet Training INFO: --> epoch:  84/120  avg CE: 3.1552  lr: 0.02473288486245143  
[05/05 21:48:23] SuperNet Training INFO: iter: 101160/144360  CE: 3.0513  
[05/05 21:49:22] SuperNet Training INFO: iter: 101280/144360  CE: 3.2612  
[05/05 21:50:23] SuperNet Training INFO: iter: 101400/144360  CE: 3.1774  
[05/05 21:51:24] SuperNet Training INFO: iter: 101520/144360  CE: 2.9776  
[05/05 21:52:24] SuperNet Training INFO: iter: 101640/144360  CE: 3.2296  
[05/05 21:53:25] SuperNet Training INFO: iter: 101760/144360  CE: 3.0995  
[05/05 21:54:25] SuperNet Training INFO: iter: 101880/144360  CE: 3.1986  
[05/05 21:55:24] SuperNet Training INFO: iter: 102000/144360  CE: 3.1466  
[05/05 21:56:23] SuperNet Training INFO: iter: 102120/144360  CE: 3.0085  
[05/05 21:57:21] SuperNet Training INFO: iter: 102240/144360  CE: 3.0242  
[05/05 21:57:27] SuperNet Training INFO: --> epoch:  85/120  avg CE: 3.1527  lr: 0.023474314259476523  
[05/05 21:58:55] SuperNet Training INFO: iter: 102360/144360  CE: 3.1301  
[05/05 21:59:55] SuperNet Training INFO: iter: 102480/144360  CE: 3.2244  
[05/05 22:00:55] SuperNet Training INFO: iter: 102600/144360  CE: 3.3711  
[05/05 22:01:56] SuperNet Training INFO: iter: 102720/144360  CE: 2.7938  
[05/05 22:02:56] SuperNet Training INFO: iter: 102840/144360  CE: 3.1634  
[05/05 22:03:58] SuperNet Training INFO: iter: 102960/144360  CE: 3.0711  
[05/05 22:04:58] SuperNet Training INFO: iter: 103080/144360  CE: 3.0759  
[05/05 22:05:58] SuperNet Training INFO: iter: 103200/144360  CE: 3.1784  
[05/05 22:06:59] SuperNet Training INFO: iter: 103320/144360  CE: 3.0129  
[05/05 22:07:59] SuperNet Training INFO: iter: 103440/144360  CE: 3.3217  
[05/05 22:08:08] SuperNet Training INFO: --> epoch:  86/120  avg CE: 3.1338  lr: 0.02224077653700959  
[05/05 22:09:34] SuperNet Training INFO: iter: 103560/144360  CE: 3.1667  
[05/05 22:10:34] SuperNet Training INFO: iter: 103680/144360  CE: 2.8164  
[05/05 22:11:34] SuperNet Training INFO: iter: 103800/144360  CE: 3.0710  
[05/05 22:12:34] SuperNet Training INFO: iter: 103920/144360  CE: 3.1639  
[05/05 22:13:32] SuperNet Training INFO: iter: 104040/144360  CE: 3.0763  
[05/05 22:14:32] SuperNet Training INFO: iter: 104160/144360  CE: 3.0541  
[05/05 22:15:29] SuperNet Training INFO: iter: 104280/144360  CE: 2.9167  
[05/05 22:16:27] SuperNet Training INFO: iter: 104400/144360  CE: 3.1334  
[05/05 22:17:25] SuperNet Training INFO: iter: 104520/144360  CE: 3.1859  
[05/05 22:18:23] SuperNet Training INFO: iter: 104640/144360  CE: 3.1179  
[05/05 22:18:32] SuperNet Training INFO: --> epoch:  87/120  avg CE: 3.1200  lr: 0.02103311710018879  
[05/05 22:19:58] SuperNet Training INFO: iter: 104760/144360  CE: 3.3395  
[05/05 22:20:57] SuperNet Training INFO: iter: 104880/144360  CE: 3.0333  
[05/05 22:21:58] SuperNet Training INFO: iter: 105000/144360  CE: 3.6006  
[05/05 22:22:58] SuperNet Training INFO: iter: 105120/144360  CE: 3.1187  
[05/05 22:23:59] SuperNet Training INFO: iter: 105240/144360  CE: 3.3507  
[05/05 22:24:58] SuperNet Training INFO: iter: 105360/144360  CE: 3.0022  
[05/05 22:25:59] SuperNet Training INFO: iter: 105480/144360  CE: 3.5772  
[05/05 22:26:59] SuperNet Training INFO: iter: 105600/144360  CE: 3.3320  
[05/05 22:27:59] SuperNet Training INFO: iter: 105720/144360  CE: 3.0156  
[05/05 22:28:59] SuperNet Training INFO: iter: 105840/144360  CE: 3.0262  
[05/05 22:29:10] SuperNet Training INFO: --> epoch:  88/120  avg CE: 3.1197  lr: 0.019852163618468272  
[05/05 22:30:33] SuperNet Training INFO: iter: 105960/144360  CE: 3.3209  
[05/05 22:31:33] SuperNet Training INFO: iter: 106080/144360  CE: 3.1181  
[05/05 22:32:33] SuperNet Training INFO: iter: 106200/144360  CE: 2.7589  
[05/05 22:33:33] SuperNet Training INFO: iter: 106320/144360  CE: 2.8977  
[05/05 22:34:35] SuperNet Training INFO: iter: 106440/144360  CE: 3.1157  
[05/05 22:35:34] SuperNet Training INFO: iter: 106560/144360  CE: 3.2040  
[05/05 22:36:35] SuperNet Training INFO: iter: 106680/144360  CE: 2.9786  
[05/05 22:37:35] SuperNet Training INFO: iter: 106800/144360  CE: 3.1933  
[05/05 22:38:34] SuperNet Training INFO: iter: 106920/144360  CE: 3.1038  
[05/05 22:39:32] SuperNet Training INFO: iter: 107040/144360  CE: 3.2132  
[05/05 22:39:44] SuperNet Training INFO: --> epoch:  89/120  avg CE: 3.1109  lr: 0.018698725458374543  
[05/05 22:41:07] SuperNet Training INFO: iter: 107160/144360  CE: 3.3055  
[05/05 22:42:07] SuperNet Training INFO: iter: 107280/144360  CE: 3.1175  
[05/05 22:43:08] SuperNet Training INFO: iter: 107400/144360  CE: 3.2806  
[05/05 22:44:08] SuperNet Training INFO: iter: 107520/144360  CE: 2.9920  
[05/05 22:45:08] SuperNet Training INFO: iter: 107640/144360  CE: 3.1675  
[05/05 22:46:06] SuperNet Training INFO: iter: 107760/144360  CE: 2.8920  
[05/05 22:47:05] SuperNet Training INFO: iter: 107880/144360  CE: 3.0495  
[05/05 22:48:05] SuperNet Training INFO: iter: 108000/144360  CE: 2.7539  
[05/05 22:49:04] SuperNet Training INFO: iter: 108120/144360  CE: 3.0004  
[05/05 22:50:04] SuperNet Training INFO: iter: 108240/144360  CE: 3.1494  
[05/05 22:50:18] SuperNet Training INFO: --> epoch:  90/120  avg CE: 3.0906  lr: 0.01757359312880703  
[05/05 22:51:38] SuperNet Training INFO: iter: 108360/144360  CE: 2.9738  
[05/05 22:52:38] SuperNet Training INFO: iter: 108480/144360  CE: 3.0334  
[05/05 22:53:38] SuperNet Training INFO: iter: 108600/144360  CE: 2.9233  
[05/05 22:54:38] SuperNet Training INFO: iter: 108720/144360  CE: 3.4388  
[05/05 22:55:38] SuperNet Training INFO: iter: 108840/144360  CE: 3.0252  
[05/05 22:56:39] SuperNet Training INFO: iter: 108960/144360  CE: 2.9012  
[05/05 22:57:38] SuperNet Training INFO: iter: 109080/144360  CE: 3.1090  
[05/05 22:58:37] SuperNet Training INFO: iter: 109200/144360  CE: 2.9439  
[05/05 22:59:37] SuperNet Training INFO: iter: 109320/144360  CE: 3.1671  
[05/05 23:00:36] SuperNet Training INFO: iter: 109440/144360  CE: 3.1933  
[05/05 23:00:52] SuperNet Training INFO: --> epoch:  91/120  avg CE: 3.0839  lr: 0.016477537739262627  
[05/05 23:02:09] SuperNet Training INFO: iter: 109560/144360  CE: 3.1309  
[05/05 23:03:08] SuperNet Training INFO: iter: 109680/144360  CE: 3.0767  
[05/05 23:04:08] SuperNet Training INFO: iter: 109800/144360  CE: 2.9780  
[05/05 23:05:09] SuperNet Training INFO: iter: 109920/144360  CE: 2.8700  
[05/05 23:06:07] SuperNet Training INFO: iter: 110040/144360  CE: 3.0126  
[05/05 23:07:07] SuperNet Training INFO: iter: 110160/144360  CE: 2.7416  
[05/05 23:08:05] SuperNet Training INFO: iter: 110280/144360  CE: 2.8890  
[05/05 23:09:05] SuperNet Training INFO: iter: 110400/144360  CE: 3.0931  
[05/05 23:10:06] SuperNet Training INFO: iter: 110520/144360  CE: 3.2087  
[05/05 23:11:05] SuperNet Training INFO: iter: 110640/144360  CE: 3.1092  
[05/05 23:11:22] SuperNet Training INFO: --> epoch:  92/120  avg CE: 3.0767  lr: 0.015411310471356233  
[05/05 23:12:38] SuperNet Training INFO: iter: 110760/144360  CE: 3.1723  
[05/05 23:13:36] SuperNet Training INFO: iter: 110880/144360  CE: 3.0577  
[05/05 23:14:35] SuperNet Training INFO: iter: 111000/144360  CE: 2.8945  
[05/05 23:15:35] SuperNet Training INFO: iter: 111120/144360  CE: 3.0653  
[05/05 23:16:35] SuperNet Training INFO: iter: 111240/144360  CE: 3.0282  
[05/05 23:17:35] SuperNet Training INFO: iter: 111360/144360  CE: 2.6043  
[05/05 23:18:35] SuperNet Training INFO: iter: 111480/144360  CE: 2.8293  
[05/05 23:19:35] SuperNet Training INFO: iter: 111600/144360  CE: 2.6687  
[05/05 23:20:34] SuperNet Training INFO: iter: 111720/144360  CE: 3.1514  
[05/05 23:21:31] SuperNet Training INFO: iter: 111840/144360  CE: 3.1572  
[05/05 23:21:50] SuperNet Training INFO: --> epoch:  93/120  avg CE: 3.0605  lr: 0.014375642063998082  
[05/05 23:23:05] SuperNet Training INFO: iter: 111960/144360  CE: 3.1130  
[05/05 23:24:04] SuperNet Training INFO: iter: 112080/144360  CE: 3.2187  
[05/05 23:25:03] SuperNet Training INFO: iter: 112200/144360  CE: 2.7967  
[05/05 23:26:03] SuperNet Training INFO: iter: 112320/144360  CE: 3.0293  
[05/05 23:27:02] SuperNet Training INFO: iter: 112440/144360  CE: 3.3415  
[05/05 23:28:01] SuperNet Training INFO: iter: 112560/144360  CE: 3.1396  
[05/05 23:29:00] SuperNet Training INFO: iter: 112680/144360  CE: 2.8774  
[05/05 23:29:57] SuperNet Training INFO: iter: 112800/144360  CE: 2.6747  
[05/05 23:30:55] SuperNet Training INFO: iter: 112920/144360  CE: 2.8194  
[05/05 23:31:53] SuperNet Training INFO: iter: 113040/144360  CE: 2.8511  
[05/05 23:32:12] SuperNet Training INFO: --> epoch:  94/120  avg CE: 3.0574  lr: 0.01337124231258167  
[05/05 23:33:26] SuperNet Training INFO: iter: 113160/144360  CE: 3.0086  
[05/05 23:34:24] SuperNet Training INFO: iter: 113280/144360  CE: 3.0408  
[05/05 23:35:26] SuperNet Training INFO: iter: 113400/144360  CE: 3.0174  
[05/05 23:36:25] SuperNet Training INFO: iter: 113520/144360  CE: 3.2960  
[05/05 23:37:25] SuperNet Training INFO: iter: 113640/144360  CE: 3.0031  
[05/05 23:38:25] SuperNet Training INFO: iter: 113760/144360  CE: 2.8523  
[05/05 23:39:25] SuperNet Training INFO: iter: 113880/144360  CE: 2.8708  
[05/05 23:40:26] SuperNet Training INFO: iter: 114000/144360  CE: 3.0448  
[05/05 23:41:26] SuperNet Training INFO: iter: 114120/144360  CE: 2.9485  
[05/05 23:42:25] SuperNet Training INFO: iter: 114240/144360  CE: 3.2037  
[05/05 23:42:46] SuperNet Training INFO: --> epoch:  95/120  avg CE: 3.0444  lr: 0.012398799582525794  
[05/05 23:43:58] SuperNet Training INFO: iter: 114360/144360  CE: 3.0372  
[05/05 23:44:56] SuperNet Training INFO: iter: 114480/144360  CE: 2.7530  
[05/05 23:45:55] SuperNet Training INFO: iter: 114600/144360  CE: 2.8293  
[05/05 23:46:54] SuperNet Training INFO: iter: 114720/144360  CE: 2.8885  
[05/05 23:47:53] SuperNet Training INFO: iter: 114840/144360  CE: 3.2486  
[05/05 23:48:53] SuperNet Training INFO: iter: 114960/144360  CE: 3.0740  
[05/05 23:49:52] SuperNet Training INFO: iter: 115080/144360  CE: 3.0179  
[05/05 23:50:51] SuperNet Training INFO: iter: 115200/144360  CE: 2.9675  
[05/05 23:51:51] SuperNet Training INFO: iter: 115320/144360  CE: 2.9350  
[05/05 23:52:50] SuperNet Training INFO: iter: 115440/144360  CE: 2.9980  
[05/05 23:53:13] SuperNet Training INFO: --> epoch:  96/120  avg CE: 3.0423  lr: 0.01145898033750309  
[05/05 23:54:24] SuperNet Training INFO: iter: 115560/144360  CE: 3.0580  
[05/05 23:55:25] SuperNet Training INFO: iter: 115680/144360  CE: 2.8098  
[05/05 23:56:24] SuperNet Training INFO: iter: 115800/144360  CE: 2.9741  
[05/05 23:57:24] SuperNet Training INFO: iter: 115920/144360  CE: 3.0255  
[05/05 23:58:23] SuperNet Training INFO: iter: 116040/144360  CE: 3.2364  
[05/05 23:59:24] SuperNet Training INFO: iter: 116160/144360  CE: 2.8187  
[05/06 00:00:23] SuperNet Training INFO: iter: 116280/144360  CE: 2.7879  
[05/06 00:01:23] SuperNet Training INFO: iter: 116400/144360  CE: 2.8093  
[05/06 00:02:22] SuperNet Training INFO: iter: 116520/144360  CE: 3.1297  
[05/06 00:03:22] SuperNet Training INFO: iter: 116640/144360  CE: 2.8666  
[05/06 00:03:47] SuperNet Training INFO: --> epoch:  97/120  avg CE: 3.0307  lr: 0.01055242868267905  
[05/06 00:04:56] SuperNet Training INFO: iter: 116760/144360  CE: 3.1092  
[05/06 00:05:53] SuperNet Training INFO: iter: 116880/144360  CE: 3.0696  
[05/06 00:06:52] SuperNet Training INFO: iter: 117000/144360  CE: 2.9532  
[05/06 00:07:50] SuperNet Training INFO: iter: 117120/144360  CE: 3.0694  
[05/06 00:08:47] SuperNet Training INFO: iter: 117240/144360  CE: 2.9857  
[05/06 00:09:46] SuperNet Training INFO: iter: 117360/144360  CE: 3.1949  
[05/06 00:10:44] SuperNet Training INFO: iter: 117480/144360  CE: 2.8803  
[05/06 00:11:44] SuperNet Training INFO: iter: 117600/144360  CE: 3.0843  
[05/06 00:12:43] SuperNet Training INFO: iter: 117720/144360  CE: 2.9448  
[05/06 00:13:42] SuperNet Training INFO: iter: 117840/144360  CE: 2.9683  
[05/06 00:14:06] SuperNet Training INFO: --> epoch:  98/120  avg CE: 3.0104  lr: 0.009679765923274538  
[05/06 00:15:14] SuperNet Training INFO: iter: 117960/144360  CE: 3.0873  
[05/06 00:16:13] SuperNet Training INFO: iter: 118080/144360  CE: 3.0476  
[05/06 00:17:12] SuperNet Training INFO: iter: 118200/144360  CE: 2.9075  
[05/06 00:18:12] SuperNet Training INFO: iter: 118320/144360  CE: 2.9218  
[05/06 00:19:11] SuperNet Training INFO: iter: 118440/144360  CE: 3.1491  
[05/06 00:20:09] SuperNet Training INFO: iter: 118560/144360  CE: 2.6526  
[05/06 00:21:09] SuperNet Training INFO: iter: 118680/144360  CE: 2.7776  
[05/06 00:22:08] SuperNet Training INFO: iter: 118800/144360  CE: 2.8472  
[05/06 00:23:08] SuperNet Training INFO: iter: 118920/144360  CE: 2.9909  
[05/06 00:24:08] SuperNet Training INFO: iter: 119040/144360  CE: 2.9834  
[05/06 00:24:35] SuperNet Training INFO: --> epoch:  99/120  avg CE: 3.0068  lr: 0.008841590138754444  
[05/06 00:25:42] SuperNet Training INFO: iter: 119160/144360  CE: 2.8209  
[05/06 00:26:41] SuperNet Training INFO: iter: 119280/144360  CE: 2.8629  
[05/06 00:27:42] SuperNet Training INFO: iter: 119400/144360  CE: 2.9088  
[05/06 00:28:39] SuperNet Training INFO: iter: 119520/144360  CE: 2.9502  
[05/06 00:29:38] SuperNet Training INFO: iter: 119640/144360  CE: 2.9869  
[05/06 00:30:38] SuperNet Training INFO: iter: 119760/144360  CE: 2.9811  
[05/06 00:31:38] SuperNet Training INFO: iter: 119880/144360  CE: 2.9194  
[05/06 00:32:38] SuperNet Training INFO: iter: 120000/144360  CE: 3.1292  
[05/06 00:33:38] SuperNet Training INFO: iter: 120120/144360  CE: 3.1067  
[05/06 00:34:37] SuperNet Training INFO: iter: 120240/144360  CE: 3.0265  
[05/06 00:35:06] SuperNet Training INFO: --> epoch: 100/120  avg CE: 3.0004  lr: 0.00803847577293368  
[05/06 00:36:10] SuperNet Training INFO: iter: 120360/144360  CE: 2.9773  
[05/06 00:37:09] SuperNet Training INFO: iter: 120480/144360  CE: 2.7849  
[05/06 00:38:07] SuperNet Training INFO: iter: 120600/144360  CE: 3.0063  
[05/06 00:39:05] SuperNet Training INFO: iter: 120720/144360  CE: 3.3762  
[05/06 00:40:03] SuperNet Training INFO: iter: 120840/144360  CE: 3.2948  
[05/06 00:41:02] SuperNet Training INFO: iter: 120960/144360  CE: 2.9182  
[05/06 00:41:59] SuperNet Training INFO: iter: 121080/144360  CE: 3.0400  
[05/06 00:42:57] SuperNet Training INFO: iter: 121200/144360  CE: 2.9216  
[05/06 00:43:54] SuperNet Training INFO: iter: 121320/144360  CE: 3.0798  
[05/06 00:44:53] SuperNet Training INFO: iter: 121440/144360  CE: 2.9347  
[05/06 00:45:22] SuperNet Training INFO: --> epoch: 101/120  avg CE: 3.0001  lr: 0.007270973240282054  
[05/06 00:46:27] SuperNet Training INFO: iter: 121560/144360  CE: 3.0110  
[05/06 00:47:27] SuperNet Training INFO: iter: 121680/144360  CE: 2.9040  
[05/06 00:48:28] SuperNet Training INFO: iter: 121800/144360  CE: 2.9140  
[05/06 00:49:27] SuperNet Training INFO: iter: 121920/144360  CE: 2.7495  
[05/06 00:50:28] SuperNet Training INFO: iter: 122040/144360  CE: 3.2125  
[05/06 00:51:28] SuperNet Training INFO: iter: 122160/144360  CE: 3.3740  
[05/06 00:52:28] SuperNet Training INFO: iter: 122280/144360  CE: 2.8643  
[05/06 00:53:28] SuperNet Training INFO: iter: 122400/144360  CE: 2.9336  
[05/06 00:54:29] SuperNet Training INFO: iter: 122520/144360  CE: 2.8407  
[05/06 00:55:29] SuperNet Training INFO: iter: 122640/144360  CE: 2.8448  
[05/06 00:56:00] SuperNet Training INFO: --> epoch: 102/120  avg CE: 2.9831  lr: 0.006539608548697928  
[05/06 00:57:05] SuperNet Training INFO: iter: 122760/144360  CE: 2.9471  
[05/06 00:58:05] SuperNet Training INFO: iter: 122880/144360  CE: 2.8899  
[05/06 00:59:06] SuperNet Training INFO: iter: 123000/144360  CE: 3.0551  
[05/06 01:00:07] SuperNet Training INFO: iter: 123120/144360  CE: 2.8976  
[05/06 01:01:08] SuperNet Training INFO: iter: 123240/144360  CE: 2.8887  
[05/06 01:02:09] SuperNet Training INFO: iter: 123360/144360  CE: 2.9000  
[05/06 01:03:08] SuperNet Training INFO: iter: 123480/144360  CE: 3.0167  
[05/06 01:04:09] SuperNet Training INFO: iter: 123600/144360  CE: 2.9901  
[05/06 01:05:08] SuperNet Training INFO: iter: 123720/144360  CE: 3.0546  
[05/06 01:06:09] SuperNet Training INFO: iter: 123840/144360  CE: 2.7513  
[05/06 01:06:42] SuperNet Training INFO: --> epoch: 103/120  avg CE: 2.9935  lr: 0.00584488293900834  
[05/06 01:07:44] SuperNet Training INFO: iter: 123960/144360  CE: 2.9530  
[05/06 01:08:44] SuperNet Training INFO: iter: 124080/144360  CE: 3.1888  
[05/06 01:09:43] SuperNet Training INFO: iter: 124200/144360  CE: 3.0414  
[05/06 01:10:43] SuperNet Training INFO: iter: 124320/144360  CE: 3.1170  
[05/06 01:11:41] SuperNet Training INFO: iter: 124440/144360  CE: 2.7986  
[05/06 01:12:41] SuperNet Training INFO: iter: 124560/144360  CE: 2.7880  
[05/06 01:13:40] SuperNet Training INFO: iter: 124680/144360  CE: 3.0317  
[05/06 01:14:38] SuperNet Training INFO: iter: 124800/144360  CE: 2.8274  
[05/06 01:15:38] SuperNet Training INFO: iter: 124920/144360  CE: 3.0982  
[05/06 01:16:37] SuperNet Training INFO: iter: 125040/144360  CE: 3.0079  
[05/06 01:17:12] SuperNet Training INFO: --> epoch: 104/120  avg CE: 2.9716  lr: 0.005187272541443939  
[05/06 01:18:10] SuperNet Training INFO: iter: 125160/144360  CE: 2.9999  
[05/06 01:19:11] SuperNet Training INFO: iter: 125280/144360  CE: 3.1288  
[05/06 01:20:12] SuperNet Training INFO: iter: 125400/144360  CE: 3.0220  
[05/06 01:21:11] SuperNet Training INFO: iter: 125520/144360  CE: 2.9531  
[05/06 01:22:11] SuperNet Training INFO: iter: 125640/144360  CE: 3.2889  
[05/06 01:23:12] SuperNet Training INFO: iter: 125760/144360  CE: 2.9548  
[05/06 01:24:11] SuperNet Training INFO: iter: 125880/144360  CE: 3.0367  
[05/06 01:25:09] SuperNet Training INFO: iter: 126000/144360  CE: 2.6847  
[05/06 01:26:08] SuperNet Training INFO: iter: 126120/144360  CE: 2.7022  
[05/06 01:27:08] SuperNet Training INFO: iter: 126240/144360  CE: 2.9336  
[05/06 01:27:43] SuperNet Training INFO: --> epoch: 105/120  avg CE: 2.9665  lr: 0.00456722804932279  
[05/06 01:28:41] SuperNet Training INFO: iter: 126360/144360  CE: 2.7159  
[05/06 01:29:41] SuperNet Training INFO: iter: 126480/144360  CE: 2.8285  
[05/06 01:30:40] SuperNet Training INFO: iter: 126600/144360  CE: 2.9490  
[05/06 01:31:40] SuperNet Training INFO: iter: 126720/144360  CE: 3.0152  
[05/06 01:32:39] SuperNet Training INFO: iter: 126840/144360  CE: 3.0554  
[05/06 01:33:38] SuperNet Training INFO: iter: 126960/144360  CE: 2.9757  
[05/06 01:34:38] SuperNet Training INFO: iter: 127080/144360  CE: 3.0479  
[05/06 01:35:37] SuperNet Training INFO: iter: 127200/144360  CE: 2.8932  
[05/06 01:36:37] SuperNet Training INFO: iter: 127320/144360  CE: 2.9568  
[05/06 01:37:37] SuperNet Training INFO: iter: 127440/144360  CE: 2.6519  
[05/06 01:38:14] SuperNet Training INFO: --> epoch: 106/120  avg CE: 2.9608  lr: 0.003985174410167894  
[05/06 01:39:11] SuperNet Training INFO: iter: 127560/144360  CE: 2.8578  
[05/06 01:40:12] SuperNet Training INFO: iter: 127680/144360  CE: 2.9278  
[05/06 01:41:11] SuperNet Training INFO: iter: 127800/144360  CE: 3.0322  
[05/06 01:42:11] SuperNet Training INFO: iter: 127920/144360  CE: 3.2806  
[05/06 01:43:11] SuperNet Training INFO: iter: 128040/144360  CE: 2.7405  
[05/06 01:44:12] SuperNet Training INFO: iter: 128160/144360  CE: 3.1692  
[05/06 01:45:13] SuperNet Training INFO: iter: 128280/144360  CE: 2.8257  
[05/06 01:46:13] SuperNet Training INFO: iter: 128400/144360  CE: 2.9226  
[05/06 01:47:13] SuperNet Training INFO: iter: 128520/144360  CE: 2.7589  
[05/06 01:48:13] SuperNet Training INFO: iter: 128640/144360  CE: 2.9644  
[05/06 01:48:52] SuperNet Training INFO: --> epoch: 107/120  avg CE: 2.9518  lr: 0.003441510534469298  
[05/06 01:49:48] SuperNet Training INFO: iter: 128760/144360  CE: 2.9480  
[05/06 01:50:48] SuperNet Training INFO: iter: 128880/144360  CE: 2.7369  
[05/06 01:51:48] SuperNet Training INFO: iter: 129000/144360  CE: 3.0630  
[05/06 01:52:47] SuperNet Training INFO: iter: 129120/144360  CE: 2.8721  
[05/06 01:53:48] SuperNet Training INFO: iter: 129240/144360  CE: 3.1139  
[05/06 01:54:47] SuperNet Training INFO: iter: 129360/144360  CE: 2.7910  
[05/06 01:55:47] SuperNet Training INFO: iter: 129480/144360  CE: 2.9565  
[05/06 01:56:47] SuperNet Training INFO: iter: 129600/144360  CE: 3.1483  
[05/06 01:57:45] SuperNet Training INFO: iter: 129720/144360  CE: 3.0436  
[05/06 01:58:44] SuperNet Training INFO: iter: 129840/144360  CE: 3.1130  
[05/06 01:59:25] SuperNet Training INFO: --> epoch: 108/120  avg CE: 2.9504  lr: 0.002936609022290792  
[05/06 02:00:18] SuperNet Training INFO: iter: 129960/144360  CE: 2.8130  
[05/06 02:01:18] SuperNet Training INFO: iter: 130080/144360  CE: 2.9688  
[05/06 02:02:20] SuperNet Training INFO: iter: 130200/144360  CE: 3.0982  
[05/06 02:03:20] SuperNet Training INFO: iter: 130320/144360  CE: 3.0221  
[05/06 02:04:20] SuperNet Training INFO: iter: 130440/144360  CE: 3.0182  
[05/06 02:05:21] SuperNet Training INFO: iter: 130560/144360  CE: 2.8991  
[05/06 02:06:21] SuperNet Training INFO: iter: 130680/144360  CE: 3.2106  
[05/06 02:07:20] SuperNet Training INFO: iter: 130800/144360  CE: 2.8966  
[05/06 02:08:19] SuperNet Training INFO: iter: 130920/144360  CE: 3.0282  
[05/06 02:09:17] SuperNet Training INFO: iter: 131040/144360  CE: 2.8118  
[05/06 02:09:58] SuperNet Training INFO: --> epoch: 109/120  avg CE: 2.9418  lr: 0.0024708159079084185  
[05/06 02:10:49] SuperNet Training INFO: iter: 131160/144360  CE: 2.9571  
[05/06 02:11:48] SuperNet Training INFO: iter: 131280/144360  CE: 3.0694  
[05/06 02:12:47] SuperNet Training INFO: iter: 131400/144360  CE: 2.7805  
[05/06 02:13:46] SuperNet Training INFO: iter: 131520/144360  CE: 2.8053  
[05/06 02:14:45] SuperNet Training INFO: iter: 131640/144360  CE: 3.1129  
[05/06 02:15:45] SuperNet Training INFO: iter: 131760/144360  CE: 2.9746  
[05/06 02:16:43] SuperNet Training INFO: iter: 131880/144360  CE: 3.1301  
[05/06 02:17:41] SuperNet Training INFO: iter: 132000/144360  CE: 3.0842  
[05/06 02:18:41] SuperNet Training INFO: iter: 132120/144360  CE: 3.0491  
[05/06 02:19:39] SuperNet Training INFO: iter: 132240/144360  CE: 3.1928  
[05/06 02:20:22] SuperNet Training INFO: --> epoch: 110/120  avg CE: 2.9458  lr: 0.0020444504226559065  
[05/06 02:21:12] SuperNet Training INFO: iter: 132360/144360  CE: 3.1119  
[05/06 02:22:12] SuperNet Training INFO: iter: 132480/144360  CE: 3.4306  
[05/06 02:23:13] SuperNet Training INFO: iter: 132600/144360  CE: 3.0732  
[05/06 02:24:13] SuperNet Training INFO: iter: 132720/144360  CE: 2.7710  
[05/06 02:25:14] SuperNet Training INFO: iter: 132840/144360  CE: 3.1488  
[05/06 02:26:12] SuperNet Training INFO: iter: 132960/144360  CE: 2.8757  
[05/06 02:27:11] SuperNet Training INFO: iter: 133080/144360  CE: 2.9576  
[05/06 02:28:10] SuperNet Training INFO: iter: 133200/144360  CE: 2.9042  
[05/06 02:29:09] SuperNet Training INFO: iter: 133320/144360  CE: 3.1158  
[05/06 02:30:08] SuperNet Training INFO: iter: 133440/144360  CE: 3.0358  
[05/06 02:30:52] SuperNet Training INFO: --> epoch: 111/120  avg CE: 2.9381  lr: 0.0016578047761394107  
[05/06 02:31:40] SuperNet Training INFO: iter: 133560/144360  CE: 2.9584  
[05/06 02:32:40] SuperNet Training INFO: iter: 133680/144360  CE: 3.0045  
[05/06 02:33:38] SuperNet Training INFO: iter: 133800/144360  CE: 3.0451  
[05/06 02:34:38] SuperNet Training INFO: iter: 133920/144360  CE: 3.3542  
[05/06 02:35:37] SuperNet Training INFO: iter: 134040/144360  CE: 2.7755  
[05/06 02:36:36] SuperNet Training INFO: iter: 134160/144360  CE: 3.0576  
[05/06 02:37:34] SuperNet Training INFO: iter: 134280/144360  CE: 3.0898  
[05/06 02:38:33] SuperNet Training INFO: iter: 134400/144360  CE: 2.7941  
[05/06 02:39:31] SuperNet Training INFO: iter: 134520/144360  CE: 2.9331  
[05/06 02:40:29] SuperNet Training INFO: iter: 134640/144360  CE: 3.0196  
[05/06 02:41:16] SuperNet Training INFO: --> epoch: 112/120  avg CE: 2.9464  lr: 0.0013111439559716617  
[05/06 02:42:03] SuperNet Training INFO: iter: 134760/144360  CE: 2.9480  
[05/06 02:43:03] SuperNet Training INFO: iter: 134880/144360  CE: 3.0023  
[05/06 02:44:03] SuperNet Training INFO: iter: 135000/144360  CE: 2.8529  
[05/06 02:45:03] SuperNet Training INFO: iter: 135120/144360  CE: 2.9528  
[05/06 02:46:02] SuperNet Training INFO: iter: 135240/144360  CE: 2.9769  
[05/06 02:47:03] SuperNet Training INFO: iter: 135360/144360  CE: 3.0163  
[05/06 02:48:02] SuperNet Training INFO: iter: 135480/144360  CE: 2.9393  
[05/06 02:49:02] SuperNet Training INFO: iter: 135600/144360  CE: 2.9838  
[05/06 02:50:03] SuperNet Training INFO: iter: 135720/144360  CE: 2.7286  
[05/06 02:51:03] SuperNet Training INFO: iter: 135840/144360  CE: 2.6989  
[05/06 02:51:53] SuperNet Training INFO: --> epoch: 113/120  avg CE: 2.9345  lr: 0.0010047055461627253  
[05/06 02:52:40] SuperNet Training INFO: iter: 135960/144360  CE: 2.9534  
[05/06 02:53:40] SuperNet Training INFO: iter: 136080/144360  CE: 2.9885  
[05/06 02:54:40] SuperNet Training INFO: iter: 136200/144360  CE: 2.6986  
[05/06 02:55:42] SuperNet Training INFO: iter: 136320/144360  CE: 3.1040  
[05/06 02:56:42] SuperNet Training INFO: iter: 136440/144360  CE: 2.8668  
[05/06 02:57:43] SuperNet Training INFO: iter: 136560/144360  CE: 3.0953  
[05/06 02:58:42] SuperNet Training INFO: iter: 136680/144360  CE: 2.8681  
[05/06 02:59:41] SuperNet Training INFO: iter: 136800/144360  CE: 2.7316  
[05/06 03:00:39] SuperNet Training INFO: iter: 136920/144360  CE: 2.9489  
[05/06 03:01:38] SuperNet Training INFO: iter: 137040/144360  CE: 3.0877  
[05/06 03:02:27] SuperNet Training INFO: --> epoch: 114/120  avg CE: 2.9379  lr: 0.000738699564291742  
[05/06 03:03:12] SuperNet Training INFO: iter: 137160/144360  CE: 2.8597  
[05/06 03:04:11] SuperNet Training INFO: iter: 137280/144360  CE: 2.6813  
[05/06 03:05:10] SuperNet Training INFO: iter: 137400/144360  CE: 2.7175  
[05/06 03:06:10] SuperNet Training INFO: iter: 137520/144360  CE: 2.7890  
[05/06 03:07:10] SuperNet Training INFO: iter: 137640/144360  CE: 2.7390  
[05/06 03:08:11] SuperNet Training INFO: iter: 137760/144360  CE: 2.9301  
[05/06 03:09:12] SuperNet Training INFO: iter: 137880/144360  CE: 2.7726  
[05/06 03:10:11] SuperNet Training INFO: iter: 138000/144360  CE: 2.9369  
[05/06 03:11:09] SuperNet Training INFO: iter: 138120/144360  CE: 2.9529  
[05/06 03:12:08] SuperNet Training INFO: iter: 138240/144360  CE: 2.8162  
[05/06 03:12:58] SuperNet Training INFO: --> epoch: 115/120  avg CE: 2.9299  lr: 0.0005133083175713779  
[05/06 03:13:42] SuperNet Training INFO: iter: 138360/144360  CE: 2.9709  
[05/06 03:14:42] SuperNet Training INFO: iter: 138480/144360  CE: 3.1273  
[05/06 03:15:44] SuperNet Training INFO: iter: 138600/144360  CE: 2.8804  
[05/06 03:16:44] SuperNet Training INFO: iter: 138720/144360  CE: 3.0195  
[05/06 03:17:44] SuperNet Training INFO: iter: 138840/144360  CE: 2.8318  
[05/06 03:18:46] SuperNet Training INFO: iter: 138960/144360  CE: 3.0996  
[05/06 03:19:46] SuperNet Training INFO: iter: 139080/144360  CE: 3.0581  
[05/06 03:20:45] SuperNet Training INFO: iter: 139200/144360  CE: 2.9015  
[05/06 03:21:45] SuperNet Training INFO: iter: 139320/144360  CE: 2.7760  
[05/06 03:22:45] SuperNet Training INFO: iter: 139440/144360  CE: 2.9524  
[05/06 03:23:38] SuperNet Training INFO: --> epoch: 116/120  avg CE: 2.9219  lr: 0.00032868627790359544  
[05/06 03:24:19] SuperNet Training INFO: iter: 139560/144360  CE: 2.8371  
[05/06 03:25:20] SuperNet Training INFO: iter: 139680/144360  CE: 2.7386  
[05/06 03:26:21] SuperNet Training INFO: iter: 139800/144360  CE: 2.6570  
[05/06 03:27:20] SuperNet Training INFO: iter: 139920/144360  CE: 2.8627  
[05/06 03:28:20] SuperNet Training INFO: iter: 140040/144360  CE: 2.9536  
[05/06 03:29:21] SuperNet Training INFO: iter: 140160/144360  CE: 2.9007  
[05/06 03:30:21] SuperNet Training INFO: iter: 140280/144360  CE: 2.8617  
[05/06 03:31:22] SuperNet Training INFO: iter: 140400/144360  CE: 2.8387  
[05/06 03:32:22] SuperNet Training INFO: iter: 140520/144360  CE: 2.8173  
[05/06 03:33:22] SuperNet Training INFO: iter: 140640/144360  CE: 3.0307  
[05/06 03:34:17] SuperNet Training INFO: --> epoch: 117/120  avg CE: 2.9280  lr: 0.00018495997601232129  
[05/06 03:34:57] SuperNet Training INFO: iter: 140760/144360  CE: 2.8988  
[05/06 03:35:56] SuperNet Training INFO: iter: 140880/144360  CE: 3.1276  
[05/06 03:36:56] SuperNet Training INFO: iter: 141000/144360  CE: 3.1022  
[05/06 03:37:56] SuperNet Training INFO: iter: 141120/144360  CE: 2.9944  
[05/06 03:38:56] SuperNet Training INFO: iter: 141240/144360  CE: 3.4138  
[05/06 03:39:56] SuperNet Training INFO: iter: 141360/144360  CE: 2.9913  
[05/06 03:40:58] SuperNet Training INFO: iter: 141480/144360  CE: 2.8271  
[05/06 03:41:58] SuperNet Training INFO: iter: 141600/144360  CE: 2.8310  
[05/06 03:42:57] SuperNet Training INFO: iter: 141720/144360  CE: 2.7590  
[05/06 03:43:56] SuperNet Training INFO: iter: 141840/144360  CE: 2.8429  
[05/06 03:44:52] SuperNet Training INFO: --> epoch: 118/120  avg CE: 2.9251  lr: 8.222791472556962e-05  
[05/06 03:45:30] SuperNet Training INFO: iter: 141960/144360  CE: 2.8100  
[05/06 03:46:29] SuperNet Training INFO: iter: 142080/144360  CE: 2.8597  
[05/06 03:47:29] SuperNet Training INFO: iter: 142200/144360  CE: 3.1165  
[05/06 03:48:30] SuperNet Training INFO: iter: 142320/144360  CE: 3.1837  
[05/06 03:49:28] SuperNet Training INFO: iter: 142440/144360  CE: 2.8003  
[05/06 03:50:29] SuperNet Training INFO: iter: 142560/144360  CE: 2.8013  
[05/06 03:51:29] SuperNet Training INFO: iter: 142680/144360  CE: 3.0421  
[05/06 03:52:29] SuperNet Training INFO: iter: 142800/144360  CE: 2.9899  
[05/06 03:53:28] SuperNet Training INFO: iter: 142920/144360  CE: 2.8775  
[05/06 03:54:28] SuperNet Training INFO: iter: 143040/144360  CE: 2.7498  
[05/06 03:55:26] SuperNet Training INFO: --> epoch: 119/120  avg CE: 2.9250  lr: 2.0560501466564365e-05  
[05/06 03:56:04] SuperNet Training INFO: iter: 143160/144360  CE: 2.6982  
[05/06 03:57:05] SuperNet Training INFO: iter: 143280/144360  CE: 2.6186  
[05/06 03:58:06] SuperNet Training INFO: iter: 143400/144360  CE: 2.5478  
[05/06 03:59:07] SuperNet Training INFO: iter: 143520/144360  CE: 2.5781  
[05/06 04:00:08] SuperNet Training INFO: iter: 143640/144360  CE: 3.0132  
[05/06 04:01:11] SuperNet Training INFO: iter: 143760/144360  CE: 2.9203  
[05/06 04:02:11] SuperNet Training INFO: iter: 143880/144360  CE: 3.1170  
[05/06 04:03:11] SuperNet Training INFO: iter: 144000/144360  CE: 3.1868  
[05/06 04:04:13] SuperNet Training INFO: iter: 144120/144360  CE: 2.8616  
[05/06 04:05:12] SuperNet Training INFO: iter: 144240/144360  CE: 2.9989  
[05/06 04:06:11] SuperNet Training INFO: iter: 144360/144360  CE: 3.1178  
[05/06 04:06:11] SuperNet Training INFO: --> epoch: 120/120  avg CE: 2.9221  lr: 0.0  
[05/06 04:06:11] SuperNet Training INFO: --> END mobile0-tbs238-seed-0
[05/06 04:06:11] SuperNet Training INFO: {0: 72180, 1: 72180}
[05/06 04:06:17] SuperNet Training INFO: ELAPSED TIME: 75893.5(s) = 21(h) 04(m)
