[04/02 14:51:59] SuperNet Training INFO: tag                 : gss-mobile0bn
[04/02 14:51:59] SuperNet Training INFO: seed                : 0
[04/02 14:51:59] SuperNet Training INFO: thresholds          : [58]
[04/02 14:51:59] SuperNet Training INFO: data_path           : ../../../dataset/ILSVRC2012
[04/02 14:51:59] SuperNet Training INFO: save_path           : ./SuperNet
[04/02 14:51:59] SuperNet Training INFO: search_space        : greedy
[04/02 14:51:59] SuperNet Training INFO: valid_size          : 50000
[04/02 14:51:59] SuperNet Training INFO: num_gpus            : 8
[04/02 14:51:59] SuperNet Training INFO: workers             : 4
[04/02 14:51:59] SuperNet Training INFO: interval_ep_eval    : 8
[04/02 14:51:59] SuperNet Training INFO: train_batch_size    : 1024
[04/02 14:51:59] SuperNet Training INFO: test_batch_size     : 256
[04/02 14:51:59] SuperNet Training INFO: max_epoch           : 120
[04/02 14:51:59] SuperNet Training INFO: learning_rate       : 0.12
[04/02 14:51:59] SuperNet Training INFO: momentum            : 0.9
[04/02 14:51:59] SuperNet Training INFO: weight_decay        : 4e-05
[04/02 14:51:59] SuperNet Training INFO: nesterov            : True
[04/02 14:51:59] SuperNet Training INFO: lr_schedule_type    : cosine
[04/02 14:51:59] SuperNet Training INFO: label_smooth        : 0.1
[04/02 14:51:59] SuperNet Training INFO: rank                : 0
[04/02 14:51:59] SuperNet Training INFO: gpu                 : 0
[04/02 14:51:59] SuperNet Training INFO: save_name           : gss-mobile0bn-seed-0
[04/02 14:51:59] SuperNet Training INFO: log_path            : ./SuperNet/logs/gss-mobile0bn-seed-0.txt
[04/02 14:51:59] SuperNet Training INFO: ckpt_path           : ./SuperNet/checkpoint/gss-mobile0bn-seed-0.pt
[04/02 14:51:59] SuperNet Training INFO: dist_url            : tcp://127.0.0.1:23456
[04/02 14:51:59] SuperNet Training INFO: world_size          : 8
[04/02 14:51:59] SuperNet Training INFO: distributed         : True
[04/02 14:51:59] SuperNet Training INFO: ['3x3_MBConv3', '3x3_MBConv6', '5x5_MBConv3', '5x5_MBConv6', '7x7_MBConv3', '7x7_MBConv6', '3x3_MBConv3_SE', '3x3_MBConv6_SE', '5x5_MBConv3_SE', '5x5_MBConv6_SE', '7x7_MBConv3_SE', '7x7_MBConv6_SE', 'Identity']
[04/02 14:52:50] SuperNet Training INFO: SuperNet(
  (first_conv): Sequential(
    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    (2): ReLU6(inplace=True)
  )
  (first_block): InvertedResidual(
    (depth_conv): Sequential(
      (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
      (2): ReLU6(inplace=True)
    )
    (point_linear): Sequential(
      (0): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    )
  )
  (blocks): ModuleList(
    (0): ModuleList(
      (0): InvertedResidual(
        (inverted_bottleneck): Sequential(
          (0): Conv2d(16, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (depth_conv): Sequential(
          (0): Conv2d(48, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=48, bias=False)
          (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (point_linear): Sequential(
          (0): Conv2d(48, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
      )
      (1): InvertedResidual(
        (inverted_bottleneck): Sequential(
          (0): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (depth_conv): Sequential(
          (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)
          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (point_linear): Sequential(
          (0): Conv2d(96, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
      )
      (2): InvertedResidual(
        (inverted_bottleneck): Sequential(
          (0): Conv2d(16, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (depth_conv): Sequential(
          (0): Conv2d(48, 48, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=48, bias=False)
          (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (point_linear): Sequential(
          (0): Conv2d(48, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
      )
      (3): InvertedResidual(
        (inverted_bottleneck): Sequential(
          (0): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (depth_conv): Sequential(
          (0): Conv2d(96, 96, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=96, bias=False)
          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (point_linear): Sequential(
          (0): Conv2d(96, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
      )
      (4): InvertedResidual(
        (inverted_bottleneck): Sequential(
          (0): Conv2d(16, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (depth_conv): Sequential(
          (0): Conv2d(48, 48, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), groups=48, bias=False)
          (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (point_linear): Sequential(
          (0): Conv2d(48, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
      )
      (5): InvertedResidual(
        (inverted_bottleneck): Sequential(
          (0): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (depth_conv): Sequential(
          (0): Conv2d(96, 96, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), groups=96, bias=False)
          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (point_linear): Sequential(
          (0): Conv2d(96, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
      )
      (6): InvertedResidual_SE(
        (inverted_bottleneck): Sequential(
          (0): Conv2d(16, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (depth_conv): Sequential(
          (0): Conv2d(48, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=48, bias=False)
          (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (point_linear): Sequential(
          (0): Conv2d(48, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (se): SqueezeExcite(
          (conv_reduce): Conv2d(48, 16, kernel_size=(1, 1), stride=(1, 1))
          (act1): ReLU6(inplace=True)
          (conv_expand): Conv2d(16, 48, kernel_size=(1, 1), stride=(1, 1))
          (gate): Sigmoid()
        )
      )
      (7): InvertedResidual_SE(
        (inverted_bottleneck): Sequential(
          (0): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (depth_conv): Sequential(
          (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)
          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (point_linear): Sequential(
          (0): Conv2d(96, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (se): SqueezeExcite(
          (conv_reduce): Conv2d(96, 16, kernel_size=(1, 1), stride=(1, 1))
          (act1): ReLU6(inplace=True)
          (conv_expand): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1))
          (gate): Sigmoid()
        )
      )
      (8): InvertedResidual_SE(
        (inverted_bottleneck): Sequential(
          (0): Conv2d(16, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (depth_conv): Sequential(
          (0): Conv2d(48, 48, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=48, bias=False)
          (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (point_linear): Sequential(
          (0): Conv2d(48, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (se): SqueezeExcite(
          (conv_reduce): Conv2d(48, 16, kernel_size=(1, 1), stride=(1, 1))
          (act1): ReLU6(inplace=True)
          (conv_expand): Conv2d(16, 48, kernel_size=(1, 1), stride=(1, 1))
          (gate): Sigmoid()
        )
      )
      (9): InvertedResidual_SE(
        (inverted_bottleneck): Sequential(
          (0): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (depth_conv): Sequential(
          (0): Conv2d(96, 96, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=96, bias=False)
          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (point_linear): Sequential(
          (0): Conv2d(96, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (se): SqueezeExcite(
          (conv_reduce): Conv2d(96, 16, kernel_size=(1, 1), stride=(1, 1))
          (act1): ReLU6(inplace=True)
          (conv_expand): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1))
          (gate): Sigmoid()
        )
      )
      (10): InvertedResidual_SE(
        (inverted_bottleneck): Sequential(
          (0): Conv2d(16, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (depth_conv): Sequential(
          (0): Conv2d(48, 48, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), groups=48, bias=False)
          (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (point_linear): Sequential(
          (0): Conv2d(48, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (se): SqueezeExcite(
          (conv_reduce): Conv2d(48, 16, kernel_size=(1, 1), stride=(1, 1))
          (act1): ReLU6(inplace=True)
          (conv_expand): Conv2d(16, 48, kernel_size=(1, 1), stride=(1, 1))
          (gate): Sigmoid()
        )
      )
      (11): InvertedResidual_SE(
        (inverted_bottleneck): Sequential(
          (0): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (depth_conv): Sequential(
          (0): Conv2d(96, 96, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), groups=96, bias=False)
          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (point_linear): Sequential(
          (0): Conv2d(96, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (se): SqueezeExcite(
          (conv_reduce): Conv2d(96, 16, kernel_size=(1, 1), stride=(1, 1))
          (act1): ReLU6(inplace=True)
          (conv_expand): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1))
          (gate): Sigmoid()
        )
      )
    )
    (1): ModuleList(
      (0): InvertedResidual(
        (inverted_bottleneck): Sequential(
          (0): Conv2d(32, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (depth_conv): Sequential(
          (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (point_linear): Sequential(
          (0): Conv2d(96, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
      )
      (1): InvertedResidual(
        (inverted_bottleneck): Sequential(
          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (depth_conv): Sequential(
          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (point_linear): Sequential(
          (0): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
      )
      (2): InvertedResidual(
        (inverted_bottleneck): Sequential(
          (0): Conv2d(32, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (depth_conv): Sequential(
          (0): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96, bias=False)
          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (point_linear): Sequential(
          (0): Conv2d(96, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
      )
      (3): InvertedResidual(
        (inverted_bottleneck): Sequential(
          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (depth_conv): Sequential(
          (0): Conv2d(192, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=192, bias=False)
          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (point_linear): Sequential(
          (0): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
      )
      (4): InvertedResidual(
        (inverted_bottleneck): Sequential(
          (0): Conv2d(32, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (depth_conv): Sequential(
          (0): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96, bias=False)
          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (point_linear): Sequential(
          (0): Conv2d(96, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
      )
      (5): InvertedResidual(
        (inverted_bottleneck): Sequential(
          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (depth_conv): Sequential(
          (0): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192, bias=False)
          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (point_linear): Sequential(
          (0): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
      )
      (6): InvertedResidual_SE(
        (inverted_bottleneck): Sequential(
          (0): Conv2d(32, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (depth_conv): Sequential(
          (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (point_linear): Sequential(
          (0): Conv2d(96, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (se): SqueezeExcite(
          (conv_reduce): Conv2d(96, 32, kernel_size=(1, 1), stride=(1, 1))
          (act1): ReLU6(inplace=True)
          (conv_expand): Conv2d(32, 96, kernel_size=(1, 1), stride=(1, 1))
          (gate): Sigmoid()
        )
      )
      (7): InvertedResidual_SE(
        (inverted_bottleneck): Sequential(
          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (depth_conv): Sequential(
          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (point_linear): Sequential(
          (0): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (se): SqueezeExcite(
          (conv_reduce): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1))
          (act1): ReLU6(inplace=True)
          (conv_expand): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1))
          (gate): Sigmoid()
        )
      )
      (8): InvertedResidual_SE(
        (inverted_bottleneck): Sequential(
          (0): Conv2d(32, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (depth_conv): Sequential(
          (0): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96, bias=False)
          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (point_linear): Sequential(
          (0): Conv2d(96, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (se): SqueezeExcite(
          (conv_reduce): Conv2d(96, 32, kernel_size=(1, 1), stride=(1, 1))
          (act1): ReLU6(inplace=True)
          (conv_expand): Conv2d(32, 96, kernel_size=(1, 1), stride=(1, 1))
          (gate): Sigmoid()
        )
      )
      (9): InvertedResidual_SE(
        (inverted_bottleneck): Sequential(
          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (depth_conv): Sequential(
          (0): Conv2d(192, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=192, bias=False)
          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (point_linear): Sequential(
          (0): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (se): SqueezeExcite(
          (conv_reduce): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1))
          (act1): ReLU6(inplace=True)
          (conv_expand): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1))
          (gate): Sigmoid()
        )
      )
      (10): InvertedResidual_SE(
        (inverted_bottleneck): Sequential(
          (0): Conv2d(32, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (depth_conv): Sequential(
          (0): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96, bias=False)
          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (point_linear): Sequential(
          (0): Conv2d(96, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (se): SqueezeExcite(
          (conv_reduce): Conv2d(96, 32, kernel_size=(1, 1), stride=(1, 1))
          (act1): ReLU6(inplace=True)
          (conv_expand): Conv2d(32, 96, kernel_size=(1, 1), stride=(1, 1))
          (gate): Sigmoid()
        )
      )
      (11): InvertedResidual_SE(
        (inverted_bottleneck): Sequential(
          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (depth_conv): Sequential(
          (0): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192, bias=False)
          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (point_linear): Sequential(
          (0): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (se): SqueezeExcite(
          (conv_reduce): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1))
          (act1): ReLU6(inplace=True)
          (conv_expand): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1))
          (gate): Sigmoid()
        )
      )
      (12): Identity()
    )
    (2): ModuleList(
      (0): InvertedResidual(
        (inverted_bottleneck): Sequential(
          (0): Conv2d(32, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (depth_conv): Sequential(
          (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (point_linear): Sequential(
          (0): Conv2d(96, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
      )
      (1): InvertedResidual(
        (inverted_bottleneck): Sequential(
          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (depth_conv): Sequential(
          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (point_linear): Sequential(
          (0): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
      )
      (2): InvertedResidual(
        (inverted_bottleneck): Sequential(
          (0): Conv2d(32, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (depth_conv): Sequential(
          (0): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96, bias=False)
          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (point_linear): Sequential(
          (0): Conv2d(96, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
      )
      (3): InvertedResidual(
        (inverted_bottleneck): Sequential(
          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (depth_conv): Sequential(
          (0): Conv2d(192, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=192, bias=False)
          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (point_linear): Sequential(
          (0): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
      )
      (4): InvertedResidual(
        (inverted_bottleneck): Sequential(
          (0): Conv2d(32, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (depth_conv): Sequential(
          (0): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96, bias=False)
          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (point_linear): Sequential(
          (0): Conv2d(96, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
      )
      (5): InvertedResidual(
        (inverted_bottleneck): Sequential(
          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (depth_conv): Sequential(
          (0): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192, bias=False)
          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (point_linear): Sequential(
          (0): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
      )
      (6): InvertedResidual_SE(
        (inverted_bottleneck): Sequential(
          (0): Conv2d(32, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (depth_conv): Sequential(
          (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (point_linear): Sequential(
          (0): Conv2d(96, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (se): SqueezeExcite(
          (conv_reduce): Conv2d(96, 32, kernel_size=(1, 1), stride=(1, 1))
          (act1): ReLU6(inplace=True)
          (conv_expand): Conv2d(32, 96, kernel_size=(1, 1), stride=(1, 1))
          (gate): Sigmoid()
        )
      )
      (7): InvertedResidual_SE(
        (inverted_bottleneck): Sequential(
          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (depth_conv): Sequential(
          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (point_linear): Sequential(
          (0): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (se): SqueezeExcite(
          (conv_reduce): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1))
          (act1): ReLU6(inplace=True)
          (conv_expand): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1))
          (gate): Sigmoid()
        )
      )
      (8): InvertedResidual_SE(
        (inverted_bottleneck): Sequential(
          (0): Conv2d(32, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (depth_conv): Sequential(
          (0): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96, bias=False)
          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (point_linear): Sequential(
          (0): Conv2d(96, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (se): SqueezeExcite(
          (conv_reduce): Conv2d(96, 32, kernel_size=(1, 1), stride=(1, 1))
          (act1): ReLU6(inplace=True)
          (conv_expand): Conv2d(32, 96, kernel_size=(1, 1), stride=(1, 1))
          (gate): Sigmoid()
        )
      )
      (9): InvertedResidual_SE(
        (inverted_bottleneck): Sequential(
          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (depth_conv): Sequential(
          (0): Conv2d(192, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=192, bias=False)
          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (point_linear): Sequential(
          (0): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (se): SqueezeExcite(
          (conv_reduce): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1))
          (act1): ReLU6(inplace=True)
          (conv_expand): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1))
          (gate): Sigmoid()
        )
      )
      (10): InvertedResidual_SE(
        (inverted_bottleneck): Sequential(
          (0): Conv2d(32, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (depth_conv): Sequential(
          (0): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96, bias=False)
          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (point_linear): Sequential(
          (0): Conv2d(96, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (se): SqueezeExcite(
          (conv_reduce): Conv2d(96, 32, kernel_size=(1, 1), stride=(1, 1))
          (act1): ReLU6(inplace=True)
          (conv_expand): Conv2d(32, 96, kernel_size=(1, 1), stride=(1, 1))
          (gate): Sigmoid()
        )
      )
      (11): InvertedResidual_SE(
        (inverted_bottleneck): Sequential(
          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (depth_conv): Sequential(
          (0): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192, bias=False)
          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (point_linear): Sequential(
          (0): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (se): SqueezeExcite(
          (conv_reduce): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1))
          (act1): ReLU6(inplace=True)
          (conv_expand): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1))
          (gate): Sigmoid()
        )
      )
      (12): Identity()
    )
    (3): ModuleList(
      (0): InvertedResidual(
        (inverted_bottleneck): Sequential(
          (0): Conv2d(32, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (depth_conv): Sequential(
          (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (point_linear): Sequential(
          (0): Conv2d(96, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
      )
      (1): InvertedResidual(
        (inverted_bottleneck): Sequential(
          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (depth_conv): Sequential(
          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (point_linear): Sequential(
          (0): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
      )
      (2): InvertedResidual(
        (inverted_bottleneck): Sequential(
          (0): Conv2d(32, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (depth_conv): Sequential(
          (0): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96, bias=False)
          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (point_linear): Sequential(
          (0): Conv2d(96, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
      )
      (3): InvertedResidual(
        (inverted_bottleneck): Sequential(
          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (depth_conv): Sequential(
          (0): Conv2d(192, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=192, bias=False)
          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (point_linear): Sequential(
          (0): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
      )
      (4): InvertedResidual(
        (inverted_bottleneck): Sequential(
          (0): Conv2d(32, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (depth_conv): Sequential(
          (0): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96, bias=False)
          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (point_linear): Sequential(
          (0): Conv2d(96, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
      )
      (5): InvertedResidual(
        (inverted_bottleneck): Sequential(
          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (depth_conv): Sequential(
          (0): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192, bias=False)
          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (point_linear): Sequential(
          (0): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
      )
      (6): InvertedResidual_SE(
        (inverted_bottleneck): Sequential(
          (0): Conv2d(32, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (depth_conv): Sequential(
          (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (point_linear): Sequential(
          (0): Conv2d(96, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (se): SqueezeExcite(
          (conv_reduce): Conv2d(96, 32, kernel_size=(1, 1), stride=(1, 1))
          (act1): ReLU6(inplace=True)
          (conv_expand): Conv2d(32, 96, kernel_size=(1, 1), stride=(1, 1))
          (gate): Sigmoid()
        )
      )
      (7): InvertedResidual_SE(
        (inverted_bottleneck): Sequential(
          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (depth_conv): Sequential(
          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (point_linear): Sequential(
          (0): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (se): SqueezeExcite(
          (conv_reduce): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1))
          (act1): ReLU6(inplace=True)
          (conv_expand): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1))
          (gate): Sigmoid()
        )
      )
      (8): InvertedResidual_SE(
        (inverted_bottleneck): Sequential(
          (0): Conv2d(32, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (depth_conv): Sequential(
          (0): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96, bias=False)
          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (point_linear): Sequential(
          (0): Conv2d(96, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (se): SqueezeExcite(
          (conv_reduce): Conv2d(96, 32, kernel_size=(1, 1), stride=(1, 1))
          (act1): ReLU6(inplace=True)
          (conv_expand): Conv2d(32, 96, kernel_size=(1, 1), stride=(1, 1))
          (gate): Sigmoid()
        )
      )
      (9): InvertedResidual_SE(
        (inverted_bottleneck): Sequential(
          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (depth_conv): Sequential(
          (0): Conv2d(192, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=192, bias=False)
          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (point_linear): Sequential(
          (0): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (se): SqueezeExcite(
          (conv_reduce): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1))
          (act1): ReLU6(inplace=True)
          (conv_expand): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1))
          (gate): Sigmoid()
        )
      )
      (10): InvertedResidual_SE(
        (inverted_bottleneck): Sequential(
          (0): Conv2d(32, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (depth_conv): Sequential(
          (0): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96, bias=False)
          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (point_linear): Sequential(
          (0): Conv2d(96, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (se): SqueezeExcite(
          (conv_reduce): Conv2d(96, 32, kernel_size=(1, 1), stride=(1, 1))
          (act1): ReLU6(inplace=True)
          (conv_expand): Conv2d(32, 96, kernel_size=(1, 1), stride=(1, 1))
          (gate): Sigmoid()
        )
      )
      (11): InvertedResidual_SE(
        (inverted_bottleneck): Sequential(
          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (depth_conv): Sequential(
          (0): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192, bias=False)
          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (point_linear): Sequential(
          (0): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (se): SqueezeExcite(
          (conv_reduce): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1))
          (act1): ReLU6(inplace=True)
          (conv_expand): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1))
          (gate): Sigmoid()
        )
      )
      (12): Identity()
    )
    (4): ModuleList(
      (0): InvertedResidual(
        (inverted_bottleneck): Sequential(
          (0): Conv2d(32, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (depth_conv): Sequential(
          (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)
          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (point_linear): Sequential(
          (0): Conv2d(96, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
      )
      (1): InvertedResidual(
        (inverted_bottleneck): Sequential(
          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (depth_conv): Sequential(
          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)
          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (point_linear): Sequential(
          (0): Conv2d(192, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
      )
      (2): InvertedResidual(
        (inverted_bottleneck): Sequential(
          (0): Conv2d(32, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (depth_conv): Sequential(
          (0): Conv2d(96, 96, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=96, bias=False)
          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (point_linear): Sequential(
          (0): Conv2d(96, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
      )
      (3): InvertedResidual(
        (inverted_bottleneck): Sequential(
          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (depth_conv): Sequential(
          (0): Conv2d(192, 192, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=192, bias=False)
          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (point_linear): Sequential(
          (0): Conv2d(192, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
      )
      (4): InvertedResidual(
        (inverted_bottleneck): Sequential(
          (0): Conv2d(32, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (depth_conv): Sequential(
          (0): Conv2d(96, 96, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), groups=96, bias=False)
          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (point_linear): Sequential(
          (0): Conv2d(96, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
      )
      (5): InvertedResidual(
        (inverted_bottleneck): Sequential(
          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (depth_conv): Sequential(
          (0): Conv2d(192, 192, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), groups=192, bias=False)
          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (point_linear): Sequential(
          (0): Conv2d(192, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
      )
      (6): InvertedResidual_SE(
        (inverted_bottleneck): Sequential(
          (0): Conv2d(32, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (depth_conv): Sequential(
          (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)
          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (point_linear): Sequential(
          (0): Conv2d(96, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (se): SqueezeExcite(
          (conv_reduce): Conv2d(96, 32, kernel_size=(1, 1), stride=(1, 1))
          (act1): ReLU6(inplace=True)
          (conv_expand): Conv2d(32, 96, kernel_size=(1, 1), stride=(1, 1))
          (gate): Sigmoid()
        )
      )
      (7): InvertedResidual_SE(
        (inverted_bottleneck): Sequential(
          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (depth_conv): Sequential(
          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)
          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (point_linear): Sequential(
          (0): Conv2d(192, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (se): SqueezeExcite(
          (conv_reduce): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1))
          (act1): ReLU6(inplace=True)
          (conv_expand): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1))
          (gate): Sigmoid()
        )
      )
      (8): InvertedResidual_SE(
        (inverted_bottleneck): Sequential(
          (0): Conv2d(32, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (depth_conv): Sequential(
          (0): Conv2d(96, 96, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=96, bias=False)
          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (point_linear): Sequential(
          (0): Conv2d(96, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (se): SqueezeExcite(
          (conv_reduce): Conv2d(96, 32, kernel_size=(1, 1), stride=(1, 1))
          (act1): ReLU6(inplace=True)
          (conv_expand): Conv2d(32, 96, kernel_size=(1, 1), stride=(1, 1))
          (gate): Sigmoid()
        )
      )
      (9): InvertedResidual_SE(
        (inverted_bottleneck): Sequential(
          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (depth_conv): Sequential(
          (0): Conv2d(192, 192, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=192, bias=False)
          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (point_linear): Sequential(
          (0): Conv2d(192, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (se): SqueezeExcite(
          (conv_reduce): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1))
          (act1): ReLU6(inplace=True)
          (conv_expand): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1))
          (gate): Sigmoid()
        )
      )
      (10): InvertedResidual_SE(
        (inverted_bottleneck): Sequential(
          (0): Conv2d(32, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (depth_conv): Sequential(
          (0): Conv2d(96, 96, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), groups=96, bias=False)
          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (point_linear): Sequential(
          (0): Conv2d(96, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (se): SqueezeExcite(
          (conv_reduce): Conv2d(96, 32, kernel_size=(1, 1), stride=(1, 1))
          (act1): ReLU6(inplace=True)
          (conv_expand): Conv2d(32, 96, kernel_size=(1, 1), stride=(1, 1))
          (gate): Sigmoid()
        )
      )
      (11): InvertedResidual_SE(
        (inverted_bottleneck): Sequential(
          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (depth_conv): Sequential(
          (0): Conv2d(192, 192, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), groups=192, bias=False)
          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (point_linear): Sequential(
          (0): Conv2d(192, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (se): SqueezeExcite(
          (conv_reduce): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1))
          (act1): ReLU6(inplace=True)
          (conv_expand): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1))
          (gate): Sigmoid()
        )
      )
    )
    (5): ModuleList(
      (0): InvertedResidual(
        (inverted_bottleneck): Sequential(
          (0): Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (depth_conv): Sequential(
          (0): Conv2d(120, 120, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=120, bias=False)
          (1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (point_linear): Sequential(
          (0): Conv2d(120, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
      )
      (1): InvertedResidual(
        (inverted_bottleneck): Sequential(
          (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (depth_conv): Sequential(
          (0): Conv2d(240, 240, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=240, bias=False)
          (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (point_linear): Sequential(
          (0): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
      )
      (2): InvertedResidual(
        (inverted_bottleneck): Sequential(
          (0): Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (depth_conv): Sequential(
          (0): Conv2d(120, 120, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=120, bias=False)
          (1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (point_linear): Sequential(
          (0): Conv2d(120, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
      )
      (3): InvertedResidual(
        (inverted_bottleneck): Sequential(
          (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (depth_conv): Sequential(
          (0): Conv2d(240, 240, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=240, bias=False)
          (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (point_linear): Sequential(
          (0): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
      )
      (4): InvertedResidual(
        (inverted_bottleneck): Sequential(
          (0): Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (depth_conv): Sequential(
          (0): Conv2d(120, 120, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=120, bias=False)
          (1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (point_linear): Sequential(
          (0): Conv2d(120, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
      )
      (5): InvertedResidual(
        (inverted_bottleneck): Sequential(
          (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (depth_conv): Sequential(
          (0): Conv2d(240, 240, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=240, bias=False)
          (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (point_linear): Sequential(
          (0): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
      )
      (6): InvertedResidual_SE(
        (inverted_bottleneck): Sequential(
          (0): Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (depth_conv): Sequential(
          (0): Conv2d(120, 120, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=120, bias=False)
          (1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (point_linear): Sequential(
          (0): Conv2d(120, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (se): SqueezeExcite(
          (conv_reduce): Conv2d(120, 40, kernel_size=(1, 1), stride=(1, 1))
          (act1): ReLU6(inplace=True)
          (conv_expand): Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1))
          (gate): Sigmoid()
        )
      )
      (7): InvertedResidual_SE(
        (inverted_bottleneck): Sequential(
          (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (depth_conv): Sequential(
          (0): Conv2d(240, 240, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=240, bias=False)
          (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (point_linear): Sequential(
          (0): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (se): SqueezeExcite(
          (conv_reduce): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1))
          (act1): ReLU6(inplace=True)
          (conv_expand): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1))
          (gate): Sigmoid()
        )
      )
      (8): InvertedResidual_SE(
        (inverted_bottleneck): Sequential(
          (0): Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (depth_conv): Sequential(
          (0): Conv2d(120, 120, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=120, bias=False)
          (1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (point_linear): Sequential(
          (0): Conv2d(120, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (se): SqueezeExcite(
          (conv_reduce): Conv2d(120, 40, kernel_size=(1, 1), stride=(1, 1))
          (act1): ReLU6(inplace=True)
          (conv_expand): Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1))
          (gate): Sigmoid()
        )
      )
      (9): InvertedResidual_SE(
        (inverted_bottleneck): Sequential(
          (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (depth_conv): Sequential(
          (0): Conv2d(240, 240, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=240, bias=False)
          (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (point_linear): Sequential(
          (0): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (se): SqueezeExcite(
          (conv_reduce): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1))
          (act1): ReLU6(inplace=True)
          (conv_expand): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1))
          (gate): Sigmoid()
        )
      )
      (10): InvertedResidual_SE(
        (inverted_bottleneck): Sequential(
          (0): Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (depth_conv): Sequential(
          (0): Conv2d(120, 120, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=120, bias=False)
          (1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (point_linear): Sequential(
          (0): Conv2d(120, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (se): SqueezeExcite(
          (conv_reduce): Conv2d(120, 40, kernel_size=(1, 1), stride=(1, 1))
          (act1): ReLU6(inplace=True)
          (conv_expand): Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1))
          (gate): Sigmoid()
        )
      )
      (11): InvertedResidual_SE(
        (inverted_bottleneck): Sequential(
          (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (depth_conv): Sequential(
          (0): Conv2d(240, 240, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=240, bias=False)
          (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (point_linear): Sequential(
          (0): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (se): SqueezeExcite(
          (conv_reduce): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1))
          (act1): ReLU6(inplace=True)
          (conv_expand): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1))
          (gate): Sigmoid()
        )
      )
      (12): Identity()
    )
    (6): ModuleList(
      (0): InvertedResidual(
        (inverted_bottleneck): Sequential(
          (0): Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (depth_conv): Sequential(
          (0): Conv2d(120, 120, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=120, bias=False)
          (1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (point_linear): Sequential(
          (0): Conv2d(120, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
      )
      (1): InvertedResidual(
        (inverted_bottleneck): Sequential(
          (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (depth_conv): Sequential(
          (0): Conv2d(240, 240, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=240, bias=False)
          (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (point_linear): Sequential(
          (0): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
      )
      (2): InvertedResidual(
        (inverted_bottleneck): Sequential(
          (0): Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (depth_conv): Sequential(
          (0): Conv2d(120, 120, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=120, bias=False)
          (1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (point_linear): Sequential(
          (0): Conv2d(120, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
      )
      (3): InvertedResidual(
        (inverted_bottleneck): Sequential(
          (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (depth_conv): Sequential(
          (0): Conv2d(240, 240, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=240, bias=False)
          (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (point_linear): Sequential(
          (0): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
      )
      (4): InvertedResidual(
        (inverted_bottleneck): Sequential(
          (0): Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (depth_conv): Sequential(
          (0): Conv2d(120, 120, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=120, bias=False)
          (1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (point_linear): Sequential(
          (0): Conv2d(120, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
      )
      (5): InvertedResidual(
        (inverted_bottleneck): Sequential(
          (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (depth_conv): Sequential(
          (0): Conv2d(240, 240, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=240, bias=False)
          (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (point_linear): Sequential(
          (0): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
      )
      (6): InvertedResidual_SE(
        (inverted_bottleneck): Sequential(
          (0): Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (depth_conv): Sequential(
          (0): Conv2d(120, 120, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=120, bias=False)
          (1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (point_linear): Sequential(
          (0): Conv2d(120, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (se): SqueezeExcite(
          (conv_reduce): Conv2d(120, 40, kernel_size=(1, 1), stride=(1, 1))
          (act1): ReLU6(inplace=True)
          (conv_expand): Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1))
          (gate): Sigmoid()
        )
      )
      (7): InvertedResidual_SE(
        (inverted_bottleneck): Sequential(
          (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (depth_conv): Sequential(
          (0): Conv2d(240, 240, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=240, bias=False)
          (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (point_linear): Sequential(
          (0): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (se): SqueezeExcite(
          (conv_reduce): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1))
          (act1): ReLU6(inplace=True)
          (conv_expand): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1))
          (gate): Sigmoid()
        )
      )
      (8): InvertedResidual_SE(
        (inverted_bottleneck): Sequential(
          (0): Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (depth_conv): Sequential(
          (0): Conv2d(120, 120, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=120, bias=False)
          (1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (point_linear): Sequential(
          (0): Conv2d(120, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (se): SqueezeExcite(
          (conv_reduce): Conv2d(120, 40, kernel_size=(1, 1), stride=(1, 1))
          (act1): ReLU6(inplace=True)
          (conv_expand): Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1))
          (gate): Sigmoid()
        )
      )
      (9): InvertedResidual_SE(
        (inverted_bottleneck): Sequential(
          (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (depth_conv): Sequential(
          (0): Conv2d(240, 240, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=240, bias=False)
          (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (point_linear): Sequential(
          (0): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (se): SqueezeExcite(
          (conv_reduce): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1))
          (act1): ReLU6(inplace=True)
          (conv_expand): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1))
          (gate): Sigmoid()
        )
      )
      (10): InvertedResidual_SE(
        (inverted_bottleneck): Sequential(
          (0): Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (depth_conv): Sequential(
          (0): Conv2d(120, 120, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=120, bias=False)
          (1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (point_linear): Sequential(
          (0): Conv2d(120, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (se): SqueezeExcite(
          (conv_reduce): Conv2d(120, 40, kernel_size=(1, 1), stride=(1, 1))
          (act1): ReLU6(inplace=True)
          (conv_expand): Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1))
          (gate): Sigmoid()
        )
      )
      (11): InvertedResidual_SE(
        (inverted_bottleneck): Sequential(
          (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (depth_conv): Sequential(
          (0): Conv2d(240, 240, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=240, bias=False)
          (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (point_linear): Sequential(
          (0): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (se): SqueezeExcite(
          (conv_reduce): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1))
          (act1): ReLU6(inplace=True)
          (conv_expand): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1))
          (gate): Sigmoid()
        )
      )
      (12): Identity()
    )
    (7): ModuleList(
      (0): InvertedResidual(
        (inverted_bottleneck): Sequential(
          (0): Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (depth_conv): Sequential(
          (0): Conv2d(120, 120, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=120, bias=False)
          (1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (point_linear): Sequential(
          (0): Conv2d(120, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
      )
      (1): InvertedResidual(
        (inverted_bottleneck): Sequential(
          (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (depth_conv): Sequential(
          (0): Conv2d(240, 240, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=240, bias=False)
          (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (point_linear): Sequential(
          (0): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
      )
      (2): InvertedResidual(
        (inverted_bottleneck): Sequential(
          (0): Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (depth_conv): Sequential(
          (0): Conv2d(120, 120, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=120, bias=False)
          (1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (point_linear): Sequential(
          (0): Conv2d(120, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
      )
      (3): InvertedResidual(
        (inverted_bottleneck): Sequential(
          (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (depth_conv): Sequential(
          (0): Conv2d(240, 240, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=240, bias=False)
          (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (point_linear): Sequential(
          (0): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
      )
      (4): InvertedResidual(
        (inverted_bottleneck): Sequential(
          (0): Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (depth_conv): Sequential(
          (0): Conv2d(120, 120, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=120, bias=False)
          (1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (point_linear): Sequential(
          (0): Conv2d(120, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
      )
      (5): InvertedResidual(
        (inverted_bottleneck): Sequential(
          (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (depth_conv): Sequential(
          (0): Conv2d(240, 240, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=240, bias=False)
          (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (point_linear): Sequential(
          (0): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
      )
      (6): InvertedResidual_SE(
        (inverted_bottleneck): Sequential(
          (0): Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (depth_conv): Sequential(
          (0): Conv2d(120, 120, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=120, bias=False)
          (1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (point_linear): Sequential(
          (0): Conv2d(120, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (se): SqueezeExcite(
          (conv_reduce): Conv2d(120, 40, kernel_size=(1, 1), stride=(1, 1))
          (act1): ReLU6(inplace=True)
          (conv_expand): Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1))
          (gate): Sigmoid()
        )
      )
      (7): InvertedResidual_SE(
        (inverted_bottleneck): Sequential(
          (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (depth_conv): Sequential(
          (0): Conv2d(240, 240, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=240, bias=False)
          (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (point_linear): Sequential(
          (0): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (se): SqueezeExcite(
          (conv_reduce): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1))
          (act1): ReLU6(inplace=True)
          (conv_expand): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1))
          (gate): Sigmoid()
        )
      )
      (8): InvertedResidual_SE(
        (inverted_bottleneck): Sequential(
          (0): Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (depth_conv): Sequential(
          (0): Conv2d(120, 120, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=120, bias=False)
          (1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (point_linear): Sequential(
          (0): Conv2d(120, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (se): SqueezeExcite(
          (conv_reduce): Conv2d(120, 40, kernel_size=(1, 1), stride=(1, 1))
          (act1): ReLU6(inplace=True)
          (conv_expand): Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1))
          (gate): Sigmoid()
        )
      )
      (9): InvertedResidual_SE(
        (inverted_bottleneck): Sequential(
          (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (depth_conv): Sequential(
          (0): Conv2d(240, 240, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=240, bias=False)
          (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (point_linear): Sequential(
          (0): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (se): SqueezeExcite(
          (conv_reduce): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1))
          (act1): ReLU6(inplace=True)
          (conv_expand): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1))
          (gate): Sigmoid()
        )
      )
      (10): InvertedResidual_SE(
        (inverted_bottleneck): Sequential(
          (0): Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (depth_conv): Sequential(
          (0): Conv2d(120, 120, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=120, bias=False)
          (1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (point_linear): Sequential(
          (0): Conv2d(120, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (se): SqueezeExcite(
          (conv_reduce): Conv2d(120, 40, kernel_size=(1, 1), stride=(1, 1))
          (act1): ReLU6(inplace=True)
          (conv_expand): Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1))
          (gate): Sigmoid()
        )
      )
      (11): InvertedResidual_SE(
        (inverted_bottleneck): Sequential(
          (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (depth_conv): Sequential(
          (0): Conv2d(240, 240, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=240, bias=False)
          (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (point_linear): Sequential(
          (0): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (se): SqueezeExcite(
          (conv_reduce): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1))
          (act1): ReLU6(inplace=True)
          (conv_expand): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1))
          (gate): Sigmoid()
        )
      )
      (12): Identity()
    )
    (8): ModuleList(
      (0): InvertedResidual(
        (inverted_bottleneck): Sequential(
          (0): Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (depth_conv): Sequential(
          (0): Conv2d(120, 120, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=120, bias=False)
          (1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (point_linear): Sequential(
          (0): Conv2d(120, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
      )
      (1): InvertedResidual(
        (inverted_bottleneck): Sequential(
          (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (depth_conv): Sequential(
          (0): Conv2d(240, 240, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=240, bias=False)
          (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (point_linear): Sequential(
          (0): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
      )
      (2): InvertedResidual(
        (inverted_bottleneck): Sequential(
          (0): Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (depth_conv): Sequential(
          (0): Conv2d(120, 120, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=120, bias=False)
          (1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (point_linear): Sequential(
          (0): Conv2d(120, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
      )
      (3): InvertedResidual(
        (inverted_bottleneck): Sequential(
          (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (depth_conv): Sequential(
          (0): Conv2d(240, 240, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=240, bias=False)
          (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (point_linear): Sequential(
          (0): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
      )
      (4): InvertedResidual(
        (inverted_bottleneck): Sequential(
          (0): Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (depth_conv): Sequential(
          (0): Conv2d(120, 120, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), groups=120, bias=False)
          (1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (point_linear): Sequential(
          (0): Conv2d(120, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
      )
      (5): InvertedResidual(
        (inverted_bottleneck): Sequential(
          (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (depth_conv): Sequential(
          (0): Conv2d(240, 240, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), groups=240, bias=False)
          (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (point_linear): Sequential(
          (0): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
      )
      (6): InvertedResidual_SE(
        (inverted_bottleneck): Sequential(
          (0): Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (depth_conv): Sequential(
          (0): Conv2d(120, 120, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=120, bias=False)
          (1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (point_linear): Sequential(
          (0): Conv2d(120, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (se): SqueezeExcite(
          (conv_reduce): Conv2d(120, 40, kernel_size=(1, 1), stride=(1, 1))
          (act1): ReLU6(inplace=True)
          (conv_expand): Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1))
          (gate): Sigmoid()
        )
      )
      (7): InvertedResidual_SE(
        (inverted_bottleneck): Sequential(
          (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (depth_conv): Sequential(
          (0): Conv2d(240, 240, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=240, bias=False)
          (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (point_linear): Sequential(
          (0): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (se): SqueezeExcite(
          (conv_reduce): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1))
          (act1): ReLU6(inplace=True)
          (conv_expand): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1))
          (gate): Sigmoid()
        )
      )
      (8): InvertedResidual_SE(
        (inverted_bottleneck): Sequential(
          (0): Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (depth_conv): Sequential(
          (0): Conv2d(120, 120, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=120, bias=False)
          (1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (point_linear): Sequential(
          (0): Conv2d(120, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (se): SqueezeExcite(
          (conv_reduce): Conv2d(120, 40, kernel_size=(1, 1), stride=(1, 1))
          (act1): ReLU6(inplace=True)
          (conv_expand): Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1))
          (gate): Sigmoid()
        )
      )
      (9): InvertedResidual_SE(
        (inverted_bottleneck): Sequential(
          (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (depth_conv): Sequential(
          (0): Conv2d(240, 240, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=240, bias=False)
          (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (point_linear): Sequential(
          (0): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (se): SqueezeExcite(
          (conv_reduce): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1))
          (act1): ReLU6(inplace=True)
          (conv_expand): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1))
          (gate): Sigmoid()
        )
      )
      (10): InvertedResidual_SE(
        (inverted_bottleneck): Sequential(
          (0): Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (depth_conv): Sequential(
          (0): Conv2d(120, 120, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), groups=120, bias=False)
          (1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (point_linear): Sequential(
          (0): Conv2d(120, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (se): SqueezeExcite(
          (conv_reduce): Conv2d(120, 40, kernel_size=(1, 1), stride=(1, 1))
          (act1): ReLU6(inplace=True)
          (conv_expand): Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1))
          (gate): Sigmoid()
        )
      )
      (11): InvertedResidual_SE(
        (inverted_bottleneck): Sequential(
          (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (depth_conv): Sequential(
          (0): Conv2d(240, 240, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), groups=240, bias=False)
          (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (point_linear): Sequential(
          (0): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (se): SqueezeExcite(
          (conv_reduce): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1))
          (act1): ReLU6(inplace=True)
          (conv_expand): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1))
          (gate): Sigmoid()
        )
      )
    )
    (9): ModuleList(
      (0): InvertedResidual(
        (inverted_bottleneck): Sequential(
          (0): Conv2d(80, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (depth_conv): Sequential(
          (0): Conv2d(240, 240, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=240, bias=False)
          (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (point_linear): Sequential(
          (0): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
      )
      (1): InvertedResidual(
        (inverted_bottleneck): Sequential(
          (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (depth_conv): Sequential(
          (0): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)
          (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (point_linear): Sequential(
          (0): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
      )
      (2): InvertedResidual(
        (inverted_bottleneck): Sequential(
          (0): Conv2d(80, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (depth_conv): Sequential(
          (0): Conv2d(240, 240, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=240, bias=False)
          (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (point_linear): Sequential(
          (0): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
      )
      (3): InvertedResidual(
        (inverted_bottleneck): Sequential(
          (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (depth_conv): Sequential(
          (0): Conv2d(480, 480, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=480, bias=False)
          (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (point_linear): Sequential(
          (0): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
      )
      (4): InvertedResidual(
        (inverted_bottleneck): Sequential(
          (0): Conv2d(80, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (depth_conv): Sequential(
          (0): Conv2d(240, 240, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=240, bias=False)
          (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (point_linear): Sequential(
          (0): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
      )
      (5): InvertedResidual(
        (inverted_bottleneck): Sequential(
          (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (depth_conv): Sequential(
          (0): Conv2d(480, 480, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=480, bias=False)
          (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (point_linear): Sequential(
          (0): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
      )
      (6): InvertedResidual_SE(
        (inverted_bottleneck): Sequential(
          (0): Conv2d(80, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (depth_conv): Sequential(
          (0): Conv2d(240, 240, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=240, bias=False)
          (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (point_linear): Sequential(
          (0): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (se): SqueezeExcite(
          (conv_reduce): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1))
          (act1): ReLU6(inplace=True)
          (conv_expand): Conv2d(80, 240, kernel_size=(1, 1), stride=(1, 1))
          (gate): Sigmoid()
        )
      )
      (7): InvertedResidual_SE(
        (inverted_bottleneck): Sequential(
          (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (depth_conv): Sequential(
          (0): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)
          (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (point_linear): Sequential(
          (0): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (se): SqueezeExcite(
          (conv_reduce): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1))
          (act1): ReLU6(inplace=True)
          (conv_expand): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1))
          (gate): Sigmoid()
        )
      )
      (8): InvertedResidual_SE(
        (inverted_bottleneck): Sequential(
          (0): Conv2d(80, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (depth_conv): Sequential(
          (0): Conv2d(240, 240, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=240, bias=False)
          (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (point_linear): Sequential(
          (0): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (se): SqueezeExcite(
          (conv_reduce): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1))
          (act1): ReLU6(inplace=True)
          (conv_expand): Conv2d(80, 240, kernel_size=(1, 1), stride=(1, 1))
          (gate): Sigmoid()
        )
      )
      (9): InvertedResidual_SE(
        (inverted_bottleneck): Sequential(
          (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (depth_conv): Sequential(
          (0): Conv2d(480, 480, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=480, bias=False)
          (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (point_linear): Sequential(
          (0): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (se): SqueezeExcite(
          (conv_reduce): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1))
          (act1): ReLU6(inplace=True)
          (conv_expand): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1))
          (gate): Sigmoid()
        )
      )
      (10): InvertedResidual_SE(
        (inverted_bottleneck): Sequential(
          (0): Conv2d(80, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (depth_conv): Sequential(
          (0): Conv2d(240, 240, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=240, bias=False)
          (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (point_linear): Sequential(
          (0): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (se): SqueezeExcite(
          (conv_reduce): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1))
          (act1): ReLU6(inplace=True)
          (conv_expand): Conv2d(80, 240, kernel_size=(1, 1), stride=(1, 1))
          (gate): Sigmoid()
        )
      )
      (11): InvertedResidual_SE(
        (inverted_bottleneck): Sequential(
          (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (depth_conv): Sequential(
          (0): Conv2d(480, 480, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=480, bias=False)
          (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (point_linear): Sequential(
          (0): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (se): SqueezeExcite(
          (conv_reduce): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1))
          (act1): ReLU6(inplace=True)
          (conv_expand): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1))
          (gate): Sigmoid()
        )
      )
      (12): Identity()
    )
    (10): ModuleList(
      (0): InvertedResidual(
        (inverted_bottleneck): Sequential(
          (0): Conv2d(80, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (depth_conv): Sequential(
          (0): Conv2d(240, 240, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=240, bias=False)
          (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (point_linear): Sequential(
          (0): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
      )
      (1): InvertedResidual(
        (inverted_bottleneck): Sequential(
          (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (depth_conv): Sequential(
          (0): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)
          (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (point_linear): Sequential(
          (0): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
      )
      (2): InvertedResidual(
        (inverted_bottleneck): Sequential(
          (0): Conv2d(80, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (depth_conv): Sequential(
          (0): Conv2d(240, 240, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=240, bias=False)
          (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (point_linear): Sequential(
          (0): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
      )
      (3): InvertedResidual(
        (inverted_bottleneck): Sequential(
          (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (depth_conv): Sequential(
          (0): Conv2d(480, 480, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=480, bias=False)
          (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (point_linear): Sequential(
          (0): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
      )
      (4): InvertedResidual(
        (inverted_bottleneck): Sequential(
          (0): Conv2d(80, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (depth_conv): Sequential(
          (0): Conv2d(240, 240, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=240, bias=False)
          (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (point_linear): Sequential(
          (0): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
      )
      (5): InvertedResidual(
        (inverted_bottleneck): Sequential(
          (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (depth_conv): Sequential(
          (0): Conv2d(480, 480, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=480, bias=False)
          (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (point_linear): Sequential(
          (0): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
      )
      (6): InvertedResidual_SE(
        (inverted_bottleneck): Sequential(
          (0): Conv2d(80, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (depth_conv): Sequential(
          (0): Conv2d(240, 240, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=240, bias=False)
          (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (point_linear): Sequential(
          (0): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (se): SqueezeExcite(
          (conv_reduce): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1))
          (act1): ReLU6(inplace=True)
          (conv_expand): Conv2d(80, 240, kernel_size=(1, 1), stride=(1, 1))
          (gate): Sigmoid()
        )
      )
      (7): InvertedResidual_SE(
        (inverted_bottleneck): Sequential(
          (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (depth_conv): Sequential(
          (0): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)
          (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (point_linear): Sequential(
          (0): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (se): SqueezeExcite(
          (conv_reduce): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1))
          (act1): ReLU6(inplace=True)
          (conv_expand): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1))
          (gate): Sigmoid()
        )
      )
      (8): InvertedResidual_SE(
        (inverted_bottleneck): Sequential(
          (0): Conv2d(80, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (depth_conv): Sequential(
          (0): Conv2d(240, 240, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=240, bias=False)
          (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (point_linear): Sequential(
          (0): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (se): SqueezeExcite(
          (conv_reduce): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1))
          (act1): ReLU6(inplace=True)
          (conv_expand): Conv2d(80, 240, kernel_size=(1, 1), stride=(1, 1))
          (gate): Sigmoid()
        )
      )
      (9): InvertedResidual_SE(
        (inverted_bottleneck): Sequential(
          (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (depth_conv): Sequential(
          (0): Conv2d(480, 480, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=480, bias=False)
          (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (point_linear): Sequential(
          (0): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (se): SqueezeExcite(
          (conv_reduce): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1))
          (act1): ReLU6(inplace=True)
          (conv_expand): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1))
          (gate): Sigmoid()
        )
      )
      (10): InvertedResidual_SE(
        (inverted_bottleneck): Sequential(
          (0): Conv2d(80, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (depth_conv): Sequential(
          (0): Conv2d(240, 240, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=240, bias=False)
          (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (point_linear): Sequential(
          (0): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (se): SqueezeExcite(
          (conv_reduce): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1))
          (act1): ReLU6(inplace=True)
          (conv_expand): Conv2d(80, 240, kernel_size=(1, 1), stride=(1, 1))
          (gate): Sigmoid()
        )
      )
      (11): InvertedResidual_SE(
        (inverted_bottleneck): Sequential(
          (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (depth_conv): Sequential(
          (0): Conv2d(480, 480, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=480, bias=False)
          (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (point_linear): Sequential(
          (0): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (se): SqueezeExcite(
          (conv_reduce): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1))
          (act1): ReLU6(inplace=True)
          (conv_expand): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1))
          (gate): Sigmoid()
        )
      )
      (12): Identity()
    )
    (11): ModuleList(
      (0): InvertedResidual(
        (inverted_bottleneck): Sequential(
          (0): Conv2d(80, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (depth_conv): Sequential(
          (0): Conv2d(240, 240, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=240, bias=False)
          (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (point_linear): Sequential(
          (0): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
      )
      (1): InvertedResidual(
        (inverted_bottleneck): Sequential(
          (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (depth_conv): Sequential(
          (0): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)
          (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (point_linear): Sequential(
          (0): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
      )
      (2): InvertedResidual(
        (inverted_bottleneck): Sequential(
          (0): Conv2d(80, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (depth_conv): Sequential(
          (0): Conv2d(240, 240, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=240, bias=False)
          (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (point_linear): Sequential(
          (0): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
      )
      (3): InvertedResidual(
        (inverted_bottleneck): Sequential(
          (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (depth_conv): Sequential(
          (0): Conv2d(480, 480, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=480, bias=False)
          (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (point_linear): Sequential(
          (0): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
      )
      (4): InvertedResidual(
        (inverted_bottleneck): Sequential(
          (0): Conv2d(80, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (depth_conv): Sequential(
          (0): Conv2d(240, 240, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=240, bias=False)
          (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (point_linear): Sequential(
          (0): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
      )
      (5): InvertedResidual(
        (inverted_bottleneck): Sequential(
          (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (depth_conv): Sequential(
          (0): Conv2d(480, 480, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=480, bias=False)
          (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (point_linear): Sequential(
          (0): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
      )
      (6): InvertedResidual_SE(
        (inverted_bottleneck): Sequential(
          (0): Conv2d(80, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (depth_conv): Sequential(
          (0): Conv2d(240, 240, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=240, bias=False)
          (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (point_linear): Sequential(
          (0): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (se): SqueezeExcite(
          (conv_reduce): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1))
          (act1): ReLU6(inplace=True)
          (conv_expand): Conv2d(80, 240, kernel_size=(1, 1), stride=(1, 1))
          (gate): Sigmoid()
        )
      )
      (7): InvertedResidual_SE(
        (inverted_bottleneck): Sequential(
          (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (depth_conv): Sequential(
          (0): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)
          (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (point_linear): Sequential(
          (0): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (se): SqueezeExcite(
          (conv_reduce): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1))
          (act1): ReLU6(inplace=True)
          (conv_expand): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1))
          (gate): Sigmoid()
        )
      )
      (8): InvertedResidual_SE(
        (inverted_bottleneck): Sequential(
          (0): Conv2d(80, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (depth_conv): Sequential(
          (0): Conv2d(240, 240, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=240, bias=False)
          (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (point_linear): Sequential(
          (0): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (se): SqueezeExcite(
          (conv_reduce): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1))
          (act1): ReLU6(inplace=True)
          (conv_expand): Conv2d(80, 240, kernel_size=(1, 1), stride=(1, 1))
          (gate): Sigmoid()
        )
      )
      (9): InvertedResidual_SE(
        (inverted_bottleneck): Sequential(
          (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (depth_conv): Sequential(
          (0): Conv2d(480, 480, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=480, bias=False)
          (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (point_linear): Sequential(
          (0): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (se): SqueezeExcite(
          (conv_reduce): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1))
          (act1): ReLU6(inplace=True)
          (conv_expand): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1))
          (gate): Sigmoid()
        )
      )
      (10): InvertedResidual_SE(
        (inverted_bottleneck): Sequential(
          (0): Conv2d(80, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (depth_conv): Sequential(
          (0): Conv2d(240, 240, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=240, bias=False)
          (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (point_linear): Sequential(
          (0): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (se): SqueezeExcite(
          (conv_reduce): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1))
          (act1): ReLU6(inplace=True)
          (conv_expand): Conv2d(80, 240, kernel_size=(1, 1), stride=(1, 1))
          (gate): Sigmoid()
        )
      )
      (11): InvertedResidual_SE(
        (inverted_bottleneck): Sequential(
          (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (depth_conv): Sequential(
          (0): Conv2d(480, 480, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=480, bias=False)
          (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (point_linear): Sequential(
          (0): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (se): SqueezeExcite(
          (conv_reduce): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1))
          (act1): ReLU6(inplace=True)
          (conv_expand): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1))
          (gate): Sigmoid()
        )
      )
      (12): Identity()
    )
    (12): ModuleList(
      (0): InvertedResidual(
        (inverted_bottleneck): Sequential(
          (0): Conv2d(80, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (depth_conv): Sequential(
          (0): Conv2d(240, 240, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=240, bias=False)
          (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (point_linear): Sequential(
          (0): Conv2d(240, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
      )
      (1): InvertedResidual(
        (inverted_bottleneck): Sequential(
          (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (depth_conv): Sequential(
          (0): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)
          (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (point_linear): Sequential(
          (0): Conv2d(480, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
      )
      (2): InvertedResidual(
        (inverted_bottleneck): Sequential(
          (0): Conv2d(80, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (depth_conv): Sequential(
          (0): Conv2d(240, 240, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=240, bias=False)
          (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (point_linear): Sequential(
          (0): Conv2d(240, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
      )
      (3): InvertedResidual(
        (inverted_bottleneck): Sequential(
          (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (depth_conv): Sequential(
          (0): Conv2d(480, 480, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=480, bias=False)
          (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (point_linear): Sequential(
          (0): Conv2d(480, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
      )
      (4): InvertedResidual(
        (inverted_bottleneck): Sequential(
          (0): Conv2d(80, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (depth_conv): Sequential(
          (0): Conv2d(240, 240, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=240, bias=False)
          (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (point_linear): Sequential(
          (0): Conv2d(240, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
      )
      (5): InvertedResidual(
        (inverted_bottleneck): Sequential(
          (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (depth_conv): Sequential(
          (0): Conv2d(480, 480, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=480, bias=False)
          (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (point_linear): Sequential(
          (0): Conv2d(480, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
      )
      (6): InvertedResidual_SE(
        (inverted_bottleneck): Sequential(
          (0): Conv2d(80, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (depth_conv): Sequential(
          (0): Conv2d(240, 240, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=240, bias=False)
          (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (point_linear): Sequential(
          (0): Conv2d(240, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (se): SqueezeExcite(
          (conv_reduce): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1))
          (act1): ReLU6(inplace=True)
          (conv_expand): Conv2d(80, 240, kernel_size=(1, 1), stride=(1, 1))
          (gate): Sigmoid()
        )
      )
      (7): InvertedResidual_SE(
        (inverted_bottleneck): Sequential(
          (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (depth_conv): Sequential(
          (0): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)
          (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (point_linear): Sequential(
          (0): Conv2d(480, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (se): SqueezeExcite(
          (conv_reduce): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1))
          (act1): ReLU6(inplace=True)
          (conv_expand): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1))
          (gate): Sigmoid()
        )
      )
      (8): InvertedResidual_SE(
        (inverted_bottleneck): Sequential(
          (0): Conv2d(80, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (depth_conv): Sequential(
          (0): Conv2d(240, 240, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=240, bias=False)
          (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (point_linear): Sequential(
          (0): Conv2d(240, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (se): SqueezeExcite(
          (conv_reduce): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1))
          (act1): ReLU6(inplace=True)
          (conv_expand): Conv2d(80, 240, kernel_size=(1, 1), stride=(1, 1))
          (gate): Sigmoid()
        )
      )
      (9): InvertedResidual_SE(
        (inverted_bottleneck): Sequential(
          (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (depth_conv): Sequential(
          (0): Conv2d(480, 480, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=480, bias=False)
          (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (point_linear): Sequential(
          (0): Conv2d(480, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (se): SqueezeExcite(
          (conv_reduce): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1))
          (act1): ReLU6(inplace=True)
          (conv_expand): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1))
          (gate): Sigmoid()
        )
      )
      (10): InvertedResidual_SE(
        (inverted_bottleneck): Sequential(
          (0): Conv2d(80, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (depth_conv): Sequential(
          (0): Conv2d(240, 240, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=240, bias=False)
          (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (point_linear): Sequential(
          (0): Conv2d(240, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (se): SqueezeExcite(
          (conv_reduce): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1))
          (act1): ReLU6(inplace=True)
          (conv_expand): Conv2d(80, 240, kernel_size=(1, 1), stride=(1, 1))
          (gate): Sigmoid()
        )
      )
      (11): InvertedResidual_SE(
        (inverted_bottleneck): Sequential(
          (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (depth_conv): Sequential(
          (0): Conv2d(480, 480, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=480, bias=False)
          (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (point_linear): Sequential(
          (0): Conv2d(480, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (se): SqueezeExcite(
          (conv_reduce): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1))
          (act1): ReLU6(inplace=True)
          (conv_expand): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1))
          (gate): Sigmoid()
        )
      )
    )
    (13): ModuleList(
      (0): InvertedResidual(
        (inverted_bottleneck): Sequential(
          (0): Conv2d(96, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (depth_conv): Sequential(
          (0): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
          (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (point_linear): Sequential(
          (0): Conv2d(288, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
      )
      (1): InvertedResidual(
        (inverted_bottleneck): Sequential(
          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (depth_conv): Sequential(
          (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)
          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (point_linear): Sequential(
          (0): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
      )
      (2): InvertedResidual(
        (inverted_bottleneck): Sequential(
          (0): Conv2d(96, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (depth_conv): Sequential(
          (0): Conv2d(288, 288, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=288, bias=False)
          (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (point_linear): Sequential(
          (0): Conv2d(288, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
      )
      (3): InvertedResidual(
        (inverted_bottleneck): Sequential(
          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (depth_conv): Sequential(
          (0): Conv2d(576, 576, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=576, bias=False)
          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (point_linear): Sequential(
          (0): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
      )
      (4): InvertedResidual(
        (inverted_bottleneck): Sequential(
          (0): Conv2d(96, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (depth_conv): Sequential(
          (0): Conv2d(288, 288, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=288, bias=False)
          (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (point_linear): Sequential(
          (0): Conv2d(288, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
      )
      (5): InvertedResidual(
        (inverted_bottleneck): Sequential(
          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (depth_conv): Sequential(
          (0): Conv2d(576, 576, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=576, bias=False)
          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (point_linear): Sequential(
          (0): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
      )
      (6): InvertedResidual_SE(
        (inverted_bottleneck): Sequential(
          (0): Conv2d(96, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (depth_conv): Sequential(
          (0): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
          (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (point_linear): Sequential(
          (0): Conv2d(288, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (se): SqueezeExcite(
          (conv_reduce): Conv2d(288, 96, kernel_size=(1, 1), stride=(1, 1))
          (act1): ReLU6(inplace=True)
          (conv_expand): Conv2d(96, 288, kernel_size=(1, 1), stride=(1, 1))
          (gate): Sigmoid()
        )
      )
      (7): InvertedResidual_SE(
        (inverted_bottleneck): Sequential(
          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (depth_conv): Sequential(
          (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)
          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (point_linear): Sequential(
          (0): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (se): SqueezeExcite(
          (conv_reduce): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1))
          (act1): ReLU6(inplace=True)
          (conv_expand): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1))
          (gate): Sigmoid()
        )
      )
      (8): InvertedResidual_SE(
        (inverted_bottleneck): Sequential(
          (0): Conv2d(96, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (depth_conv): Sequential(
          (0): Conv2d(288, 288, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=288, bias=False)
          (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (point_linear): Sequential(
          (0): Conv2d(288, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (se): SqueezeExcite(
          (conv_reduce): Conv2d(288, 96, kernel_size=(1, 1), stride=(1, 1))
          (act1): ReLU6(inplace=True)
          (conv_expand): Conv2d(96, 288, kernel_size=(1, 1), stride=(1, 1))
          (gate): Sigmoid()
        )
      )
      (9): InvertedResidual_SE(
        (inverted_bottleneck): Sequential(
          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (depth_conv): Sequential(
          (0): Conv2d(576, 576, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=576, bias=False)
          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (point_linear): Sequential(
          (0): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (se): SqueezeExcite(
          (conv_reduce): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1))
          (act1): ReLU6(inplace=True)
          (conv_expand): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1))
          (gate): Sigmoid()
        )
      )
      (10): InvertedResidual_SE(
        (inverted_bottleneck): Sequential(
          (0): Conv2d(96, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (depth_conv): Sequential(
          (0): Conv2d(288, 288, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=288, bias=False)
          (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (point_linear): Sequential(
          (0): Conv2d(288, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (se): SqueezeExcite(
          (conv_reduce): Conv2d(288, 96, kernel_size=(1, 1), stride=(1, 1))
          (act1): ReLU6(inplace=True)
          (conv_expand): Conv2d(96, 288, kernel_size=(1, 1), stride=(1, 1))
          (gate): Sigmoid()
        )
      )
      (11): InvertedResidual_SE(
        (inverted_bottleneck): Sequential(
          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (depth_conv): Sequential(
          (0): Conv2d(576, 576, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=576, bias=False)
          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (point_linear): Sequential(
          (0): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (se): SqueezeExcite(
          (conv_reduce): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1))
          (act1): ReLU6(inplace=True)
          (conv_expand): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1))
          (gate): Sigmoid()
        )
      )
      (12): Identity()
    )
    (14): ModuleList(
      (0): InvertedResidual(
        (inverted_bottleneck): Sequential(
          (0): Conv2d(96, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (depth_conv): Sequential(
          (0): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
          (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (point_linear): Sequential(
          (0): Conv2d(288, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
      )
      (1): InvertedResidual(
        (inverted_bottleneck): Sequential(
          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (depth_conv): Sequential(
          (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)
          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (point_linear): Sequential(
          (0): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
      )
      (2): InvertedResidual(
        (inverted_bottleneck): Sequential(
          (0): Conv2d(96, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (depth_conv): Sequential(
          (0): Conv2d(288, 288, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=288, bias=False)
          (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (point_linear): Sequential(
          (0): Conv2d(288, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
      )
      (3): InvertedResidual(
        (inverted_bottleneck): Sequential(
          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (depth_conv): Sequential(
          (0): Conv2d(576, 576, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=576, bias=False)
          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (point_linear): Sequential(
          (0): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
      )
      (4): InvertedResidual(
        (inverted_bottleneck): Sequential(
          (0): Conv2d(96, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (depth_conv): Sequential(
          (0): Conv2d(288, 288, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=288, bias=False)
          (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (point_linear): Sequential(
          (0): Conv2d(288, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
      )
      (5): InvertedResidual(
        (inverted_bottleneck): Sequential(
          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (depth_conv): Sequential(
          (0): Conv2d(576, 576, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=576, bias=False)
          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (point_linear): Sequential(
          (0): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
      )
      (6): InvertedResidual_SE(
        (inverted_bottleneck): Sequential(
          (0): Conv2d(96, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (depth_conv): Sequential(
          (0): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
          (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (point_linear): Sequential(
          (0): Conv2d(288, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (se): SqueezeExcite(
          (conv_reduce): Conv2d(288, 96, kernel_size=(1, 1), stride=(1, 1))
          (act1): ReLU6(inplace=True)
          (conv_expand): Conv2d(96, 288, kernel_size=(1, 1), stride=(1, 1))
          (gate): Sigmoid()
        )
      )
      (7): InvertedResidual_SE(
        (inverted_bottleneck): Sequential(
          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (depth_conv): Sequential(
          (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)
          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (point_linear): Sequential(
          (0): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (se): SqueezeExcite(
          (conv_reduce): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1))
          (act1): ReLU6(inplace=True)
          (conv_expand): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1))
          (gate): Sigmoid()
        )
      )
      (8): InvertedResidual_SE(
        (inverted_bottleneck): Sequential(
          (0): Conv2d(96, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (depth_conv): Sequential(
          (0): Conv2d(288, 288, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=288, bias=False)
          (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (point_linear): Sequential(
          (0): Conv2d(288, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (se): SqueezeExcite(
          (conv_reduce): Conv2d(288, 96, kernel_size=(1, 1), stride=(1, 1))
          (act1): ReLU6(inplace=True)
          (conv_expand): Conv2d(96, 288, kernel_size=(1, 1), stride=(1, 1))
          (gate): Sigmoid()
        )
      )
      (9): InvertedResidual_SE(
        (inverted_bottleneck): Sequential(
          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (depth_conv): Sequential(
          (0): Conv2d(576, 576, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=576, bias=False)
          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (point_linear): Sequential(
          (0): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (se): SqueezeExcite(
          (conv_reduce): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1))
          (act1): ReLU6(inplace=True)
          (conv_expand): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1))
          (gate): Sigmoid()
        )
      )
      (10): InvertedResidual_SE(
        (inverted_bottleneck): Sequential(
          (0): Conv2d(96, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (depth_conv): Sequential(
          (0): Conv2d(288, 288, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=288, bias=False)
          (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (point_linear): Sequential(
          (0): Conv2d(288, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (se): SqueezeExcite(
          (conv_reduce): Conv2d(288, 96, kernel_size=(1, 1), stride=(1, 1))
          (act1): ReLU6(inplace=True)
          (conv_expand): Conv2d(96, 288, kernel_size=(1, 1), stride=(1, 1))
          (gate): Sigmoid()
        )
      )
      (11): InvertedResidual_SE(
        (inverted_bottleneck): Sequential(
          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (depth_conv): Sequential(
          (0): Conv2d(576, 576, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=576, bias=False)
          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (point_linear): Sequential(
          (0): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (se): SqueezeExcite(
          (conv_reduce): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1))
          (act1): ReLU6(inplace=True)
          (conv_expand): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1))
          (gate): Sigmoid()
        )
      )
      (12): Identity()
    )
    (15): ModuleList(
      (0): InvertedResidual(
        (inverted_bottleneck): Sequential(
          (0): Conv2d(96, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (depth_conv): Sequential(
          (0): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
          (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (point_linear): Sequential(
          (0): Conv2d(288, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
      )
      (1): InvertedResidual(
        (inverted_bottleneck): Sequential(
          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (depth_conv): Sequential(
          (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)
          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (point_linear): Sequential(
          (0): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
      )
      (2): InvertedResidual(
        (inverted_bottleneck): Sequential(
          (0): Conv2d(96, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (depth_conv): Sequential(
          (0): Conv2d(288, 288, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=288, bias=False)
          (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (point_linear): Sequential(
          (0): Conv2d(288, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
      )
      (3): InvertedResidual(
        (inverted_bottleneck): Sequential(
          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (depth_conv): Sequential(
          (0): Conv2d(576, 576, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=576, bias=False)
          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (point_linear): Sequential(
          (0): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
      )
      (4): InvertedResidual(
        (inverted_bottleneck): Sequential(
          (0): Conv2d(96, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (depth_conv): Sequential(
          (0): Conv2d(288, 288, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=288, bias=False)
          (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (point_linear): Sequential(
          (0): Conv2d(288, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
      )
      (5): InvertedResidual(
        (inverted_bottleneck): Sequential(
          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (depth_conv): Sequential(
          (0): Conv2d(576, 576, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=576, bias=False)
          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (point_linear): Sequential(
          (0): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
      )
      (6): InvertedResidual_SE(
        (inverted_bottleneck): Sequential(
          (0): Conv2d(96, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (depth_conv): Sequential(
          (0): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
          (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (point_linear): Sequential(
          (0): Conv2d(288, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (se): SqueezeExcite(
          (conv_reduce): Conv2d(288, 96, kernel_size=(1, 1), stride=(1, 1))
          (act1): ReLU6(inplace=True)
          (conv_expand): Conv2d(96, 288, kernel_size=(1, 1), stride=(1, 1))
          (gate): Sigmoid()
        )
      )
      (7): InvertedResidual_SE(
        (inverted_bottleneck): Sequential(
          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (depth_conv): Sequential(
          (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)
          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (point_linear): Sequential(
          (0): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (se): SqueezeExcite(
          (conv_reduce): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1))
          (act1): ReLU6(inplace=True)
          (conv_expand): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1))
          (gate): Sigmoid()
        )
      )
      (8): InvertedResidual_SE(
        (inverted_bottleneck): Sequential(
          (0): Conv2d(96, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (depth_conv): Sequential(
          (0): Conv2d(288, 288, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=288, bias=False)
          (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (point_linear): Sequential(
          (0): Conv2d(288, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (se): SqueezeExcite(
          (conv_reduce): Conv2d(288, 96, kernel_size=(1, 1), stride=(1, 1))
          (act1): ReLU6(inplace=True)
          (conv_expand): Conv2d(96, 288, kernel_size=(1, 1), stride=(1, 1))
          (gate): Sigmoid()
        )
      )
      (9): InvertedResidual_SE(
        (inverted_bottleneck): Sequential(
          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (depth_conv): Sequential(
          (0): Conv2d(576, 576, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=576, bias=False)
          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (point_linear): Sequential(
          (0): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (se): SqueezeExcite(
          (conv_reduce): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1))
          (act1): ReLU6(inplace=True)
          (conv_expand): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1))
          (gate): Sigmoid()
        )
      )
      (10): InvertedResidual_SE(
        (inverted_bottleneck): Sequential(
          (0): Conv2d(96, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (depth_conv): Sequential(
          (0): Conv2d(288, 288, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=288, bias=False)
          (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (point_linear): Sequential(
          (0): Conv2d(288, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (se): SqueezeExcite(
          (conv_reduce): Conv2d(288, 96, kernel_size=(1, 1), stride=(1, 1))
          (act1): ReLU6(inplace=True)
          (conv_expand): Conv2d(96, 288, kernel_size=(1, 1), stride=(1, 1))
          (gate): Sigmoid()
        )
      )
      (11): InvertedResidual_SE(
        (inverted_bottleneck): Sequential(
          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (depth_conv): Sequential(
          (0): Conv2d(576, 576, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=576, bias=False)
          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (point_linear): Sequential(
          (0): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (se): SqueezeExcite(
          (conv_reduce): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1))
          (act1): ReLU6(inplace=True)
          (conv_expand): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1))
          (gate): Sigmoid()
        )
      )
      (12): Identity()
    )
    (16): ModuleList(
      (0): InvertedResidual(
        (inverted_bottleneck): Sequential(
          (0): Conv2d(96, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (depth_conv): Sequential(
          (0): Conv2d(288, 288, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=288, bias=False)
          (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (point_linear): Sequential(
          (0): Conv2d(288, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
      )
      (1): InvertedResidual(
        (inverted_bottleneck): Sequential(
          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (depth_conv): Sequential(
          (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=576, bias=False)
          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (point_linear): Sequential(
          (0): Conv2d(576, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
      )
      (2): InvertedResidual(
        (inverted_bottleneck): Sequential(
          (0): Conv2d(96, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (depth_conv): Sequential(
          (0): Conv2d(288, 288, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=288, bias=False)
          (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (point_linear): Sequential(
          (0): Conv2d(288, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
      )
      (3): InvertedResidual(
        (inverted_bottleneck): Sequential(
          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (depth_conv): Sequential(
          (0): Conv2d(576, 576, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=576, bias=False)
          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (point_linear): Sequential(
          (0): Conv2d(576, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
      )
      (4): InvertedResidual(
        (inverted_bottleneck): Sequential(
          (0): Conv2d(96, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (depth_conv): Sequential(
          (0): Conv2d(288, 288, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), groups=288, bias=False)
          (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (point_linear): Sequential(
          (0): Conv2d(288, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
      )
      (5): InvertedResidual(
        (inverted_bottleneck): Sequential(
          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (depth_conv): Sequential(
          (0): Conv2d(576, 576, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), groups=576, bias=False)
          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (point_linear): Sequential(
          (0): Conv2d(576, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
      )
      (6): InvertedResidual_SE(
        (inverted_bottleneck): Sequential(
          (0): Conv2d(96, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (depth_conv): Sequential(
          (0): Conv2d(288, 288, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=288, bias=False)
          (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (point_linear): Sequential(
          (0): Conv2d(288, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (se): SqueezeExcite(
          (conv_reduce): Conv2d(288, 96, kernel_size=(1, 1), stride=(1, 1))
          (act1): ReLU6(inplace=True)
          (conv_expand): Conv2d(96, 288, kernel_size=(1, 1), stride=(1, 1))
          (gate): Sigmoid()
        )
      )
      (7): InvertedResidual_SE(
        (inverted_bottleneck): Sequential(
          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (depth_conv): Sequential(
          (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=576, bias=False)
          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (point_linear): Sequential(
          (0): Conv2d(576, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (se): SqueezeExcite(
          (conv_reduce): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1))
          (act1): ReLU6(inplace=True)
          (conv_expand): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1))
          (gate): Sigmoid()
        )
      )
      (8): InvertedResidual_SE(
        (inverted_bottleneck): Sequential(
          (0): Conv2d(96, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (depth_conv): Sequential(
          (0): Conv2d(288, 288, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=288, bias=False)
          (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (point_linear): Sequential(
          (0): Conv2d(288, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (se): SqueezeExcite(
          (conv_reduce): Conv2d(288, 96, kernel_size=(1, 1), stride=(1, 1))
          (act1): ReLU6(inplace=True)
          (conv_expand): Conv2d(96, 288, kernel_size=(1, 1), stride=(1, 1))
          (gate): Sigmoid()
        )
      )
      (9): InvertedResidual_SE(
        (inverted_bottleneck): Sequential(
          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (depth_conv): Sequential(
          (0): Conv2d(576, 576, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=576, bias=False)
          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (point_linear): Sequential(
          (0): Conv2d(576, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (se): SqueezeExcite(
          (conv_reduce): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1))
          (act1): ReLU6(inplace=True)
          (conv_expand): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1))
          (gate): Sigmoid()
        )
      )
      (10): InvertedResidual_SE(
        (inverted_bottleneck): Sequential(
          (0): Conv2d(96, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (depth_conv): Sequential(
          (0): Conv2d(288, 288, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), groups=288, bias=False)
          (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (point_linear): Sequential(
          (0): Conv2d(288, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (se): SqueezeExcite(
          (conv_reduce): Conv2d(288, 96, kernel_size=(1, 1), stride=(1, 1))
          (act1): ReLU6(inplace=True)
          (conv_expand): Conv2d(96, 288, kernel_size=(1, 1), stride=(1, 1))
          (gate): Sigmoid()
        )
      )
      (11): InvertedResidual_SE(
        (inverted_bottleneck): Sequential(
          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (depth_conv): Sequential(
          (0): Conv2d(576, 576, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), groups=576, bias=False)
          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (point_linear): Sequential(
          (0): Conv2d(576, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (se): SqueezeExcite(
          (conv_reduce): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1))
          (act1): ReLU6(inplace=True)
          (conv_expand): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1))
          (gate): Sigmoid()
        )
      )
    )
    (17): ModuleList(
      (0): InvertedResidual(
        (inverted_bottleneck): Sequential(
          (0): Conv2d(192, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (depth_conv): Sequential(
          (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)
          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (point_linear): Sequential(
          (0): Conv2d(576, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
      )
      (1): InvertedResidual(
        (inverted_bottleneck): Sequential(
          (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (depth_conv): Sequential(
          (0): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152, bias=False)
          (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (point_linear): Sequential(
          (0): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
      )
      (2): InvertedResidual(
        (inverted_bottleneck): Sequential(
          (0): Conv2d(192, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (depth_conv): Sequential(
          (0): Conv2d(576, 576, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=576, bias=False)
          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (point_linear): Sequential(
          (0): Conv2d(576, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
      )
      (3): InvertedResidual(
        (inverted_bottleneck): Sequential(
          (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (depth_conv): Sequential(
          (0): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)
          (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (point_linear): Sequential(
          (0): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
      )
      (4): InvertedResidual(
        (inverted_bottleneck): Sequential(
          (0): Conv2d(192, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (depth_conv): Sequential(
          (0): Conv2d(576, 576, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=576, bias=False)
          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (point_linear): Sequential(
          (0): Conv2d(576, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
      )
      (5): InvertedResidual(
        (inverted_bottleneck): Sequential(
          (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (depth_conv): Sequential(
          (0): Conv2d(1152, 1152, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1152, bias=False)
          (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (point_linear): Sequential(
          (0): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
      )
      (6): InvertedResidual_SE(
        (inverted_bottleneck): Sequential(
          (0): Conv2d(192, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (depth_conv): Sequential(
          (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)
          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (point_linear): Sequential(
          (0): Conv2d(576, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (se): SqueezeExcite(
          (conv_reduce): Conv2d(576, 192, kernel_size=(1, 1), stride=(1, 1))
          (act1): ReLU6(inplace=True)
          (conv_expand): Conv2d(192, 576, kernel_size=(1, 1), stride=(1, 1))
          (gate): Sigmoid()
        )
      )
      (7): InvertedResidual_SE(
        (inverted_bottleneck): Sequential(
          (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (depth_conv): Sequential(
          (0): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152, bias=False)
          (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (point_linear): Sequential(
          (0): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (se): SqueezeExcite(
          (conv_reduce): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1))
          (act1): ReLU6(inplace=True)
          (conv_expand): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1))
          (gate): Sigmoid()
        )
      )
      (8): InvertedResidual_SE(
        (inverted_bottleneck): Sequential(
          (0): Conv2d(192, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (depth_conv): Sequential(
          (0): Conv2d(576, 576, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=576, bias=False)
          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (point_linear): Sequential(
          (0): Conv2d(576, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (se): SqueezeExcite(
          (conv_reduce): Conv2d(576, 192, kernel_size=(1, 1), stride=(1, 1))
          (act1): ReLU6(inplace=True)
          (conv_expand): Conv2d(192, 576, kernel_size=(1, 1), stride=(1, 1))
          (gate): Sigmoid()
        )
      )
      (9): InvertedResidual_SE(
        (inverted_bottleneck): Sequential(
          (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (depth_conv): Sequential(
          (0): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)
          (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (point_linear): Sequential(
          (0): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (se): SqueezeExcite(
          (conv_reduce): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1))
          (act1): ReLU6(inplace=True)
          (conv_expand): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1))
          (gate): Sigmoid()
        )
      )
      (10): InvertedResidual_SE(
        (inverted_bottleneck): Sequential(
          (0): Conv2d(192, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (depth_conv): Sequential(
          (0): Conv2d(576, 576, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=576, bias=False)
          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (point_linear): Sequential(
          (0): Conv2d(576, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (se): SqueezeExcite(
          (conv_reduce): Conv2d(576, 192, kernel_size=(1, 1), stride=(1, 1))
          (act1): ReLU6(inplace=True)
          (conv_expand): Conv2d(192, 576, kernel_size=(1, 1), stride=(1, 1))
          (gate): Sigmoid()
        )
      )
      (11): InvertedResidual_SE(
        (inverted_bottleneck): Sequential(
          (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (depth_conv): Sequential(
          (0): Conv2d(1152, 1152, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1152, bias=False)
          (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (point_linear): Sequential(
          (0): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (se): SqueezeExcite(
          (conv_reduce): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1))
          (act1): ReLU6(inplace=True)
          (conv_expand): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1))
          (gate): Sigmoid()
        )
      )
      (12): Identity()
    )
    (18): ModuleList(
      (0): InvertedResidual(
        (inverted_bottleneck): Sequential(
          (0): Conv2d(192, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (depth_conv): Sequential(
          (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)
          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (point_linear): Sequential(
          (0): Conv2d(576, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
      )
      (1): InvertedResidual(
        (inverted_bottleneck): Sequential(
          (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (depth_conv): Sequential(
          (0): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152, bias=False)
          (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (point_linear): Sequential(
          (0): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
      )
      (2): InvertedResidual(
        (inverted_bottleneck): Sequential(
          (0): Conv2d(192, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (depth_conv): Sequential(
          (0): Conv2d(576, 576, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=576, bias=False)
          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (point_linear): Sequential(
          (0): Conv2d(576, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
      )
      (3): InvertedResidual(
        (inverted_bottleneck): Sequential(
          (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (depth_conv): Sequential(
          (0): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)
          (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (point_linear): Sequential(
          (0): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
      )
      (4): InvertedResidual(
        (inverted_bottleneck): Sequential(
          (0): Conv2d(192, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (depth_conv): Sequential(
          (0): Conv2d(576, 576, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=576, bias=False)
          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (point_linear): Sequential(
          (0): Conv2d(576, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
      )
      (5): InvertedResidual(
        (inverted_bottleneck): Sequential(
          (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (depth_conv): Sequential(
          (0): Conv2d(1152, 1152, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1152, bias=False)
          (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (point_linear): Sequential(
          (0): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
      )
      (6): InvertedResidual_SE(
        (inverted_bottleneck): Sequential(
          (0): Conv2d(192, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (depth_conv): Sequential(
          (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)
          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (point_linear): Sequential(
          (0): Conv2d(576, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (se): SqueezeExcite(
          (conv_reduce): Conv2d(576, 192, kernel_size=(1, 1), stride=(1, 1))
          (act1): ReLU6(inplace=True)
          (conv_expand): Conv2d(192, 576, kernel_size=(1, 1), stride=(1, 1))
          (gate): Sigmoid()
        )
      )
      (7): InvertedResidual_SE(
        (inverted_bottleneck): Sequential(
          (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (depth_conv): Sequential(
          (0): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152, bias=False)
          (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (point_linear): Sequential(
          (0): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (se): SqueezeExcite(
          (conv_reduce): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1))
          (act1): ReLU6(inplace=True)
          (conv_expand): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1))
          (gate): Sigmoid()
        )
      )
      (8): InvertedResidual_SE(
        (inverted_bottleneck): Sequential(
          (0): Conv2d(192, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (depth_conv): Sequential(
          (0): Conv2d(576, 576, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=576, bias=False)
          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (point_linear): Sequential(
          (0): Conv2d(576, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (se): SqueezeExcite(
          (conv_reduce): Conv2d(576, 192, kernel_size=(1, 1), stride=(1, 1))
          (act1): ReLU6(inplace=True)
          (conv_expand): Conv2d(192, 576, kernel_size=(1, 1), stride=(1, 1))
          (gate): Sigmoid()
        )
      )
      (9): InvertedResidual_SE(
        (inverted_bottleneck): Sequential(
          (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (depth_conv): Sequential(
          (0): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)
          (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (point_linear): Sequential(
          (0): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (se): SqueezeExcite(
          (conv_reduce): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1))
          (act1): ReLU6(inplace=True)
          (conv_expand): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1))
          (gate): Sigmoid()
        )
      )
      (10): InvertedResidual_SE(
        (inverted_bottleneck): Sequential(
          (0): Conv2d(192, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (depth_conv): Sequential(
          (0): Conv2d(576, 576, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=576, bias=False)
          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (point_linear): Sequential(
          (0): Conv2d(576, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (se): SqueezeExcite(
          (conv_reduce): Conv2d(576, 192, kernel_size=(1, 1), stride=(1, 1))
          (act1): ReLU6(inplace=True)
          (conv_expand): Conv2d(192, 576, kernel_size=(1, 1), stride=(1, 1))
          (gate): Sigmoid()
        )
      )
      (11): InvertedResidual_SE(
        (inverted_bottleneck): Sequential(
          (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (depth_conv): Sequential(
          (0): Conv2d(1152, 1152, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1152, bias=False)
          (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (point_linear): Sequential(
          (0): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (se): SqueezeExcite(
          (conv_reduce): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1))
          (act1): ReLU6(inplace=True)
          (conv_expand): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1))
          (gate): Sigmoid()
        )
      )
      (12): Identity()
    )
    (19): ModuleList(
      (0): InvertedResidual(
        (inverted_bottleneck): Sequential(
          (0): Conv2d(192, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (depth_conv): Sequential(
          (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)
          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (point_linear): Sequential(
          (0): Conv2d(576, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
      )
      (1): InvertedResidual(
        (inverted_bottleneck): Sequential(
          (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (depth_conv): Sequential(
          (0): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152, bias=False)
          (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (point_linear): Sequential(
          (0): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
      )
      (2): InvertedResidual(
        (inverted_bottleneck): Sequential(
          (0): Conv2d(192, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (depth_conv): Sequential(
          (0): Conv2d(576, 576, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=576, bias=False)
          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (point_linear): Sequential(
          (0): Conv2d(576, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
      )
      (3): InvertedResidual(
        (inverted_bottleneck): Sequential(
          (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (depth_conv): Sequential(
          (0): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)
          (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (point_linear): Sequential(
          (0): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
      )
      (4): InvertedResidual(
        (inverted_bottleneck): Sequential(
          (0): Conv2d(192, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (depth_conv): Sequential(
          (0): Conv2d(576, 576, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=576, bias=False)
          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (point_linear): Sequential(
          (0): Conv2d(576, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
      )
      (5): InvertedResidual(
        (inverted_bottleneck): Sequential(
          (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (depth_conv): Sequential(
          (0): Conv2d(1152, 1152, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1152, bias=False)
          (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (point_linear): Sequential(
          (0): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
      )
      (6): InvertedResidual_SE(
        (inverted_bottleneck): Sequential(
          (0): Conv2d(192, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (depth_conv): Sequential(
          (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)
          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (point_linear): Sequential(
          (0): Conv2d(576, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (se): SqueezeExcite(
          (conv_reduce): Conv2d(576, 192, kernel_size=(1, 1), stride=(1, 1))
          (act1): ReLU6(inplace=True)
          (conv_expand): Conv2d(192, 576, kernel_size=(1, 1), stride=(1, 1))
          (gate): Sigmoid()
        )
      )
      (7): InvertedResidual_SE(
        (inverted_bottleneck): Sequential(
          (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (depth_conv): Sequential(
          (0): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152, bias=False)
          (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (point_linear): Sequential(
          (0): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (se): SqueezeExcite(
          (conv_reduce): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1))
          (act1): ReLU6(inplace=True)
          (conv_expand): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1))
          (gate): Sigmoid()
        )
      )
      (8): InvertedResidual_SE(
        (inverted_bottleneck): Sequential(
          (0): Conv2d(192, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (depth_conv): Sequential(
          (0): Conv2d(576, 576, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=576, bias=False)
          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (point_linear): Sequential(
          (0): Conv2d(576, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (se): SqueezeExcite(
          (conv_reduce): Conv2d(576, 192, kernel_size=(1, 1), stride=(1, 1))
          (act1): ReLU6(inplace=True)
          (conv_expand): Conv2d(192, 576, kernel_size=(1, 1), stride=(1, 1))
          (gate): Sigmoid()
        )
      )
      (9): InvertedResidual_SE(
        (inverted_bottleneck): Sequential(
          (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (depth_conv): Sequential(
          (0): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)
          (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (point_linear): Sequential(
          (0): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (se): SqueezeExcite(
          (conv_reduce): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1))
          (act1): ReLU6(inplace=True)
          (conv_expand): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1))
          (gate): Sigmoid()
        )
      )
      (10): InvertedResidual_SE(
        (inverted_bottleneck): Sequential(
          (0): Conv2d(192, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (depth_conv): Sequential(
          (0): Conv2d(576, 576, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=576, bias=False)
          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (point_linear): Sequential(
          (0): Conv2d(576, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (se): SqueezeExcite(
          (conv_reduce): Conv2d(576, 192, kernel_size=(1, 1), stride=(1, 1))
          (act1): ReLU6(inplace=True)
          (conv_expand): Conv2d(192, 576, kernel_size=(1, 1), stride=(1, 1))
          (gate): Sigmoid()
        )
      )
      (11): InvertedResidual_SE(
        (inverted_bottleneck): Sequential(
          (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (depth_conv): Sequential(
          (0): Conv2d(1152, 1152, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1152, bias=False)
          (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (point_linear): Sequential(
          (0): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (se): SqueezeExcite(
          (conv_reduce): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1))
          (act1): ReLU6(inplace=True)
          (conv_expand): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1))
          (gate): Sigmoid()
        )
      )
      (12): Identity()
    )
    (20): ModuleList(
      (0): InvertedResidual(
        (inverted_bottleneck): Sequential(
          (0): Conv2d(192, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (depth_conv): Sequential(
          (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)
          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (point_linear): Sequential(
          (0): Conv2d(576, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
      )
      (1): InvertedResidual(
        (inverted_bottleneck): Sequential(
          (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (depth_conv): Sequential(
          (0): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152, bias=False)
          (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (point_linear): Sequential(
          (0): Conv2d(1152, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
      )
      (2): InvertedResidual(
        (inverted_bottleneck): Sequential(
          (0): Conv2d(192, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (depth_conv): Sequential(
          (0): Conv2d(576, 576, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=576, bias=False)
          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (point_linear): Sequential(
          (0): Conv2d(576, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
      )
      (3): InvertedResidual(
        (inverted_bottleneck): Sequential(
          (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (depth_conv): Sequential(
          (0): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)
          (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (point_linear): Sequential(
          (0): Conv2d(1152, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
      )
      (4): InvertedResidual(
        (inverted_bottleneck): Sequential(
          (0): Conv2d(192, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (depth_conv): Sequential(
          (0): Conv2d(576, 576, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=576, bias=False)
          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (point_linear): Sequential(
          (0): Conv2d(576, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
      )
      (5): InvertedResidual(
        (inverted_bottleneck): Sequential(
          (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (depth_conv): Sequential(
          (0): Conv2d(1152, 1152, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1152, bias=False)
          (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (point_linear): Sequential(
          (0): Conv2d(1152, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
      )
      (6): InvertedResidual_SE(
        (inverted_bottleneck): Sequential(
          (0): Conv2d(192, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (depth_conv): Sequential(
          (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)
          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (point_linear): Sequential(
          (0): Conv2d(576, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (se): SqueezeExcite(
          (conv_reduce): Conv2d(576, 192, kernel_size=(1, 1), stride=(1, 1))
          (act1): ReLU6(inplace=True)
          (conv_expand): Conv2d(192, 576, kernel_size=(1, 1), stride=(1, 1))
          (gate): Sigmoid()
        )
      )
      (7): InvertedResidual_SE(
        (inverted_bottleneck): Sequential(
          (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (depth_conv): Sequential(
          (0): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152, bias=False)
          (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (point_linear): Sequential(
          (0): Conv2d(1152, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (se): SqueezeExcite(
          (conv_reduce): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1))
          (act1): ReLU6(inplace=True)
          (conv_expand): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1))
          (gate): Sigmoid()
        )
      )
      (8): InvertedResidual_SE(
        (inverted_bottleneck): Sequential(
          (0): Conv2d(192, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (depth_conv): Sequential(
          (0): Conv2d(576, 576, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=576, bias=False)
          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (point_linear): Sequential(
          (0): Conv2d(576, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (se): SqueezeExcite(
          (conv_reduce): Conv2d(576, 192, kernel_size=(1, 1), stride=(1, 1))
          (act1): ReLU6(inplace=True)
          (conv_expand): Conv2d(192, 576, kernel_size=(1, 1), stride=(1, 1))
          (gate): Sigmoid()
        )
      )
      (9): InvertedResidual_SE(
        (inverted_bottleneck): Sequential(
          (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (depth_conv): Sequential(
          (0): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)
          (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (point_linear): Sequential(
          (0): Conv2d(1152, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (se): SqueezeExcite(
          (conv_reduce): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1))
          (act1): ReLU6(inplace=True)
          (conv_expand): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1))
          (gate): Sigmoid()
        )
      )
      (10): InvertedResidual_SE(
        (inverted_bottleneck): Sequential(
          (0): Conv2d(192, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (depth_conv): Sequential(
          (0): Conv2d(576, 576, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=576, bias=False)
          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (point_linear): Sequential(
          (0): Conv2d(576, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (se): SqueezeExcite(
          (conv_reduce): Conv2d(576, 192, kernel_size=(1, 1), stride=(1, 1))
          (act1): ReLU6(inplace=True)
          (conv_expand): Conv2d(192, 576, kernel_size=(1, 1), stride=(1, 1))
          (gate): Sigmoid()
        )
      )
      (11): InvertedResidual_SE(
        (inverted_bottleneck): Sequential(
          (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (depth_conv): Sequential(
          (0): Conv2d(1152, 1152, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1152, bias=False)
          (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
          (2): ReLU6(inplace=True)
        )
        (point_linear): Sequential(
          (0): Conv2d(1152, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (se): SqueezeExcite(
          (conv_reduce): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1))
          (act1): ReLU6(inplace=True)
          (conv_expand): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1))
          (gate): Sigmoid()
        )
      )
    )
  )
  (feature_mix_layer): Sequential(
    (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (1): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
    (2): ReLU6(inplace=True)
  )
  (avgpool): AvgPool2d(kernel_size=7, stride=7, padding=0)
  (classifier): Sequential(
    (0): Linear(in_features=1280, out_features=1000, bias=True)
  )
)
[04/02 14:53:24] SuperNet Training INFO: Trainset Size: 1231167
[04/02 14:53:24] SuperNet Training INFO: Validset Size:   50000
[04/02 14:53:24] SuperNet Training INFO: Compose(
    RandomResizedCrop(size=(224, 224), scale=(0.08, 1.0), ratio=(0.75, 1.3333), interpolation=bilinear)
    RandomHorizontalFlip(p=0.5)
    ToTensor()
    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
)
[04/02 14:53:24] SuperNet Training INFO: --> START gss-mobile0bn-seed-0
[04/02 14:56:36] SuperNet Training INFO: iter:   120/144360  CE: 6.9398  
[04/02 14:57:43] SuperNet Training INFO: iter:   240/144360  CE: 6.9077  
[04/02 14:58:49] SuperNet Training INFO: iter:   360/144360  CE: 6.9234  
[04/02 14:59:55] SuperNet Training INFO: iter:   480/144360  CE: 6.9485  
[04/02 15:01:00] SuperNet Training INFO: iter:   600/144360  CE: 6.8745  
[04/02 15:02:05] SuperNet Training INFO: iter:   720/144360  CE: 6.9296  
[04/02 15:03:11] SuperNet Training INFO: iter:   840/144360  CE: 6.9244  
[04/02 15:04:16] SuperNet Training INFO: iter:   960/144360  CE: 6.9282  
[04/02 15:05:21] SuperNet Training INFO: iter:  1080/144360  CE: 6.9283  
[04/02 15:06:26] SuperNet Training INFO: iter:  1200/144360  CE: 6.9587  
[04/02 15:06:28] SuperNet Training INFO: --> epoch:   1/120  avg CE: 6.9133  lr: 0.11997943949853311  
[04/02 15:08:10] SuperNet Training INFO: iter:  1320/144360  CE: 6.8910  
[04/02 15:09:17] SuperNet Training INFO: iter:  1440/144360  CE: 6.8738  
[04/02 15:10:24] SuperNet Training INFO: iter:  1560/144360  CE: 6.8771  
[04/02 15:11:30] SuperNet Training INFO: iter:  1680/144360  CE: 6.8968  
[04/02 15:12:36] SuperNet Training INFO: iter:  1800/144360  CE: 6.8663  
[04/02 15:13:42] SuperNet Training INFO: iter:  1920/144360  CE: 6.9544  
[04/02 15:14:47] SuperNet Training INFO: iter:  2040/144360  CE: 6.8627  
[04/02 15:15:54] SuperNet Training INFO: iter:  2160/144360  CE: 6.9450  
[04/02 15:16:59] SuperNet Training INFO: iter:  2280/144360  CE: 6.8289  
[04/02 15:18:05] SuperNet Training INFO: iter:  2400/144360  CE: 6.8211  
[04/02 15:18:08] SuperNet Training INFO: --> epoch:   2/120  avg CE: 6.8801  lr: 0.11991777208527424  
[04/02 15:19:49] SuperNet Training INFO: iter:  2520/144360  CE: 6.8358  
[04/02 15:20:56] SuperNet Training INFO: iter:  2640/144360  CE: 6.8876  
[04/02 15:22:02] SuperNet Training INFO: iter:  2760/144360  CE: 6.8712  
[04/02 15:23:08] SuperNet Training INFO: iter:  2880/144360  CE: 6.8261  
[04/02 15:24:14] SuperNet Training INFO: iter:  3000/144360  CE: 6.8345  
[04/02 15:25:19] SuperNet Training INFO: iter:  3120/144360  CE: 6.8071  
[04/02 15:26:25] SuperNet Training INFO: iter:  3240/144360  CE: 6.8051  
[04/02 15:27:31] SuperNet Training INFO: iter:  3360/144360  CE: 6.8770  
[04/02 15:28:36] SuperNet Training INFO: iter:  3480/144360  CE: 6.8064  
[04/02 15:29:43] SuperNet Training INFO: iter:  3600/144360  CE: 6.7866  
[04/02 15:29:47] SuperNet Training INFO: --> epoch:   3/120  avg CE: 6.8383  lr: 0.11981504002398749  
[04/02 15:31:24] SuperNet Training INFO: iter:  3720/144360  CE: 6.7169  
[04/02 15:32:31] SuperNet Training INFO: iter:  3840/144360  CE: 6.8459  
[04/02 15:33:37] SuperNet Training INFO: iter:  3960/144360  CE: 6.8595  
[04/02 15:34:43] SuperNet Training INFO: iter:  4080/144360  CE: 6.7566  
[04/02 15:35:49] SuperNet Training INFO: iter:  4200/144360  CE: 6.8412  
[04/02 15:36:54] SuperNet Training INFO: iter:  4320/144360  CE: 6.7469  
[04/02 15:38:00] SuperNet Training INFO: iter:  4440/144360  CE: 6.7953  
[04/02 15:39:07] SuperNet Training INFO: iter:  4560/144360  CE: 6.7875  
[04/02 15:40:12] SuperNet Training INFO: iter:  4680/144360  CE: 6.7509  
[04/02 15:41:17] SuperNet Training INFO: iter:  4800/144360  CE: 6.7562  
[04/02 15:41:23] SuperNet Training INFO: --> epoch:   4/120  avg CE: 6.7800  lr: 0.11967131372209595  
[04/02 15:43:00] SuperNet Training INFO: iter:  4920/144360  CE: 6.8103  
[04/02 15:44:07] SuperNet Training INFO: iter:  5040/144360  CE: 6.7518  
[04/02 15:45:14] SuperNet Training INFO: iter:  5160/144360  CE: 6.7760  
[04/02 15:46:21] SuperNet Training INFO: iter:  5280/144360  CE: 6.6443  
[04/02 15:47:27] SuperNet Training INFO: iter:  5400/144360  CE: 6.6973  
[04/02 15:48:33] SuperNet Training INFO: iter:  5520/144360  CE: 6.6725  
[04/02 15:49:39] SuperNet Training INFO: iter:  5640/144360  CE: 6.7476  
[04/02 15:50:45] SuperNet Training INFO: iter:  5760/144360  CE: 6.7978  
[04/02 15:51:51] SuperNet Training INFO: iter:  5880/144360  CE: 6.6418  
[04/02 15:52:57] SuperNet Training INFO: iter:  6000/144360  CE: 6.7409  
[04/02 15:53:04] SuperNet Training INFO: --> epoch:   5/120  avg CE: 6.7278  lr: 0.11948669168242801  
[04/02 15:54:38] SuperNet Training INFO: iter:  6120/144360  CE: 6.7075  
[04/02 15:55:44] SuperNet Training INFO: iter:  6240/144360  CE: 6.6939  
[04/02 15:56:50] SuperNet Training INFO: iter:  6360/144360  CE: 6.6626  
[04/02 15:57:56] SuperNet Training INFO: iter:  6480/144360  CE: 6.6614  
[04/02 15:59:02] SuperNet Training INFO: iter:  6600/144360  CE: 6.5979  
[04/02 16:00:08] SuperNet Training INFO: iter:  6720/144360  CE: 6.7054  
[04/02 16:01:14] SuperNet Training INFO: iter:  6840/144360  CE: 6.7369  
[04/02 16:02:20] SuperNet Training INFO: iter:  6960/144360  CE: 6.6575  
[04/02 16:03:25] SuperNet Training INFO: iter:  7080/144360  CE: 6.7008  
[04/02 16:04:31] SuperNet Training INFO: iter:  7200/144360  CE: 6.7094  
[04/02 16:04:40] SuperNet Training INFO: --> epoch:   6/120  avg CE: 6.6741  lr: 0.1192613004357081  
[04/02 16:06:12] SuperNet Training INFO: iter:  7320/144360  CE: 6.7851  
[04/02 16:07:19] SuperNet Training INFO: iter:  7440/144360  CE: 6.6027  
[04/02 16:08:25] SuperNet Training INFO: iter:  7560/144360  CE: 6.5462  
[04/02 16:09:32] SuperNet Training INFO: iter:  7680/144360  CE: 6.6429  
[04/02 16:10:38] SuperNet Training INFO: iter:  7800/144360  CE: 6.5969  
[04/02 16:11:44] SuperNet Training INFO: iter:  7920/144360  CE: 6.5994  
[04/02 16:12:49] SuperNet Training INFO: iter:  8040/144360  CE: 6.5326  
[04/02 16:13:55] SuperNet Training INFO: iter:  8160/144360  CE: 6.7834  
[04/02 16:15:01] SuperNet Training INFO: iter:  8280/144360  CE: 6.5504  
[04/02 16:16:07] SuperNet Training INFO: iter:  8400/144360  CE: 6.5466  
[04/02 16:16:17] SuperNet Training INFO: --> epoch:   7/120  avg CE: 6.5961  lr: 0.11899529445383715  
[04/02 16:17:49] SuperNet Training INFO: iter:  8520/144360  CE: 6.5850  
[04/02 16:18:57] SuperNet Training INFO: iter:  8640/144360  CE: 6.5746  
[04/02 16:20:04] SuperNet Training INFO: iter:  8760/144360  CE: 6.5255  
[04/02 16:21:10] SuperNet Training INFO: iter:  8880/144360  CE: 6.4413  
[04/02 16:22:17] SuperNet Training INFO: iter:  9000/144360  CE: 6.4327  
[04/02 16:23:23] SuperNet Training INFO: iter:  9120/144360  CE: 6.4185  
[04/02 16:24:29] SuperNet Training INFO: iter:  9240/144360  CE: 6.3886  
[04/02 16:25:35] SuperNet Training INFO: iter:  9360/144360  CE: 6.4942  
[04/02 16:26:40] SuperNet Training INFO: iter:  9480/144360  CE: 6.5730  
[04/02 16:27:46] SuperNet Training INFO: iter:  9600/144360  CE: 6.5364  
[04/02 16:27:59] SuperNet Training INFO: --> epoch:   8/120  avg CE: 6.5030  lr: 0.11868885604402826  
[04/02 16:29:29] SuperNet Training INFO: iter:  9720/144360  CE: 6.4877  
[04/02 16:30:37] SuperNet Training INFO: iter:  9840/144360  CE: 6.4128  
[04/02 16:31:44] SuperNet Training INFO: iter:  9960/144360  CE: 6.3226  
[04/02 16:32:51] SuperNet Training INFO: iter: 10080/144360  CE: 6.4121  
[04/02 16:33:58] SuperNet Training INFO: iter: 10200/144360  CE: 6.5172  
[04/02 16:35:04] SuperNet Training INFO: iter: 10320/144360  CE: 6.3495  
[04/02 16:36:10] SuperNet Training INFO: iter: 10440/144360  CE: 6.4294  
[04/02 16:37:16] SuperNet Training INFO: iter: 10560/144360  CE: 6.4200  
[04/02 16:38:22] SuperNet Training INFO: iter: 10680/144360  CE: 6.4391  
[04/02 16:39:28] SuperNet Training INFO: iter: 10800/144360  CE: 6.4680  
[04/02 16:39:42] SuperNet Training INFO: --> epoch:   9/120  avg CE: 6.4209  lr: 0.11834219522386061  
[04/02 16:41:11] SuperNet Training INFO: iter: 10920/144360  CE: 6.3729  
[04/02 16:42:21] SuperNet Training INFO: iter: 11040/144360  CE: 6.2007  
[04/02 16:43:27] SuperNet Training INFO: iter: 11160/144360  CE: 6.3747  
[04/02 16:44:34] SuperNet Training INFO: iter: 11280/144360  CE: 6.3850  
[04/02 16:45:41] SuperNet Training INFO: iter: 11400/144360  CE: 6.2407  
[04/02 16:46:47] SuperNet Training INFO: iter: 11520/144360  CE: 6.4768  
[04/02 16:47:52] SuperNet Training INFO: iter: 11640/144360  CE: 6.3697  
[04/02 16:48:58] SuperNet Training INFO: iter: 11760/144360  CE: 6.3080  
[04/02 16:50:04] SuperNet Training INFO: iter: 11880/144360  CE: 6.2188  
[04/02 16:51:10] SuperNet Training INFO: iter: 12000/144360  CE: 6.2094  
[04/02 16:51:25] SuperNet Training INFO: --> epoch:  10/120  avg CE: 6.3460  lr: 0.1179555495773443  
[04/02 16:52:52] SuperNet Training INFO: iter: 12120/144360  CE: 6.3116  
[04/02 16:53:59] SuperNet Training INFO: iter: 12240/144360  CE: 6.4180  
[04/02 16:55:06] SuperNet Training INFO: iter: 12360/144360  CE: 6.1899  
[04/02 16:56:12] SuperNet Training INFO: iter: 12480/144360  CE: 6.4116  
[04/02 16:57:18] SuperNet Training INFO: iter: 12600/144360  CE: 6.1513  
[04/02 16:58:24] SuperNet Training INFO: iter: 12720/144360  CE: 6.2641  
[04/02 16:59:29] SuperNet Training INFO: iter: 12840/144360  CE: 6.1466  
[04/02 17:00:35] SuperNet Training INFO: iter: 12960/144360  CE: 6.0514  
[04/02 17:01:41] SuperNet Training INFO: iter: 13080/144360  CE: 6.1941  
[04/02 17:02:47] SuperNet Training INFO: iter: 13200/144360  CE: 6.0731  
[04/02 17:03:05] SuperNet Training INFO: --> epoch:  11/120  avg CE: 6.2733  lr: 0.11752918409209158  
[04/02 17:04:29] SuperNet Training INFO: iter: 13320/144360  CE: 6.1425  
[04/02 17:05:36] SuperNet Training INFO: iter: 13440/144360  CE: 6.0376  
[04/02 17:06:42] SuperNet Training INFO: iter: 13560/144360  CE: 6.1764  
[04/02 17:07:49] SuperNet Training INFO: iter: 13680/144360  CE: 6.1451  
[04/02 17:08:55] SuperNet Training INFO: iter: 13800/144360  CE: 6.0687  
[04/02 17:10:01] SuperNet Training INFO: iter: 13920/144360  CE: 6.2688  
[04/02 17:11:06] SuperNet Training INFO: iter: 14040/144360  CE: 6.1193  
[04/02 17:12:12] SuperNet Training INFO: iter: 14160/144360  CE: 6.1820  
[04/02 17:13:19] SuperNet Training INFO: iter: 14280/144360  CE: 6.1236  
[04/02 17:14:25] SuperNet Training INFO: iter: 14400/144360  CE: 6.1644  
[04/02 17:14:44] SuperNet Training INFO: --> epoch:  12/120  avg CE: 6.1938  lr: 0.11706339097770935  
[04/02 17:16:07] SuperNet Training INFO: iter: 14520/144360  CE: 6.1064  
[04/02 17:17:14] SuperNet Training INFO: iter: 14640/144360  CE: 6.0198  
[04/02 17:18:20] SuperNet Training INFO: iter: 14760/144360  CE: 6.0320  
[04/02 17:19:26] SuperNet Training INFO: iter: 14880/144360  CE: 6.1267  
[04/02 17:20:32] SuperNet Training INFO: iter: 15000/144360  CE: 6.1658  
[04/02 17:21:38] SuperNet Training INFO: iter: 15120/144360  CE: 6.1104  
[04/02 17:22:44] SuperNet Training INFO: iter: 15240/144360  CE: 5.9445  
[04/02 17:23:50] SuperNet Training INFO: iter: 15360/144360  CE: 5.9713  
[04/02 17:24:56] SuperNet Training INFO: iter: 15480/144360  CE: 5.9821  
[04/02 17:26:01] SuperNet Training INFO: iter: 15600/144360  CE: 6.1103  
[04/02 17:26:23] SuperNet Training INFO: --> epoch:  13/120  avg CE: 6.1064  lr: 0.11655848946553125  
[04/02 17:27:43] SuperNet Training INFO: iter: 15720/144360  CE: 6.1435  
[04/02 17:28:50] SuperNet Training INFO: iter: 15840/144360  CE: 5.9559  
[04/02 17:29:56] SuperNet Training INFO: iter: 15960/144360  CE: 6.1423  
[04/02 17:31:02] SuperNet Training INFO: iter: 16080/144360  CE: 5.8765  
[04/02 17:32:07] SuperNet Training INFO: iter: 16200/144360  CE: 6.0777  
[04/02 17:33:13] SuperNet Training INFO: iter: 16320/144360  CE: 6.0805  
[04/02 17:34:20] SuperNet Training INFO: iter: 16440/144360  CE: 6.0540  
[04/02 17:35:25] SuperNet Training INFO: iter: 16560/144360  CE: 6.1320  
[04/02 17:36:31] SuperNet Training INFO: iter: 16680/144360  CE: 6.0430  
[04/02 17:37:36] SuperNet Training INFO: iter: 16800/144360  CE: 6.1882  
[04/02 17:37:58] SuperNet Training INFO: --> epoch:  14/120  avg CE: 6.0282  lr: 0.11601482558983225  
[04/02 17:39:18] SuperNet Training INFO: iter: 16920/144360  CE: 6.0582  
[04/02 17:40:25] SuperNet Training INFO: iter: 17040/144360  CE: 6.0393  
[04/02 17:41:31] SuperNet Training INFO: iter: 17160/144360  CE: 5.9736  
[04/02 17:42:37] SuperNet Training INFO: iter: 17280/144360  CE: 5.7483  
[04/02 17:43:44] SuperNet Training INFO: iter: 17400/144360  CE: 6.1859  
[04/02 17:44:52] SuperNet Training INFO: iter: 17520/144360  CE: 5.7976  
[04/02 17:45:59] SuperNet Training INFO: iter: 17640/144360  CE: 5.8496  
[04/02 17:47:05] SuperNet Training INFO: iter: 17760/144360  CE: 6.0070  
[04/02 17:48:11] SuperNet Training INFO: iter: 17880/144360  CE: 5.9039  
[04/02 17:49:17] SuperNet Training INFO: iter: 18000/144360  CE: 5.8622  
[04/02 17:49:40] SuperNet Training INFO: --> epoch:  15/120  avg CE: 5.9505  lr: 0.11543277195067722  
[04/02 17:50:58] SuperNet Training INFO: iter: 18120/144360  CE: 5.8250  
[04/02 17:52:06] SuperNet Training INFO: iter: 18240/144360  CE: 5.7937  
[04/02 17:53:14] SuperNet Training INFO: iter: 18360/144360  CE: 5.8975  
[04/02 17:54:21] SuperNet Training INFO: iter: 18480/144360  CE: 5.8604  
[04/02 17:55:27] SuperNet Training INFO: iter: 18600/144360  CE: 6.0247  
[04/02 17:56:34] SuperNet Training INFO: iter: 18720/144360  CE: 5.7746  
[04/02 17:57:40] SuperNet Training INFO: iter: 18840/144360  CE: 6.0463  
[04/02 17:58:47] SuperNet Training INFO: iter: 18960/144360  CE: 5.7012  
[04/02 17:59:53] SuperNet Training INFO: iter: 19080/144360  CE: 6.0133  
[04/02 18:00:59] SuperNet Training INFO: iter: 19200/144360  CE: 5.7959  
[04/02 18:01:24] SuperNet Training INFO: --> epoch:  16/120  avg CE: 5.8799  lr: 0.1148127274585561  
[04/02 18:02:41] SuperNet Training INFO: iter: 19320/144360  CE: 5.9370  
[04/02 18:03:48] SuperNet Training INFO: iter: 19440/144360  CE: 5.6074  
[04/02 18:04:56] SuperNet Training INFO: iter: 19560/144360  CE: 5.9992  
[04/02 18:06:03] SuperNet Training INFO: iter: 19680/144360  CE: 5.7133  
[04/02 18:07:09] SuperNet Training INFO: iter: 19800/144360  CE: 5.6560  
[04/02 18:08:15] SuperNet Training INFO: iter: 19920/144360  CE: 5.6508  
[04/02 18:09:22] SuperNet Training INFO: iter: 20040/144360  CE: 5.7880  
[04/02 18:10:29] SuperNet Training INFO: iter: 20160/144360  CE: 5.7225  
[04/02 18:11:36] SuperNet Training INFO: iter: 20280/144360  CE: 5.7914  
[04/02 18:12:43] SuperNet Training INFO: iter: 20400/144360  CE: 5.8102  
[04/02 18:13:11] SuperNet Training INFO: --> epoch:  17/120  avg CE: 5.8126  lr: 0.11415511706099139  
[04/02 18:14:25] SuperNet Training INFO: iter: 20520/144360  CE: 5.8400  
[04/02 18:15:32] SuperNet Training INFO: iter: 20640/144360  CE: 5.6264  
[04/02 18:16:39] SuperNet Training INFO: iter: 20760/144360  CE: 5.8672  
[04/02 18:17:45] SuperNet Training INFO: iter: 20880/144360  CE: 5.9074  
[04/02 18:18:51] SuperNet Training INFO: iter: 21000/144360  CE: 5.6650  
[04/02 18:19:58] SuperNet Training INFO: iter: 21120/144360  CE: 5.7553  
[04/02 18:21:05] SuperNet Training INFO: iter: 21240/144360  CE: 5.7861  
[04/02 18:22:11] SuperNet Training INFO: iter: 21360/144360  CE: 5.6229  
[04/02 18:23:16] SuperNet Training INFO: iter: 21480/144360  CE: 5.6499  
[04/02 18:24:22] SuperNet Training INFO: iter: 21600/144360  CE: 5.5035  
[04/02 18:24:52] SuperNet Training INFO: --> epoch:  18/120  avg CE: 5.7337  lr: 0.11346039145130195  
[04/02 18:26:05] SuperNet Training INFO: iter: 21720/144360  CE: 5.6215  
[04/02 18:27:13] SuperNet Training INFO: iter: 21840/144360  CE: 5.7769  
[04/02 18:28:19] SuperNet Training INFO: iter: 21960/144360  CE: 5.5123  
[04/02 18:29:26] SuperNet Training INFO: iter: 22080/144360  CE: 5.4631  
[04/02 18:30:32] SuperNet Training INFO: iter: 22200/144360  CE: 5.6139  
[04/02 18:31:39] SuperNet Training INFO: iter: 22320/144360  CE: 5.4774  
[04/02 18:32:45] SuperNet Training INFO: iter: 22440/144360  CE: 5.8681  
[04/02 18:33:50] SuperNet Training INFO: iter: 22560/144360  CE: 5.6454  
[04/02 18:34:56] SuperNet Training INFO: iter: 22680/144360  CE: 5.7572  
[04/02 18:36:00] SuperNet Training INFO: iter: 22800/144360  CE: 5.5832  
[04/02 18:36:31] SuperNet Training INFO: --> epoch:  19/120  avg CE: 5.6622  lr: 0.11272902675971772  
[04/02 18:37:41] SuperNet Training INFO: iter: 22920/144360  CE: 5.5222  
[04/02 18:38:48] SuperNet Training INFO: iter: 23040/144360  CE: 5.4132  
[04/02 18:39:55] SuperNet Training INFO: iter: 23160/144360  CE: 5.5761  
[04/02 18:41:02] SuperNet Training INFO: iter: 23280/144360  CE: 5.8334  
[04/02 18:42:08] SuperNet Training INFO: iter: 23400/144360  CE: 5.6663  
[04/02 18:43:13] SuperNet Training INFO: iter: 23520/144360  CE: 5.5401  
[04/02 18:44:19] SuperNet Training INFO: iter: 23640/144360  CE: 5.6406  
[04/02 18:45:24] SuperNet Training INFO: iter: 23760/144360  CE: 5.6240  
[04/02 18:46:30] SuperNet Training INFO: iter: 23880/144360  CE: 5.6971  
[04/02 18:47:38] SuperNet Training INFO: iter: 24000/144360  CE: 5.7601  
[04/02 18:48:10] SuperNet Training INFO: --> epoch:  20/120  avg CE: 5.5886  lr: 0.11196152422706572  
[04/02 18:49:19] SuperNet Training INFO: iter: 24120/144360  CE: 5.7442  
[04/02 18:50:26] SuperNet Training INFO: iter: 24240/144360  CE: 5.7356  
[04/02 18:51:33] SuperNet Training INFO: iter: 24360/144360  CE: 5.4521  
[04/02 18:52:40] SuperNet Training INFO: iter: 24480/144360  CE: 5.1672  
[04/02 18:53:45] SuperNet Training INFO: iter: 24600/144360  CE: 5.5483  
[04/02 18:54:52] SuperNet Training INFO: iter: 24720/144360  CE: 5.3202  
[04/02 18:55:58] SuperNet Training INFO: iter: 24840/144360  CE: 5.4250  
[04/02 18:57:05] SuperNet Training INFO: iter: 24960/144360  CE: 5.5363  
[04/02 18:58:11] SuperNet Training INFO: iter: 25080/144360  CE: 5.4837  
[04/02 18:59:17] SuperNet Training INFO: iter: 25200/144360  CE: 5.5110  
[04/02 18:59:51] SuperNet Training INFO: --> epoch:  21/120  avg CE: 5.5185  lr: 0.11115840986124514  
[04/02 19:00:59] SuperNet Training INFO: iter: 25320/144360  CE: 5.4947  
[04/02 19:02:07] SuperNet Training INFO: iter: 25440/144360  CE: 5.6001  
[04/02 19:03:13] SuperNet Training INFO: iter: 25560/144360  CE: 5.7200  
[04/02 19:04:19] SuperNet Training INFO: iter: 25680/144360  CE: 5.5192  
[04/02 19:05:25] SuperNet Training INFO: iter: 25800/144360  CE: 5.5351  
[04/02 19:06:30] SuperNet Training INFO: iter: 25920/144360  CE: 5.4375  
[04/02 19:07:36] SuperNet Training INFO: iter: 26040/144360  CE: 5.5003  
[04/02 19:08:42] SuperNet Training INFO: iter: 26160/144360  CE: 5.4300  
[04/02 19:09:47] SuperNet Training INFO: iter: 26280/144360  CE: 5.4973  
[04/02 19:10:53] SuperNet Training INFO: iter: 26400/144360  CE: 5.2181  
[04/02 19:11:28] SuperNet Training INFO: --> epoch:  22/120  avg CE: 5.4518  lr: 0.11032023407672516  
[04/02 19:12:36] SuperNet Training INFO: iter: 26520/144360  CE: 5.2992  
[04/02 19:13:43] SuperNet Training INFO: iter: 26640/144360  CE: 5.4527  
[04/02 19:14:50] SuperNet Training INFO: iter: 26760/144360  CE: 5.5569  
[04/02 19:15:58] SuperNet Training INFO: iter: 26880/144360  CE: 5.2816  
[04/02 19:17:05] SuperNet Training INFO: iter: 27000/144360  CE: 5.0419  
[04/02 19:18:12] SuperNet Training INFO: iter: 27120/144360  CE: 5.4499  
[04/02 19:19:19] SuperNet Training INFO: iter: 27240/144360  CE: 5.3070  
[04/02 19:20:25] SuperNet Training INFO: iter: 27360/144360  CE: 5.3306  
[04/02 19:21:33] SuperNet Training INFO: iter: 27480/144360  CE: 5.3943  
[04/02 19:22:39] SuperNet Training INFO: iter: 27600/144360  CE: 5.5004  
[04/02 19:23:16] SuperNet Training INFO: --> epoch:  23/120  avg CE: 5.3847  lr: 0.10944757131732062  
[04/02 19:24:22] SuperNet Training INFO: iter: 27720/144360  CE: 5.5608  
[04/02 19:25:29] SuperNet Training INFO: iter: 27840/144360  CE: 5.3720  
[04/02 19:26:36] SuperNet Training INFO: iter: 27960/144360  CE: 5.3536  
[04/02 19:27:43] SuperNet Training INFO: iter: 28080/144360  CE: 5.3062  
[04/02 19:28:50] SuperNet Training INFO: iter: 28200/144360  CE: 5.1879  
[04/02 19:29:56] SuperNet Training INFO: iter: 28320/144360  CE: 5.1950  
[04/02 19:31:03] SuperNet Training INFO: iter: 28440/144360  CE: 5.3407  
[04/02 19:32:09] SuperNet Training INFO: iter: 28560/144360  CE: 5.2353  
[04/02 19:33:16] SuperNet Training INFO: iter: 28680/144360  CE: 5.1290  
[04/02 19:34:22] SuperNet Training INFO: iter: 28800/144360  CE: 5.2858  
[04/02 19:35:01] SuperNet Training INFO: --> epoch:  24/120  avg CE: 5.3139  lr: 0.10854101966249682  
[04/02 19:36:03] SuperNet Training INFO: iter: 28920/144360  CE: 5.3931  
[04/02 19:37:10] SuperNet Training INFO: iter: 29040/144360  CE: 5.3190  
[04/02 19:38:17] SuperNet Training INFO: iter: 29160/144360  CE: 5.4818  
[04/02 19:39:23] SuperNet Training INFO: iter: 29280/144360  CE: 5.2748  
[04/02 19:40:29] SuperNet Training INFO: iter: 29400/144360  CE: 5.2032  
[04/02 19:41:35] SuperNet Training INFO: iter: 29520/144360  CE: 5.1627  
[04/02 19:42:41] SuperNet Training INFO: iter: 29640/144360  CE: 4.9898  
[04/02 19:43:46] SuperNet Training INFO: iter: 29760/144360  CE: 5.2376  
[04/02 19:44:52] SuperNet Training INFO: iter: 29880/144360  CE: 5.0863  
[04/02 19:45:57] SuperNet Training INFO: iter: 30000/144360  CE: 5.0942  
[04/02 19:46:38] SuperNet Training INFO: --> epoch:  25/120  avg CE: 5.2593  lr: 0.10760120041747454  
[04/02 19:47:39] SuperNet Training INFO: iter: 30120/144360  CE: 5.1031  
[04/02 19:48:46] SuperNet Training INFO: iter: 30240/144360  CE: 5.0611  
[04/02 19:49:53] SuperNet Training INFO: iter: 30360/144360  CE: 5.1044  
[04/02 19:51:01] SuperNet Training INFO: iter: 30480/144360  CE: 5.1759  
[04/02 19:52:07] SuperNet Training INFO: iter: 30600/144360  CE: 5.1989  
[04/02 19:53:13] SuperNet Training INFO: iter: 30720/144360  CE: 5.2499  
[04/02 19:54:19] SuperNet Training INFO: iter: 30840/144360  CE: 5.1990  
[04/02 19:55:25] SuperNet Training INFO: iter: 30960/144360  CE: 5.4018  
[04/02 19:56:30] SuperNet Training INFO: iter: 31080/144360  CE: 5.1111  
[04/02 19:57:36] SuperNet Training INFO: iter: 31200/144360  CE: 5.3711  
[04/02 19:58:18] SuperNet Training INFO: --> epoch:  26/120  avg CE: 5.1863  lr: 0.10662875768741867  
[04/02 19:59:18] SuperNet Training INFO: iter: 31320/144360  CE: 5.0621  
[04/02 20:00:25] SuperNet Training INFO: iter: 31440/144360  CE: 5.3751  
[04/02 20:01:31] SuperNet Training INFO: iter: 31560/144360  CE: 5.0530  
[04/02 20:02:37] SuperNet Training INFO: iter: 31680/144360  CE: 5.1620  
[04/02 20:03:42] SuperNet Training INFO: iter: 31800/144360  CE: 5.2228  
[04/02 20:04:48] SuperNet Training INFO: iter: 31920/144360  CE: 5.3616  
[04/02 20:05:54] SuperNet Training INFO: iter: 32040/144360  CE: 5.0362  
[04/02 20:06:59] SuperNet Training INFO: iter: 32160/144360  CE: 5.2022  
[04/02 20:08:05] SuperNet Training INFO: iter: 32280/144360  CE: 5.2774  
[04/02 20:09:11] SuperNet Training INFO: iter: 32400/144360  CE: 5.2024  
[04/02 20:09:54] SuperNet Training INFO: --> epoch:  27/120  avg CE: 5.1274  lr: 0.10562435793600224  
[04/02 20:10:52] SuperNet Training INFO: iter: 32520/144360  CE: 4.9206  
[04/02 20:12:00] SuperNet Training INFO: iter: 32640/144360  CE: 5.1316  
[04/02 20:13:07] SuperNet Training INFO: iter: 32760/144360  CE: 5.1441  
[04/02 20:14:14] SuperNet Training INFO: iter: 32880/144360  CE: 5.0012  
[04/02 20:15:21] SuperNet Training INFO: iter: 33000/144360  CE: 5.1679  
[04/02 20:16:27] SuperNet Training INFO: iter: 33120/144360  CE: 5.1690  
[04/02 20:17:33] SuperNet Training INFO: iter: 33240/144360  CE: 5.3659  
[04/02 20:18:40] SuperNet Training INFO: iter: 33360/144360  CE: 4.8884  
[04/02 20:19:46] SuperNet Training INFO: iter: 33480/144360  CE: 5.1321  
[04/02 20:20:52] SuperNet Training INFO: iter: 33600/144360  CE: 4.6805  
[04/02 20:21:39] SuperNet Training INFO: --> epoch:  28/120  avg CE: 5.0641  lr: 0.10458868952864393  
[04/02 20:22:35] SuperNet Training INFO: iter: 33720/144360  CE: 4.7007  
[04/02 20:23:42] SuperNet Training INFO: iter: 33840/144360  CE: 5.0085  
[04/02 20:24:48] SuperNet Training INFO: iter: 33960/144360  CE: 4.8599  
[04/02 20:25:55] SuperNet Training INFO: iter: 34080/144360  CE: 5.0196  
[04/02 20:27:01] SuperNet Training INFO: iter: 34200/144360  CE: 4.8921  
[04/02 20:28:07] SuperNet Training INFO: iter: 34320/144360  CE: 5.1053  
[04/02 20:29:13] SuperNet Training INFO: iter: 34440/144360  CE: 4.9825  
[04/02 20:30:20] SuperNet Training INFO: iter: 34560/144360  CE: 4.9072  
[04/02 20:31:25] SuperNet Training INFO: iter: 34680/144360  CE: 5.1654  
[04/02 20:32:30] SuperNet Training INFO: iter: 34800/144360  CE: 4.7271  
[04/02 20:33:17] SuperNet Training INFO: --> epoch:  29/120  avg CE: 5.0015  lr: 0.10352246226073762  
[04/02 20:34:12] SuperNet Training INFO: iter: 34920/144360  CE: 4.9030  
[04/02 20:35:19] SuperNet Training INFO: iter: 35040/144360  CE: 4.8603  
[04/02 20:36:25] SuperNet Training INFO: iter: 35160/144360  CE: 5.0394  
[04/02 20:37:32] SuperNet Training INFO: iter: 35280/144360  CE: 4.9652  
[04/02 20:38:38] SuperNet Training INFO: iter: 35400/144360  CE: 4.8343  
[04/02 20:39:44] SuperNet Training INFO: iter: 35520/144360  CE: 4.9776  
[04/02 20:40:50] SuperNet Training INFO: iter: 35640/144360  CE: 5.1169  
[04/02 20:41:56] SuperNet Training INFO: iter: 35760/144360  CE: 4.7570  
[04/02 20:43:02] SuperNet Training INFO: iter: 35880/144360  CE: 5.0144  
[04/02 20:44:07] SuperNet Training INFO: iter: 36000/144360  CE: 4.8119  
[04/02 20:44:55] SuperNet Training INFO: --> epoch:  30/120  avg CE: 4.9593  lr: 0.10242640687119343  
[04/02 20:45:48] SuperNet Training INFO: iter: 36120/144360  CE: 4.9341  
[04/02 20:46:54] SuperNet Training INFO: iter: 36240/144360  CE: 5.2402  
[04/02 20:48:00] SuperNet Training INFO: iter: 36360/144360  CE: 4.9907  
[04/02 20:49:07] SuperNet Training INFO: iter: 36480/144360  CE: 5.1455  
[04/02 20:50:14] SuperNet Training INFO: iter: 36600/144360  CE: 5.0920  
[04/02 20:51:21] SuperNet Training INFO: iter: 36720/144360  CE: 4.8262  
[04/02 20:52:28] SuperNet Training INFO: iter: 36840/144360  CE: 4.7147  
[04/02 20:53:36] SuperNet Training INFO: iter: 36960/144360  CE: 4.9978  
[04/02 20:54:42] SuperNet Training INFO: iter: 37080/144360  CE: 5.0113  
[04/02 20:55:49] SuperNet Training INFO: iter: 37200/144360  CE: 4.8427  
[04/02 20:56:40] SuperNet Training INFO: --> epoch:  31/120  avg CE: 4.8924  lr: 0.10130127454162571  
[04/02 20:57:31] SuperNet Training INFO: iter: 37320/144360  CE: 4.8329  
[04/02 20:58:38] SuperNet Training INFO: iter: 37440/144360  CE: 4.7072  
[04/02 20:59:45] SuperNet Training INFO: iter: 37560/144360  CE: 4.9687  
[04/02 21:00:52] SuperNet Training INFO: iter: 37680/144360  CE: 4.8342  
[04/02 21:01:59] SuperNet Training INFO: iter: 37800/144360  CE: 4.9635  
[04/02 21:03:05] SuperNet Training INFO: iter: 37920/144360  CE: 4.9138  
[04/02 21:04:13] SuperNet Training INFO: iter: 38040/144360  CE: 4.9197  
[04/02 21:05:19] SuperNet Training INFO: iter: 38160/144360  CE: 4.9373  
[04/02 21:06:25] SuperNet Training INFO: iter: 38280/144360  CE: 4.9945  
[04/02 21:07:31] SuperNet Training INFO: iter: 38400/144360  CE: 4.7247  
[04/02 21:08:23] SuperNet Training INFO: --> epoch:  32/120  avg CE: 4.8538  lr: 0.10014783638153192  
[04/02 21:09:13] SuperNet Training INFO: iter: 38520/144360  CE: 4.7056  
[04/02 21:10:20] SuperNet Training INFO: iter: 38640/144360  CE: 5.3706  
[04/02 21:11:27] SuperNet Training INFO: iter: 38760/144360  CE: 4.9035  
[04/02 21:12:35] SuperNet Training INFO: iter: 38880/144360  CE: 4.8584  
[04/02 21:13:42] SuperNet Training INFO: iter: 39000/144360  CE: 4.9272  
[04/02 21:14:49] SuperNet Training INFO: iter: 39120/144360  CE: 4.7585  
[04/02 21:15:55] SuperNet Training INFO: iter: 39240/144360  CE: 4.8243  
[04/02 21:17:01] SuperNet Training INFO: iter: 39360/144360  CE: 4.8564  
[04/02 21:18:07] SuperNet Training INFO: iter: 39480/144360  CE: 4.8646  
[04/02 21:19:13] SuperNet Training INFO: iter: 39600/144360  CE: 5.0510  
[04/02 21:20:08] SuperNet Training INFO: --> epoch:  33/120  avg CE: 4.8018  lr: 0.09896688289981138  
[04/02 21:20:56] SuperNet Training INFO: iter: 39720/144360  CE: 4.7988  
[04/02 21:22:03] SuperNet Training INFO: iter: 39840/144360  CE: 4.8608  
[04/02 21:23:10] SuperNet Training INFO: iter: 39960/144360  CE: 4.6262  
[04/02 21:24:17] SuperNet Training INFO: iter: 40080/144360  CE: 4.3770  
[04/02 21:25:23] SuperNet Training INFO: iter: 40200/144360  CE: 5.0350  
[04/02 21:26:30] SuperNet Training INFO: iter: 40320/144360  CE: 4.5908  
[04/02 21:27:36] SuperNet Training INFO: iter: 40440/144360  CE: 4.7240  
[04/02 21:28:43] SuperNet Training INFO: iter: 40560/144360  CE: 4.8513  
[04/02 21:29:49] SuperNet Training INFO: iter: 40680/144360  CE: 4.6911  
[04/02 21:30:55] SuperNet Training INFO: iter: 40800/144360  CE: 4.8879  
[04/02 21:31:50] SuperNet Training INFO: --> epoch:  34/120  avg CE: 4.7438  lr: 0.09775922346299062  
[04/02 21:32:36] SuperNet Training INFO: iter: 40920/144360  CE: 4.7654  
[04/02 21:33:42] SuperNet Training INFO: iter: 41040/144360  CE: 4.6699  
[04/02 21:34:49] SuperNet Training INFO: iter: 41160/144360  CE: 4.6803  
[04/02 21:35:55] SuperNet Training INFO: iter: 41280/144360  CE: 4.4756  
[04/02 21:37:02] SuperNet Training INFO: iter: 41400/144360  CE: 4.8551  
[04/02 21:38:08] SuperNet Training INFO: iter: 41520/144360  CE: 4.5957  
[04/02 21:39:15] SuperNet Training INFO: iter: 41640/144360  CE: 4.4711  
[04/02 21:40:22] SuperNet Training INFO: iter: 41760/144360  CE: 4.6357  
[04/02 21:41:29] SuperNet Training INFO: iter: 41880/144360  CE: 4.9651  
[04/02 21:42:36] SuperNet Training INFO: iter: 42000/144360  CE: 4.8601  
[04/02 21:43:34] SuperNet Training INFO: --> epoch:  35/120  avg CE: 4.7083  lr: 0.09652568574052359  
[04/02 21:44:19] SuperNet Training INFO: iter: 42120/144360  CE: 4.9659  
[04/02 21:45:26] SuperNet Training INFO: iter: 42240/144360  CE: 4.6889  
[04/02 21:46:32] SuperNet Training INFO: iter: 42360/144360  CE: 4.8806  
[04/02 21:47:39] SuperNet Training INFO: iter: 42480/144360  CE: 4.7734  
[04/02 21:48:45] SuperNet Training INFO: iter: 42600/144360  CE: 4.5909  
[04/02 21:49:51] SuperNet Training INFO: iter: 42720/144360  CE: 4.6894  
[04/02 21:50:57] SuperNet Training INFO: iter: 42840/144360  CE: 4.6387  
[04/02 21:52:03] SuperNet Training INFO: iter: 42960/144360  CE: 4.7641  
[04/02 21:53:09] SuperNet Training INFO: iter: 43080/144360  CE: 4.4207  
[04/02 21:54:14] SuperNet Training INFO: iter: 43200/144360  CE: 4.5119  
[04/02 21:55:14] SuperNet Training INFO: --> epoch:  36/120  avg CE: 4.6568  lr: 0.09526711513754865  
[04/02 21:55:57] SuperNet Training INFO: iter: 43320/144360  CE: 4.6677  
[04/02 21:57:04] SuperNet Training INFO: iter: 43440/144360  CE: 4.7995  
[04/02 21:58:10] SuperNet Training INFO: iter: 43560/144360  CE: 4.6051  
[04/02 21:59:16] SuperNet Training INFO: iter: 43680/144360  CE: 4.8190  
[04/02 22:00:22] SuperNet Training INFO: iter: 43800/144360  CE: 4.4794  
[04/02 22:01:28] SuperNet Training INFO: iter: 43920/144360  CE: 4.8635  
[04/02 22:02:34] SuperNet Training INFO: iter: 44040/144360  CE: 4.7262  
[04/02 22:03:40] SuperNet Training INFO: iter: 44160/144360  CE: 4.7138  
[04/02 22:04:45] SuperNet Training INFO: iter: 44280/144360  CE: 4.5231  
[04/02 22:05:52] SuperNet Training INFO: iter: 44400/144360  CE: 5.0612  
[04/02 22:06:52] SuperNet Training INFO: --> epoch:  37/120  avg CE: 4.6203  lr: 0.0939843742154901  
[04/02 22:07:33] SuperNet Training INFO: iter: 44520/144360  CE: 4.7934  
[04/02 22:08:40] SuperNet Training INFO: iter: 44640/144360  CE: 4.5178  
[04/02 22:09:47] SuperNet Training INFO: iter: 44760/144360  CE: 4.4610  
[04/02 22:10:53] SuperNet Training INFO: iter: 44880/144360  CE: 4.7015  
[04/02 22:12:00] SuperNet Training INFO: iter: 45000/144360  CE: 4.7387  
[04/02 22:13:06] SuperNet Training INFO: iter: 45120/144360  CE: 4.6889  
[04/02 22:14:12] SuperNet Training INFO: iter: 45240/144360  CE: 4.5519  
[04/02 22:15:19] SuperNet Training INFO: iter: 45360/144360  CE: 4.7957  
[04/02 22:16:24] SuperNet Training INFO: iter: 45480/144360  CE: 4.7501  
[04/02 22:17:30] SuperNet Training INFO: iter: 45600/144360  CE: 4.6447  
[04/02 22:18:33] SuperNet Training INFO: --> epoch:  38/120  avg CE: 4.5746  lr: 0.0926783421009017  
[04/02 22:19:12] SuperNet Training INFO: iter: 45720/144360  CE: 4.5233  
[04/02 22:20:20] SuperNet Training INFO: iter: 45840/144360  CE: 4.6431  
[04/02 22:21:27] SuperNet Training INFO: iter: 45960/144360  CE: 4.4800  
[04/02 22:22:34] SuperNet Training INFO: iter: 46080/144360  CE: 4.4651  
[04/02 22:23:40] SuperNet Training INFO: iter: 46200/144360  CE: 4.6115  
[04/02 22:24:47] SuperNet Training INFO: iter: 46320/144360  CE: 4.2988  
[04/02 22:25:53] SuperNet Training INFO: iter: 46440/144360  CE: 4.4716  
[04/02 22:26:59] SuperNet Training INFO: iter: 46560/144360  CE: 4.8053  
[04/02 22:28:05] SuperNet Training INFO: iter: 46680/144360  CE: 4.6957  
[04/02 22:29:10] SuperNet Training INFO: iter: 46800/144360  CE: 4.5533  
[04/02 22:30:13] SuperNet Training INFO: --> epoch:  39/120  avg CE: 4.5462  lr: 0.09134991388295689  
[04/02 22:30:51] SuperNet Training INFO: iter: 46920/144360  CE: 4.4242  
[04/02 22:31:58] SuperNet Training INFO: iter: 47040/144360  CE: 4.4168  
[04/02 22:33:06] SuperNet Training INFO: iter: 47160/144360  CE: 4.8240  
[04/02 22:34:12] SuperNet Training INFO: iter: 47280/144360  CE: 4.4711  
[04/02 22:35:18] SuperNet Training INFO: iter: 47400/144360  CE: 4.5432  
[04/02 22:36:25] SuperNet Training INFO: iter: 47520/144360  CE: 4.6148  
[04/02 22:37:30] SuperNet Training INFO: iter: 47640/144360  CE: 4.4852  
[04/02 22:38:36] SuperNet Training INFO: iter: 47760/144360  CE: 4.4716  
[04/02 22:39:41] SuperNet Training INFO: iter: 47880/144360  CE: 4.3341  
[04/02 22:40:47] SuperNet Training INFO: iter: 48000/144360  CE: 4.3225  
[04/02 22:41:52] SuperNet Training INFO: iter: 48120/144360  CE: 4.6262  
[04/02 22:41:52] SuperNet Training INFO: --> epoch:  40/120  avg CE: 4.5098  lr: 0.0899999999999998  
[04/02 22:43:35] SuperNet Training INFO: iter: 48240/144360  CE: 4.1501  
[04/02 22:44:42] SuperNet Training INFO: iter: 48360/144360  CE: 4.3013  
[04/02 22:45:49] SuperNet Training INFO: iter: 48480/144360  CE: 4.3905  
[04/02 22:46:56] SuperNet Training INFO: iter: 48600/144360  CE: 4.2106  
[04/02 22:48:03] SuperNet Training INFO: iter: 48720/144360  CE: 4.7675  
[04/02 22:49:09] SuperNet Training INFO: iter: 48840/144360  CE: 4.4523  
[04/02 22:50:16] SuperNet Training INFO: iter: 48960/144360  CE: 4.3506  
[04/02 22:51:22] SuperNet Training INFO: iter: 49080/144360  CE: 4.6902  
[04/02 22:52:28] SuperNet Training INFO: iter: 49200/144360  CE: 4.5096  
[04/02 22:53:34] SuperNet Training INFO: iter: 49320/144360  CE: 4.3060  
[04/02 22:53:36] SuperNet Training INFO: --> epoch:  41/120  avg CE: 4.4616  lr: 0.08862952561557644  
[04/02 22:55:17] SuperNet Training INFO: iter: 49440/144360  CE: 4.2756  
[04/02 22:56:25] SuperNet Training INFO: iter: 49560/144360  CE: 4.3835  
[04/02 22:57:32] SuperNet Training INFO: iter: 49680/144360  CE: 4.5986  
[04/02 22:58:41] SuperNet Training INFO: iter: 49800/144360  CE: 4.3941  
[04/02 22:59:48] SuperNet Training INFO: iter: 49920/144360  CE: 4.2035  
[04/02 23:00:53] SuperNet Training INFO: iter: 50040/144360  CE: 4.3311  
[04/02 23:01:59] SuperNet Training INFO: iter: 50160/144360  CE: 4.4501  
[04/02 23:03:04] SuperNet Training INFO: iter: 50280/144360  CE: 4.2204  
[04/02 23:04:10] SuperNet Training INFO: iter: 50400/144360  CE: 4.4980  
[04/02 23:05:15] SuperNet Training INFO: iter: 50520/144360  CE: 4.2749  
[04/02 23:05:18] SuperNet Training INFO: --> epoch:  42/120  avg CE: 4.4211  lr: 0.08723942998437267  
[04/02 23:06:58] SuperNet Training INFO: iter: 50640/144360  CE: 4.3526  
[04/02 23:08:05] SuperNet Training INFO: iter: 50760/144360  CE: 4.5234  
[04/02 23:09:11] SuperNet Training INFO: iter: 50880/144360  CE: 4.6741  
[04/02 23:10:18] SuperNet Training INFO: iter: 51000/144360  CE: 4.3267  
[04/02 23:11:25] SuperNet Training INFO: iter: 51120/144360  CE: 4.5206  
[04/02 23:12:31] SuperNet Training INFO: iter: 51240/144360  CE: 4.4500  
[04/02 23:13:37] SuperNet Training INFO: iter: 51360/144360  CE: 4.3595  
[04/02 23:14:43] SuperNet Training INFO: iter: 51480/144360  CE: 4.3627  
[04/02 23:15:48] SuperNet Training INFO: iter: 51600/144360  CE: 4.2276  
[04/02 23:16:54] SuperNet Training INFO: iter: 51720/144360  CE: 4.2355  
[04/02 23:16:58] SuperNet Training INFO: --> epoch:  43/120  avg CE: 4.4035  lr: 0.08583066580849745  
[04/02 23:18:36] SuperNet Training INFO: iter: 51840/144360  CE: 4.3491  
[04/02 23:19:42] SuperNet Training INFO: iter: 51960/144360  CE: 4.2364  
[04/02 23:20:49] SuperNet Training INFO: iter: 52080/144360  CE: 4.1712  
[04/02 23:21:56] SuperNet Training INFO: iter: 52200/144360  CE: 4.3978  
[04/02 23:23:02] SuperNet Training INFO: iter: 52320/144360  CE: 4.3957  
[04/02 23:24:08] SuperNet Training INFO: iter: 52440/144360  CE: 4.3149  
[04/02 23:25:14] SuperNet Training INFO: iter: 52560/144360  CE: 4.2632  
[04/02 23:26:19] SuperNet Training INFO: iter: 52680/144360  CE: 4.3913  
[04/02 23:27:25] SuperNet Training INFO: iter: 52800/144360  CE: 4.3827  
[04/02 23:28:31] SuperNet Training INFO: iter: 52920/144360  CE: 4.2497  
[04/02 23:28:37] SuperNet Training INFO: --> epoch:  44/120  avg CE: 4.3649  lr: 0.08440419858454766  
[04/02 23:30:14] SuperNet Training INFO: iter: 53040/144360  CE: 4.5486  
[04/02 23:31:21] SuperNet Training INFO: iter: 53160/144360  CE: 4.5040  
[04/02 23:32:28] SuperNet Training INFO: iter: 53280/144360  CE: 4.4256  
[04/02 23:33:35] SuperNet Training INFO: iter: 53400/144360  CE: 4.0186  
[04/02 23:34:41] SuperNet Training INFO: iter: 53520/144360  CE: 4.2130  
[04/02 23:35:47] SuperNet Training INFO: iter: 53640/144360  CE: 4.3937  
[04/02 23:36:54] SuperNet Training INFO: iter: 53760/144360  CE: 4.4072  
[04/02 23:38:00] SuperNet Training INFO: iter: 53880/144360  CE: 4.5641  
[04/02 23:39:05] SuperNet Training INFO: iter: 54000/144360  CE: 3.9944  
[04/02 23:40:11] SuperNet Training INFO: iter: 54120/144360  CE: 4.4666  
[04/02 23:40:19] SuperNet Training INFO: --> epoch:  45/120  avg CE: 4.3471  lr: 0.0829610059419049  
[04/02 23:41:54] SuperNet Training INFO: iter: 54240/144360  CE: 4.3425  
[04/02 23:43:01] SuperNet Training INFO: iter: 54360/144360  CE: 4.2056  
[04/02 23:44:07] SuperNet Training INFO: iter: 54480/144360  CE: 4.0546  
[04/02 23:45:13] SuperNet Training INFO: iter: 54600/144360  CE: 4.3483  
[04/02 23:46:19] SuperNet Training INFO: iter: 54720/144360  CE: 3.9879  
[04/02 23:47:26] SuperNet Training INFO: iter: 54840/144360  CE: 4.3880  
[04/02 23:48:31] SuperNet Training INFO: iter: 54960/144360  CE: 4.3474  
[04/02 23:49:38] SuperNet Training INFO: iter: 55080/144360  CE: 4.1839  
[04/02 23:50:44] SuperNet Training INFO: iter: 55200/144360  CE: 4.4075  
[04/02 23:51:50] SuperNet Training INFO: iter: 55320/144360  CE: 4.0167  
[04/02 23:52:00] SuperNet Training INFO: --> epoch:  46/120  avg CE: 4.3125  lr: 0.08150207697271764  
[04/02 23:53:34] SuperNet Training INFO: iter: 55440/144360  CE: 4.3625  
[04/02 23:54:41] SuperNet Training INFO: iter: 55560/144360  CE: 4.1676  
[04/02 23:55:47] SuperNet Training INFO: iter: 55680/144360  CE: 4.2766  
[04/02 23:56:54] SuperNet Training INFO: iter: 55800/144360  CE: 4.3890  
[04/02 23:58:00] SuperNet Training INFO: iter: 55920/144360  CE: 4.2718  
[04/02 23:59:07] SuperNet Training INFO: iter: 56040/144360  CE: 4.2898  
[04/03 00:00:13] SuperNet Training INFO: iter: 56160/144360  CE: 4.1278  
[04/03 00:01:21] SuperNet Training INFO: iter: 56280/144360  CE: 3.8015  
[04/03 00:02:27] SuperNet Training INFO: iter: 56400/144360  CE: 3.9930  
[04/03 00:03:33] SuperNet Training INFO: iter: 56520/144360  CE: 4.2718  
[04/03 00:03:44] SuperNet Training INFO: --> epoch:  47/120  avg CE: 4.2804  lr: 0.08002841155402596  
[04/03 00:05:16] SuperNet Training INFO: iter: 56640/144360  CE: 4.3012  
[04/03 00:06:23] SuperNet Training INFO: iter: 56760/144360  CE: 4.3426  
[04/03 00:07:30] SuperNet Training INFO: iter: 56880/144360  CE: 4.5529  
[04/03 00:08:36] SuperNet Training INFO: iter: 57000/144360  CE: 4.3123  
[04/03 00:09:42] SuperNet Training INFO: iter: 57120/144360  CE: 4.4249  
[04/03 00:10:48] SuperNet Training INFO: iter: 57240/144360  CE: 4.1542  
[04/03 00:11:54] SuperNet Training INFO: iter: 57360/144360  CE: 4.2712  
[04/03 00:12:59] SuperNet Training INFO: iter: 57480/144360  CE: 4.2393  
[04/03 00:14:05] SuperNet Training INFO: iter: 57600/144360  CE: 4.3860  
[04/03 00:15:11] SuperNet Training INFO: iter: 57720/144360  CE: 4.4349  
[04/03 00:15:23] SuperNet Training INFO: --> epoch:  48/120  avg CE: 4.2664  lr: 0.07854101966249659  
[04/03 00:16:54] SuperNet Training INFO: iter: 57840/144360  CE: 4.1518  
[04/03 00:18:01] SuperNet Training INFO: iter: 57960/144360  CE: 4.4550  
[04/03 00:19:08] SuperNet Training INFO: iter: 58080/144360  CE: 4.3542  
[04/03 00:20:16] SuperNet Training INFO: iter: 58200/144360  CE: 4.0598  
[04/03 00:21:23] SuperNet Training INFO: iter: 58320/144360  CE: 4.4250  
[04/03 00:22:30] SuperNet Training INFO: iter: 58440/144360  CE: 4.1138  
[04/03 00:23:37] SuperNet Training INFO: iter: 58560/144360  CE: 4.4570  
[04/03 00:24:44] SuperNet Training INFO: iter: 58680/144360  CE: 4.4404  
[04/03 00:25:50] SuperNet Training INFO: iter: 58800/144360  CE: 3.9013  
[04/03 00:26:57] SuperNet Training INFO: iter: 58920/144360  CE: 4.4050  
[04/03 00:27:11] SuperNet Training INFO: --> epoch:  49/120  avg CE: 4.2373  lr: 0.07704092068223518  
[04/03 00:28:40] SuperNet Training INFO: iter: 59040/144360  CE: 4.1457  
[04/03 00:29:47] SuperNet Training INFO: iter: 59160/144360  CE: 4.0954  
[04/03 00:30:54] SuperNet Training INFO: iter: 59280/144360  CE: 3.9807  
[04/03 00:32:01] SuperNet Training INFO: iter: 59400/144360  CE: 3.9798  
[04/03 00:33:07] SuperNet Training INFO: iter: 59520/144360  CE: 4.0969  
[04/03 00:34:13] SuperNet Training INFO: iter: 59640/144360  CE: 4.2401  
[04/03 00:35:20] SuperNet Training INFO: iter: 59760/144360  CE: 4.1305  
[04/03 00:36:26] SuperNet Training INFO: iter: 59880/144360  CE: 4.0880  
[04/03 00:37:33] SuperNet Training INFO: iter: 60000/144360  CE: 4.0743  
[04/03 00:38:39] SuperNet Training INFO: iter: 60120/144360  CE: 4.1363  
[04/03 00:38:55] SuperNet Training INFO: --> epoch:  50/120  avg CE: 4.2061  lr: 0.07552914270615126  
[04/03 00:40:22] SuperNet Training INFO: iter: 60240/144360  CE: 4.2528  
[04/03 00:41:29] SuperNet Training INFO: iter: 60360/144360  CE: 4.3292  
[04/03 00:42:37] SuperNet Training INFO: iter: 60480/144360  CE: 4.3436  
[04/03 00:43:43] SuperNet Training INFO: iter: 60600/144360  CE: 3.9759  
[04/03 00:44:49] SuperNet Training INFO: iter: 60720/144360  CE: 4.3793  
[04/03 00:45:56] SuperNet Training INFO: iter: 60840/144360  CE: 4.0184  
[04/03 00:47:02] SuperNet Training INFO: iter: 60960/144360  CE: 4.3498  
[04/03 00:48:07] SuperNet Training INFO: iter: 61080/144360  CE: 4.2250  
[04/03 00:49:13] SuperNet Training INFO: iter: 61200/144360  CE: 4.1900  
[04/03 00:50:19] SuperNet Training INFO: iter: 61320/144360  CE: 4.4819  
[04/03 00:50:36] SuperNet Training INFO: --> epoch:  51/120  avg CE: 4.1807  lr: 0.0740067218313545  
[04/03 00:52:01] SuperNet Training INFO: iter: 61440/144360  CE: 4.4512  
[04/03 00:53:08] SuperNet Training INFO: iter: 61560/144360  CE: 4.1157  
[04/03 00:54:15] SuperNet Training INFO: iter: 61680/144360  CE: 3.8959  
[04/03 00:55:21] SuperNet Training INFO: iter: 61800/144360  CE: 4.3480  
[04/03 00:56:28] SuperNet Training INFO: iter: 61920/144360  CE: 4.2500  
[04/03 00:57:35] SuperNet Training INFO: iter: 62040/144360  CE: 4.3256  
[04/03 00:58:42] SuperNet Training INFO: iter: 62160/144360  CE: 4.1180  
[04/03 00:59:48] SuperNet Training INFO: iter: 62280/144360  CE: 3.7830  
[04/03 01:00:54] SuperNet Training INFO: iter: 62400/144360  CE: 4.2187  
[04/03 01:02:00] SuperNet Training INFO: iter: 62520/144360  CE: 4.2644  
[04/03 01:02:20] SuperNet Training INFO: --> epoch:  52/120  avg CE: 4.1613  lr: 0.07247470144906537  
[04/03 01:03:43] SuperNet Training INFO: iter: 62640/144360  CE: 4.3241  
[04/03 01:04:52] SuperNet Training INFO: iter: 62760/144360  CE: 3.9080  
[04/03 01:05:59] SuperNet Training INFO: iter: 62880/144360  CE: 4.3257  
[04/03 01:07:06] SuperNet Training INFO: iter: 63000/144360  CE: 4.2152  
[04/03 01:08:12] SuperNet Training INFO: iter: 63120/144360  CE: 3.8560  
[04/03 01:09:19] SuperNet Training INFO: iter: 63240/144360  CE: 3.9496  
[04/03 01:10:26] SuperNet Training INFO: iter: 63360/144360  CE: 3.9602  
[04/03 01:11:31] SuperNet Training INFO: iter: 63480/144360  CE: 3.9178  
[04/03 01:12:37] SuperNet Training INFO: iter: 63600/144360  CE: 4.4393  
[04/03 01:13:43] SuperNet Training INFO: iter: 63720/144360  CE: 4.1088  
[04/03 01:14:03] SuperNet Training INFO: --> epoch:  53/120  avg CE: 4.1332  lr: 0.07093413152952865  
[04/03 01:15:25] SuperNet Training INFO: iter: 63840/144360  CE: 4.0962  
[04/03 01:16:32] SuperNet Training INFO: iter: 63960/144360  CE: 4.1988  
[04/03 01:17:39] SuperNet Training INFO: iter: 64080/144360  CE: 4.2655  
[04/03 01:18:45] SuperNet Training INFO: iter: 64200/144360  CE: 4.2313  
[04/03 01:19:51] SuperNet Training INFO: iter: 64320/144360  CE: 4.0737  
[04/03 01:20:57] SuperNet Training INFO: iter: 64440/144360  CE: 4.1238  
[04/03 01:22:02] SuperNet Training INFO: iter: 64560/144360  CE: 4.1119  
[04/03 01:23:08] SuperNet Training INFO: iter: 64680/144360  CE: 3.9709  
[04/03 01:24:14] SuperNet Training INFO: iter: 64800/144360  CE: 3.7969  
[04/03 01:25:19] SuperNet Training INFO: iter: 64920/144360  CE: 3.8468  
[04/03 01:25:42] SuperNet Training INFO: --> epoch:  54/120  avg CE: 4.1125  lr: 0.06938606790241343  
[04/03 01:27:02] SuperNet Training INFO: iter: 65040/144360  CE: 4.6756  
[04/03 01:28:09] SuperNet Training INFO: iter: 65160/144360  CE: 4.0573  
[04/03 01:29:16] SuperNet Training INFO: iter: 65280/144360  CE: 4.3827  
[04/03 01:30:22] SuperNet Training INFO: iter: 65400/144360  CE: 3.8287  
[04/03 01:31:28] SuperNet Training INFO: iter: 65520/144360  CE: 3.9684  
[04/03 01:32:35] SuperNet Training INFO: iter: 65640/144360  CE: 3.7873  
[04/03 01:33:41] SuperNet Training INFO: iter: 65760/144360  CE: 4.0323  
[04/03 01:34:46] SuperNet Training INFO: iter: 65880/144360  CE: 3.8104  
[04/03 01:35:53] SuperNet Training INFO: iter: 66000/144360  CE: 4.0295  
[04/03 01:36:58] SuperNet Training INFO: iter: 66120/144360  CE: 4.1264  
[04/03 01:37:21] SuperNet Training INFO: --> epoch:  55/120  avg CE: 4.0834  lr: 0.06783157153320266  
[04/03 01:38:39] SuperNet Training INFO: iter: 66240/144360  CE: 3.8080  
[04/03 01:39:46] SuperNet Training INFO: iter: 66360/144360  CE: 3.8365  
[04/03 01:40:54] SuperNet Training INFO: iter: 66480/144360  CE: 3.9800  
[04/03 01:42:01] SuperNet Training INFO: iter: 66600/144360  CE: 4.3619  
[04/03 01:43:08] SuperNet Training INFO: iter: 66720/144360  CE: 4.0243  
[04/03 01:44:15] SuperNet Training INFO: iter: 66840/144360  CE: 3.7544  
[04/03 01:45:22] SuperNet Training INFO: iter: 66960/144360  CE: 3.9303  
[04/03 01:46:28] SuperNet Training INFO: iter: 67080/144360  CE: 4.0334  
[04/03 01:47:35] SuperNet Training INFO: iter: 67200/144360  CE: 3.9488  
[04/03 01:48:42] SuperNet Training INFO: iter: 67320/144360  CE: 4.1669  
[04/03 01:49:08] SuperNet Training INFO: --> epoch:  56/120  avg CE: 4.0580  lr: 0.06627170779605904  
[04/03 01:50:25] SuperNet Training INFO: iter: 67440/144360  CE: 4.1783  
[04/03 01:51:31] SuperNet Training INFO: iter: 67560/144360  CE: 4.3151  
[04/03 01:52:38] SuperNet Training INFO: iter: 67680/144360  CE: 3.8624  
[04/03 01:53:45] SuperNet Training INFO: iter: 67800/144360  CE: 4.1234  
[04/03 01:54:51] SuperNet Training INFO: iter: 67920/144360  CE: 4.1774  
[04/03 01:55:57] SuperNet Training INFO: iter: 68040/144360  CE: 4.2387  
[04/03 01:57:03] SuperNet Training INFO: iter: 68160/144360  CE: 3.7675  
[04/03 01:58:09] SuperNet Training INFO: iter: 68280/144360  CE: 3.8325  
[04/03 01:59:15] SuperNet Training INFO: iter: 68400/144360  CE: 4.0315  
[04/03 02:00:20] SuperNet Training INFO: iter: 68520/144360  CE: 4.0505  
[04/03 02:00:48] SuperNet Training INFO: --> epoch:  57/120  avg CE: 4.0447  lr: 0.06470754574367053  
[04/03 02:02:02] SuperNet Training INFO: iter: 68640/144360  CE: 3.9380  
[04/03 02:03:08] SuperNet Training INFO: iter: 68760/144360  CE: 3.9196  
[04/03 02:04:15] SuperNet Training INFO: iter: 68880/144360  CE: 3.9636  
[04/03 02:05:21] SuperNet Training INFO: iter: 69000/144360  CE: 4.0483  
[04/03 02:06:29] SuperNet Training INFO: iter: 69120/144360  CE: 3.9876  
[04/03 02:07:38] SuperNet Training INFO: iter: 69240/144360  CE: 4.0551  
[04/03 02:08:44] SuperNet Training INFO: iter: 69360/144360  CE: 4.1791  
[04/03 02:09:50] SuperNet Training INFO: iter: 69480/144360  CE: 3.7873  
[04/03 02:10:56] SuperNet Training INFO: iter: 69600/144360  CE: 4.2938  
[04/03 02:12:02] SuperNet Training INFO: iter: 69720/144360  CE: 3.8397  
[04/03 02:12:31] SuperNet Training INFO: --> epoch:  58/120  avg CE: 4.0197  lr: 0.06314015737457618  
[04/03 02:13:44] SuperNet Training INFO: iter: 69840/144360  CE: 4.1647  
[04/03 02:14:51] SuperNet Training INFO: iter: 69960/144360  CE: 4.1988  
[04/03 02:15:57] SuperNet Training INFO: iter: 70080/144360  CE: 4.0019  
[04/03 02:17:05] SuperNet Training INFO: iter: 70200/144360  CE: 4.1008  
[04/03 02:18:11] SuperNet Training INFO: iter: 70320/144360  CE: 4.1000  
[04/03 02:19:18] SuperNet Training INFO: iter: 70440/144360  CE: 3.8100  
[04/03 02:20:25] SuperNet Training INFO: iter: 70560/144360  CE: 4.2910  
[04/03 02:21:30] SuperNet Training INFO: iter: 70680/144360  CE: 4.0053  
[04/03 02:22:37] SuperNet Training INFO: iter: 70800/144360  CE: 3.7863  
[04/03 02:23:44] SuperNet Training INFO: iter: 70920/144360  CE: 3.7208  
[04/03 02:24:15] SuperNet Training INFO: --> epoch:  59/120  avg CE: 3.9977  lr: 0.061570616898472125  
[04/03 02:25:26] SuperNet Training INFO: iter: 71040/144360  CE: 3.8318  
[04/03 02:26:33] SuperNet Training INFO: iter: 71160/144360  CE: 3.7472  
[04/03 02:27:40] SuperNet Training INFO: iter: 71280/144360  CE: 3.9596  
[04/03 02:28:46] SuperNet Training INFO: iter: 71400/144360  CE: 4.0241  
[04/03 02:29:53] SuperNet Training INFO: iter: 71520/144360  CE: 4.0226  
[04/03 02:30:59] SuperNet Training INFO: iter: 71640/144360  CE: 4.0430  
[04/03 02:32:05] SuperNet Training INFO: iter: 71760/144360  CE: 3.9380  
[04/03 02:33:12] SuperNet Training INFO: iter: 71880/144360  CE: 4.2300  
[04/03 02:34:17] SuperNet Training INFO: iter: 72000/144360  CE: 3.8733  
[04/03 02:35:24] SuperNet Training INFO: iter: 72120/144360  CE: 3.9817  
[04/03 02:35:56] SuperNet Training INFO: --> epoch:  60/120  avg CE: 3.9818  lr: 0.05999999999999976  
[04/03 02:37:05] SuperNet Training INFO: iter: 72240/144360  CE: 3.9234  
[04/03 02:38:13] SuperNet Training INFO: iter: 72360/144360  CE: 4.1457  
[04/03 02:39:20] SuperNet Training INFO: iter: 72480/144360  CE: 3.8373  
[04/03 02:40:27] SuperNet Training INFO: iter: 72600/144360  CE: 3.8223  
[04/03 02:41:33] SuperNet Training INFO: iter: 72720/144360  CE: 3.6094  
[04/03 02:42:40] SuperNet Training INFO: iter: 72840/144360  CE: 3.8755  
[04/03 02:43:46] SuperNet Training INFO: iter: 72960/144360  CE: 3.8125  
[04/03 02:44:52] SuperNet Training INFO: iter: 73080/144360  CE: 4.0862  
[04/03 02:45:59] SuperNet Training INFO: iter: 73200/144360  CE: 3.8955  
[04/03 02:47:05] SuperNet Training INFO: iter: 73320/144360  CE: 4.2837  
[04/03 02:47:39] SuperNet Training INFO: --> epoch:  61/120  avg CE: 3.9534  lr: 0.058429383101527455  
[04/03 02:48:46] SuperNet Training INFO: iter: 73440/144360  CE: 3.8524  
[04/03 02:49:53] SuperNet Training INFO: iter: 73560/144360  CE: 3.4214  
[04/03 02:51:00] SuperNet Training INFO: iter: 73680/144360  CE: 4.0717  
[04/03 02:52:07] SuperNet Training INFO: iter: 73800/144360  CE: 4.1475  
[04/03 02:53:14] SuperNet Training INFO: iter: 73920/144360  CE: 4.1073  
[04/03 02:54:21] SuperNet Training INFO: iter: 74040/144360  CE: 4.1473  
[04/03 02:55:28] SuperNet Training INFO: iter: 74160/144360  CE: 4.3377  
[04/03 02:56:35] SuperNet Training INFO: iter: 74280/144360  CE: 3.9776  
[04/03 02:57:41] SuperNet Training INFO: iter: 74400/144360  CE: 4.1715  
[04/03 02:58:48] SuperNet Training INFO: iter: 74520/144360  CE: 3.7021  
[04/03 02:59:23] SuperNet Training INFO: --> epoch:  62/120  avg CE: 3.9373  lr: 0.05685984262542327  
[04/03 03:00:30] SuperNet Training INFO: iter: 74640/144360  CE: 4.2190  
[04/03 03:01:36] SuperNet Training INFO: iter: 74760/144360  CE: 3.9232  
[04/03 03:02:43] SuperNet Training INFO: iter: 74880/144360  CE: 4.1782  
[04/03 03:03:50] SuperNet Training INFO: iter: 75000/144360  CE: 3.9869  
[04/03 03:04:56] SuperNet Training INFO: iter: 75120/144360  CE: 3.8605  
[04/03 03:06:02] SuperNet Training INFO: iter: 75240/144360  CE: 3.8098  
[04/03 03:07:08] SuperNet Training INFO: iter: 75360/144360  CE: 3.9336  
[04/03 03:08:14] SuperNet Training INFO: iter: 75480/144360  CE: 4.1666  
[04/03 03:09:20] SuperNet Training INFO: iter: 75600/144360  CE: 3.9192  
[04/03 03:10:29] SuperNet Training INFO: iter: 75720/144360  CE: 3.8905  
[04/03 03:11:07] SuperNet Training INFO: --> epoch:  63/120  avg CE: 3.9210  lr: 0.05529245425632924  
[04/03 03:12:11] SuperNet Training INFO: iter: 75840/144360  CE: 3.8874  
[04/03 03:13:18] SuperNet Training INFO: iter: 75960/144360  CE: 3.9138  
[04/03 03:14:25] SuperNet Training INFO: iter: 76080/144360  CE: 3.7451  
[04/03 03:15:32] SuperNet Training INFO: iter: 76200/144360  CE: 3.8665  
[04/03 03:16:37] SuperNet Training INFO: iter: 76320/144360  CE: 3.9668  
[04/03 03:17:43] SuperNet Training INFO: iter: 76440/144360  CE: 3.7989  
[04/03 03:18:48] SuperNet Training INFO: iter: 76560/144360  CE: 3.6733  
[04/03 03:19:54] SuperNet Training INFO: iter: 76680/144360  CE: 3.9172  
[04/03 03:21:00] SuperNet Training INFO: iter: 76800/144360  CE: 3.7584  
[04/03 03:22:05] SuperNet Training INFO: iter: 76920/144360  CE: 4.0086  
[04/03 03:22:44] SuperNet Training INFO: --> epoch:  64/120  avg CE: 3.8961  lr: 0.05372829220394078  
[04/03 03:23:47] SuperNet Training INFO: iter: 77040/144360  CE: 3.8086  
[04/03 03:24:54] SuperNet Training INFO: iter: 77160/144360  CE: 3.9834  
[04/03 03:26:02] SuperNet Training INFO: iter: 77280/144360  CE: 4.0898  
[04/03 03:27:08] SuperNet Training INFO: iter: 77400/144360  CE: 3.6403  
[04/03 03:28:14] SuperNet Training INFO: iter: 77520/144360  CE: 3.9377  
[04/03 03:29:21] SuperNet Training INFO: iter: 77640/144360  CE: 4.0355  
[04/03 03:30:26] SuperNet Training INFO: iter: 77760/144360  CE: 3.8395  
[04/03 03:31:33] SuperNet Training INFO: iter: 77880/144360  CE: 3.8905  
[04/03 03:32:38] SuperNet Training INFO: iter: 78000/144360  CE: 4.2984  
[04/03 03:33:45] SuperNet Training INFO: iter: 78120/144360  CE: 3.8239  
[04/03 03:34:25] SuperNet Training INFO: --> epoch:  65/120  avg CE: 3.8735  lr: 0.05216842846679694  
[04/03 03:35:26] SuperNet Training INFO: iter: 78240/144360  CE: 3.9539  
[04/03 03:36:32] SuperNet Training INFO: iter: 78360/144360  CE: 3.8347  
[04/03 03:37:40] SuperNet Training INFO: iter: 78480/144360  CE: 3.8993  
[04/03 03:38:46] SuperNet Training INFO: iter: 78600/144360  CE: 3.9475  
[04/03 03:39:54] SuperNet Training INFO: iter: 78720/144360  CE: 3.6060  
[04/03 03:41:01] SuperNet Training INFO: iter: 78840/144360  CE: 3.7804  
[04/03 03:42:07] SuperNet Training INFO: iter: 78960/144360  CE: 4.0625  
[04/03 03:43:14] SuperNet Training INFO: iter: 79080/144360  CE: 3.9722  
[04/03 03:44:20] SuperNet Training INFO: iter: 79200/144360  CE: 3.7831  
[04/03 03:45:26] SuperNet Training INFO: iter: 79320/144360  CE: 3.9342  
[04/03 03:46:08] SuperNet Training INFO: --> epoch:  66/120  avg CE: 3.8550  lr: 0.05061393209758616  
[04/03 03:47:08] SuperNet Training INFO: iter: 79440/144360  CE: 4.2291  
[04/03 03:48:15] SuperNet Training INFO: iter: 79560/144360  CE: 3.7205  
[04/03 03:49:22] SuperNet Training INFO: iter: 79680/144360  CE: 3.8783  
[04/03 03:50:29] SuperNet Training INFO: iter: 79800/144360  CE: 3.9064  
[04/03 03:51:35] SuperNet Training INFO: iter: 79920/144360  CE: 3.8663  
[04/03 03:52:40] SuperNet Training INFO: iter: 80040/144360  CE: 3.7839  
[04/03 03:53:46] SuperNet Training INFO: iter: 80160/144360  CE: 3.9654  
[04/03 03:54:52] SuperNet Training INFO: iter: 80280/144360  CE: 3.8435  
[04/03 03:55:58] SuperNet Training INFO: iter: 80400/144360  CE: 3.7545  
[04/03 03:57:03] SuperNet Training INFO: iter: 80520/144360  CE: 4.0775  
[04/03 03:57:47] SuperNet Training INFO: --> epoch:  67/120  avg CE: 3.8421  lr: 0.04906586847047105  
[04/03 03:58:44] SuperNet Training INFO: iter: 80640/144360  CE: 3.5614  
[04/03 03:59:51] SuperNet Training INFO: iter: 80760/144360  CE: 4.0058  
[04/03 04:00:57] SuperNet Training INFO: iter: 80880/144360  CE: 3.6301  
[04/03 04:02:04] SuperNet Training INFO: iter: 81000/144360  CE: 3.8720  
[04/03 04:03:10] SuperNet Training INFO: iter: 81120/144360  CE: 3.8676  
[04/03 04:04:17] SuperNet Training INFO: iter: 81240/144360  CE: 3.7676  
[04/03 04:05:23] SuperNet Training INFO: iter: 81360/144360  CE: 3.8547  
[04/03 04:06:30] SuperNet Training INFO: iter: 81480/144360  CE: 3.8436  
[04/03 04:07:36] SuperNet Training INFO: iter: 81600/144360  CE: 3.8907  
[04/03 04:08:42] SuperNet Training INFO: iter: 81720/144360  CE: 3.9061  
[04/03 04:09:28] SuperNet Training INFO: --> epoch:  68/120  avg CE: 3.8207  lr: 0.04752529855093431  
[04/03 04:10:25] SuperNet Training INFO: iter: 81840/144360  CE: 3.6962  
[04/03 04:11:32] SuperNet Training INFO: iter: 81960/144360  CE: 3.6952  
[04/03 04:12:41] SuperNet Training INFO: iter: 82080/144360  CE: 3.7868  
[04/03 04:13:49] SuperNet Training INFO: iter: 82200/144360  CE: 3.8302  
[04/03 04:14:56] SuperNet Training INFO: iter: 82320/144360  CE: 3.8482  
[04/03 04:16:03] SuperNet Training INFO: iter: 82440/144360  CE: 4.2747  
[04/03 04:17:10] SuperNet Training INFO: iter: 82560/144360  CE: 3.7526  
[04/03 04:18:17] SuperNet Training INFO: iter: 82680/144360  CE: 3.7387  
[04/03 04:19:23] SuperNet Training INFO: iter: 82800/144360  CE: 3.8314  
[04/03 04:20:29] SuperNet Training INFO: iter: 82920/144360  CE: 3.7911  
[04/03 04:21:16] SuperNet Training INFO: --> epoch:  69/120  avg CE: 3.8061  lr: 0.04599327816864548  
[04/03 04:22:12] SuperNet Training INFO: iter: 83040/144360  CE: 3.7502  
[04/03 04:23:18] SuperNet Training INFO: iter: 83160/144360  CE: 3.8885  
[04/03 04:24:26] SuperNet Training INFO: iter: 83280/144360  CE: 4.0494  
[04/03 04:25:33] SuperNet Training INFO: iter: 83400/144360  CE: 3.8875  
[04/03 04:26:40] SuperNet Training INFO: iter: 83520/144360  CE: 3.9824  
[04/03 04:27:47] SuperNet Training INFO: iter: 83640/144360  CE: 3.4203  
[04/03 04:28:53] SuperNet Training INFO: iter: 83760/144360  CE: 3.6968  
[04/03 04:30:00] SuperNet Training INFO: iter: 83880/144360  CE: 3.8213  
[04/03 04:31:06] SuperNet Training INFO: iter: 84000/144360  CE: 3.9283  
[04/03 04:32:12] SuperNet Training INFO: iter: 84120/144360  CE: 3.7614  
[04/03 04:33:02] SuperNet Training INFO: --> epoch:  70/120  avg CE: 3.7866  lr: 0.04447085729384866  
[04/03 04:33:55] SuperNet Training INFO: iter: 84240/144360  CE: 3.6365  
[04/03 04:35:02] SuperNet Training INFO: iter: 84360/144360  CE: 3.4613  
[04/03 04:36:10] SuperNet Training INFO: iter: 84480/144360  CE: 3.7962  
[04/03 04:37:17] SuperNet Training INFO: iter: 84600/144360  CE: 3.9471  
[04/03 04:38:24] SuperNet Training INFO: iter: 84720/144360  CE: 3.7708  
[04/03 04:39:30] SuperNet Training INFO: iter: 84840/144360  CE: 3.6957  
[04/03 04:40:36] SuperNet Training INFO: iter: 84960/144360  CE: 3.9134  
[04/03 04:41:43] SuperNet Training INFO: iter: 85080/144360  CE: 3.5906  
[04/03 04:42:50] SuperNet Training INFO: iter: 85200/144360  CE: 3.6522  
[04/03 04:43:56] SuperNet Training INFO: iter: 85320/144360  CE: 3.5040  
[04/03 04:44:47] SuperNet Training INFO: --> epoch:  71/120  avg CE: 3.7737  lr: 0.04295907931776456  
[04/03 04:45:39] SuperNet Training INFO: iter: 85440/144360  CE: 3.7496  
[04/03 04:46:46] SuperNet Training INFO: iter: 85560/144360  CE: 3.8832  
[04/03 04:47:55] SuperNet Training INFO: iter: 85680/144360  CE: 3.5793  
[04/03 04:49:03] SuperNet Training INFO: iter: 85800/144360  CE: 3.6154  
[04/03 04:50:09] SuperNet Training INFO: iter: 85920/144360  CE: 3.6836  
[04/03 04:51:16] SuperNet Training INFO: iter: 86040/144360  CE: 3.7687  
[04/03 04:52:21] SuperNet Training INFO: iter: 86160/144360  CE: 3.7611  
[04/03 04:53:27] SuperNet Training INFO: iter: 86280/144360  CE: 3.8737  
[04/03 04:54:32] SuperNet Training INFO: iter: 86400/144360  CE: 3.5699  
[04/03 04:55:37] SuperNet Training INFO: iter: 86520/144360  CE: 3.9511  
[04/03 04:56:30] SuperNet Training INFO: --> epoch:  72/120  avg CE: 3.7537  lr: 0.04145898033750296  
[04/03 04:57:19] SuperNet Training INFO: iter: 86640/144360  CE: 3.6959  
[04/03 04:58:27] SuperNet Training INFO: iter: 86760/144360  CE: 3.7136  
[04/03 04:59:34] SuperNet Training INFO: iter: 86880/144360  CE: 3.7964  
[04/03 05:00:41] SuperNet Training INFO: iter: 87000/144360  CE: 3.6454  
[04/03 05:01:47] SuperNet Training INFO: iter: 87120/144360  CE: 3.2904  
[04/03 05:02:53] SuperNet Training INFO: iter: 87240/144360  CE: 3.6195  
[04/03 05:03:59] SuperNet Training INFO: iter: 87360/144360  CE: 3.8918  
[04/03 05:05:05] SuperNet Training INFO: iter: 87480/144360  CE: 3.8268  
[04/03 05:06:11] SuperNet Training INFO: iter: 87600/144360  CE: 3.6015  
[04/03 05:07:17] SuperNet Training INFO: iter: 87720/144360  CE: 3.8416  
[04/03 05:08:11] SuperNet Training INFO: --> epoch:  73/120  avg CE: 3.7318  lr: 0.03997158844597365  
[04/03 05:08:59] SuperNet Training INFO: iter: 87840/144360  CE: 3.5369  
[04/03 05:10:07] SuperNet Training INFO: iter: 87960/144360  CE: 3.7949  
[04/03 05:11:14] SuperNet Training INFO: iter: 88080/144360  CE: 3.5431  
[04/03 05:12:20] SuperNet Training INFO: iter: 88200/144360  CE: 3.7513  
[04/03 05:13:26] SuperNet Training INFO: iter: 88320/144360  CE: 3.6661  
[04/03 05:14:32] SuperNet Training INFO: iter: 88440/144360  CE: 3.7276  
[04/03 05:15:40] SuperNet Training INFO: iter: 88560/144360  CE: 3.5591  
[04/03 05:16:47] SuperNet Training INFO: iter: 88680/144360  CE: 3.5979  
[04/03 05:17:52] SuperNet Training INFO: iter: 88800/144360  CE: 3.5708  
[04/03 05:18:58] SuperNet Training INFO: iter: 88920/144360  CE: 4.1716  
[04/03 05:19:53] SuperNet Training INFO: --> epoch:  74/120  avg CE: 3.7161  lr: 0.03849792302728192  
[04/03 05:20:40] SuperNet Training INFO: iter: 89040/144360  CE: 3.6616  
[04/03 05:21:47] SuperNet Training INFO: iter: 89160/144360  CE: 3.5745  
[04/03 05:22:54] SuperNet Training INFO: iter: 89280/144360  CE: 3.6241  
[04/03 05:24:01] SuperNet Training INFO: iter: 89400/144360  CE: 3.7969  
[04/03 05:25:07] SuperNet Training INFO: iter: 89520/144360  CE: 3.8334  
[04/03 05:26:14] SuperNet Training INFO: iter: 89640/144360  CE: 3.7507  
[04/03 05:27:20] SuperNet Training INFO: iter: 89760/144360  CE: 3.9711  
[04/03 05:28:26] SuperNet Training INFO: iter: 89880/144360  CE: 3.8473  
[04/03 05:29:33] SuperNet Training INFO: iter: 90000/144360  CE: 4.0324  
[04/03 05:30:40] SuperNet Training INFO: iter: 90120/144360  CE: 3.4551  
[04/03 05:31:38] SuperNet Training INFO: --> epoch:  75/120  avg CE: 3.7043  lr: 0.03703899405809455  
[04/03 05:32:23] SuperNet Training INFO: iter: 90240/144360  CE: 3.6981  
[04/03 05:33:30] SuperNet Training INFO: iter: 90360/144360  CE: 3.7432  
[04/03 05:34:37] SuperNet Training INFO: iter: 90480/144360  CE: 3.9182  
[04/03 05:35:44] SuperNet Training INFO: iter: 90600/144360  CE: 3.6375  
[04/03 05:36:51] SuperNet Training INFO: iter: 90720/144360  CE: 3.4375  
[04/03 05:37:57] SuperNet Training INFO: iter: 90840/144360  CE: 3.8655  
[04/03 05:39:04] SuperNet Training INFO: iter: 90960/144360  CE: 3.7689  
[04/03 05:40:10] SuperNet Training INFO: iter: 91080/144360  CE: 3.8091  
[04/03 05:41:17] SuperNet Training INFO: iter: 91200/144360  CE: 3.7936  
[04/03 05:42:23] SuperNet Training INFO: iter: 91320/144360  CE: 3.7026  
[04/03 05:43:22] SuperNet Training INFO: --> epoch:  76/120  avg CE: 3.6957  lr: 0.035595801415451815  
[04/03 05:44:05] SuperNet Training INFO: iter: 91440/144360  CE: 4.0779  
[04/03 05:45:11] SuperNet Training INFO: iter: 91560/144360  CE: 3.9146  
[04/03 05:46:18] SuperNet Training INFO: iter: 91680/144360  CE: 3.6550  
[04/03 05:47:24] SuperNet Training INFO: iter: 91800/144360  CE: 3.5842  
[04/03 05:48:30] SuperNet Training INFO: iter: 91920/144360  CE: 3.2455  
[04/03 05:49:36] SuperNet Training INFO: iter: 92040/144360  CE: 3.7971  
[04/03 05:50:43] SuperNet Training INFO: iter: 92160/144360  CE: 3.8028  
[04/03 05:51:49] SuperNet Training INFO: iter: 92280/144360  CE: 3.8174  
[04/03 05:52:56] SuperNet Training INFO: iter: 92400/144360  CE: 3.8063  
[04/03 05:54:02] SuperNet Training INFO: iter: 92520/144360  CE: 3.5355  
[04/03 05:55:02] SuperNet Training INFO: --> epoch:  77/120  avg CE: 3.6879  lr: 0.03416933419150217  
[04/03 05:55:43] SuperNet Training INFO: iter: 92640/144360  CE: 3.5142  
[04/03 05:56:51] SuperNet Training INFO: iter: 92760/144360  CE: 3.5775  
[04/03 05:57:58] SuperNet Training INFO: iter: 92880/144360  CE: 3.7332  
[04/03 05:59:05] SuperNet Training INFO: iter: 93000/144360  CE: 3.9246  
[04/03 06:00:11] SuperNet Training INFO: iter: 93120/144360  CE: 3.7545  
[04/03 06:01:18] SuperNet Training INFO: iter: 93240/144360  CE: 3.3018  
[04/03 06:02:24] SuperNet Training INFO: iter: 93360/144360  CE: 3.7098  
[04/03 06:03:31] SuperNet Training INFO: iter: 93480/144360  CE: 3.6434  
[04/03 06:04:37] SuperNet Training INFO: iter: 93600/144360  CE: 3.7375  
[04/03 06:05:44] SuperNet Training INFO: iter: 93720/144360  CE: 3.5241  
[04/03 06:06:45] SuperNet Training INFO: --> epoch:  78/120  avg CE: 3.6708  lr: 0.03276057001562702  
[04/03 06:07:25] SuperNet Training INFO: iter: 93840/144360  CE: 3.6948  
[04/03 06:08:32] SuperNet Training INFO: iter: 93960/144360  CE: 3.4918  
[04/03 06:09:39] SuperNet Training INFO: iter: 94080/144360  CE: 3.4279  
[04/03 06:10:46] SuperNet Training INFO: iter: 94200/144360  CE: 3.5991  
[04/03 06:11:52] SuperNet Training INFO: iter: 94320/144360  CE: 3.5112  
[04/03 06:12:59] SuperNet Training INFO: iter: 94440/144360  CE: 3.5437  
[04/03 06:14:05] SuperNet Training INFO: iter: 94560/144360  CE: 3.5836  
[04/03 06:15:11] SuperNet Training INFO: iter: 94680/144360  CE: 3.9122  
[04/03 06:16:18] SuperNet Training INFO: iter: 94800/144360  CE: 3.7751  
[04/03 06:17:24] SuperNet Training INFO: iter: 94920/144360  CE: 3.5216  
[04/03 06:18:30] SuperNet Training INFO: --> epoch:  79/120  avg CE: 3.6413  lr: 0.031370474384423336  
[04/03 06:19:08] SuperNet Training INFO: iter: 95040/144360  CE: 3.7093  
[04/03 06:20:17] SuperNet Training INFO: iter: 95160/144360  CE: 3.8915  
[04/03 06:21:24] SuperNet Training INFO: iter: 95280/144360  CE: 3.5659  
[04/03 06:22:30] SuperNet Training INFO: iter: 95400/144360  CE: 3.5071  
[04/03 06:23:36] SuperNet Training INFO: iter: 95520/144360  CE: 3.3601  
[04/03 06:24:42] SuperNet Training INFO: iter: 95640/144360  CE: 3.6885  
[04/03 06:25:48] SuperNet Training INFO: iter: 95760/144360  CE: 3.3800  
[04/03 06:26:54] SuperNet Training INFO: iter: 95880/144360  CE: 3.7796  
[04/03 06:28:00] SuperNet Training INFO: iter: 96000/144360  CE: 3.7887  
[04/03 06:29:05] SuperNet Training INFO: iter: 96120/144360  CE: 3.5282  
[04/03 06:30:11] SuperNet Training INFO: iter: 96240/144360  CE: 3.3251  
[04/03 06:30:11] SuperNet Training INFO: --> epoch:  80/120  avg CE: 3.6238  lr: 0.02999999999999983  
[04/03 06:31:54] SuperNet Training INFO: iter: 96360/144360  CE: 3.6127  
[04/03 06:33:01] SuperNet Training INFO: iter: 96480/144360  CE: 3.2056  
[04/03 06:34:08] SuperNet Training INFO: iter: 96600/144360  CE: 3.6025  
[04/03 06:35:14] SuperNet Training INFO: iter: 96720/144360  CE: 3.6700  
[04/03 06:36:20] SuperNet Training INFO: iter: 96840/144360  CE: 3.4998  
[04/03 06:37:25] SuperNet Training INFO: iter: 96960/144360  CE: 3.4981  
[04/03 06:38:31] SuperNet Training INFO: iter: 97080/144360  CE: 3.4228  
[04/03 06:39:36] SuperNet Training INFO: iter: 97200/144360  CE: 3.6806  
[04/03 06:40:42] SuperNet Training INFO: iter: 97320/144360  CE: 4.2386  
[04/03 06:41:48] SuperNet Training INFO: iter: 97440/144360  CE: 3.5719  
[04/03 06:41:49] SuperNet Training INFO: --> epoch:  81/120  avg CE: 3.6151  lr: 0.028650086117042926  
[04/03 06:43:31] SuperNet Training INFO: iter: 97560/144360  CE: 3.8408  
[04/03 06:44:38] SuperNet Training INFO: iter: 97680/144360  CE: 3.6493  
[04/03 06:45:44] SuperNet Training INFO: iter: 97800/144360  CE: 3.4289  
[04/03 06:46:51] SuperNet Training INFO: iter: 97920/144360  CE: 3.6327  
[04/03 06:47:58] SuperNet Training INFO: iter: 98040/144360  CE: 3.5806  
[04/03 06:49:04] SuperNet Training INFO: iter: 98160/144360  CE: 3.5966  
[04/03 06:50:11] SuperNet Training INFO: iter: 98280/144360  CE: 3.5685  
[04/03 06:51:17] SuperNet Training INFO: iter: 98400/144360  CE: 3.4768  
[04/03 06:52:24] SuperNet Training INFO: iter: 98520/144360  CE: 3.7922  
[04/03 06:53:30] SuperNet Training INFO: iter: 98640/144360  CE: 3.7361  
[04/03 06:53:33] SuperNet Training INFO: --> epoch:  82/120  avg CE: 3.5992  lr: 0.027321657899098132  
[04/03 06:55:14] SuperNet Training INFO: iter: 98760/144360  CE: 3.2811  
[04/03 06:56:21] SuperNet Training INFO: iter: 98880/144360  CE: 3.6296  
[04/03 06:57:29] SuperNet Training INFO: iter: 99000/144360  CE: 3.3438  
[04/03 06:58:36] SuperNet Training INFO: iter: 99120/144360  CE: 3.6228  
[04/03 06:59:43] SuperNet Training INFO: iter: 99240/144360  CE: 3.5965  
[04/03 07:00:49] SuperNet Training INFO: iter: 99360/144360  CE: 3.8383  
[04/03 07:01:55] SuperNet Training INFO: iter: 99480/144360  CE: 3.7119  
[04/03 07:03:03] SuperNet Training INFO: iter: 99600/144360  CE: 3.6505  
[04/03 07:04:09] SuperNet Training INFO: iter: 99720/144360  CE: 3.4861  
[04/03 07:05:16] SuperNet Training INFO: iter: 99840/144360  CE: 3.5328  
[04/03 07:05:20] SuperNet Training INFO: --> epoch:  83/120  avg CE: 3.5858  lr: 0.0260156257845098  
[04/03 07:06:59] SuperNet Training INFO: iter: 99960/144360  CE: 3.7531  
[04/03 07:08:06] SuperNet Training INFO: iter: 100080/144360  CE: 3.4317  
[04/03 07:09:14] SuperNet Training INFO: iter: 100200/144360  CE: 3.9736  
[04/03 07:10:20] SuperNet Training INFO: iter: 100320/144360  CE: 3.5722  
[04/03 07:11:27] SuperNet Training INFO: iter: 100440/144360  CE: 3.7120  
[04/03 07:12:34] SuperNet Training INFO: iter: 100560/144360  CE: 3.6559  
[04/03 07:13:41] SuperNet Training INFO: iter: 100680/144360  CE: 3.2760  
[04/03 07:14:47] SuperNet Training INFO: iter: 100800/144360  CE: 3.4936  
[04/03 07:15:54] SuperNet Training INFO: iter: 100920/144360  CE: 3.4506  
[04/03 07:17:00] SuperNet Training INFO: iter: 101040/144360  CE: 3.5534  
[04/03 07:17:06] SuperNet Training INFO: --> epoch:  84/120  avg CE: 3.5638  lr: 0.02473288486245143  
[04/03 07:18:43] SuperNet Training INFO: iter: 101160/144360  CE: 3.4801  
[04/03 07:19:50] SuperNet Training INFO: iter: 101280/144360  CE: 3.5622  
[04/03 07:20:57] SuperNet Training INFO: iter: 101400/144360  CE: 3.7321  
[04/03 07:22:06] SuperNet Training INFO: iter: 101520/144360  CE: 3.1941  
[04/03 07:23:13] SuperNet Training INFO: iter: 101640/144360  CE: 3.8277  
[04/03 07:24:19] SuperNet Training INFO: iter: 101760/144360  CE: 3.5676  
[04/03 07:25:25] SuperNet Training INFO: iter: 101880/144360  CE: 3.6395  
[04/03 07:26:31] SuperNet Training INFO: iter: 102000/144360  CE: 3.6204  
[04/03 07:27:37] SuperNet Training INFO: iter: 102120/144360  CE: 3.7043  
[04/03 07:28:44] SuperNet Training INFO: iter: 102240/144360  CE: 3.5285  
[04/03 07:28:52] SuperNet Training INFO: --> epoch:  85/120  avg CE: 3.5525  lr: 0.023474314259476523  
[04/03 07:30:27] SuperNet Training INFO: iter: 102360/144360  CE: 3.8576  
[04/03 07:31:34] SuperNet Training INFO: iter: 102480/144360  CE: 3.4870  
[04/03 07:32:41] SuperNet Training INFO: iter: 102600/144360  CE: 3.8289  
[04/03 07:33:47] SuperNet Training INFO: iter: 102720/144360  CE: 3.4702  
[04/03 07:34:54] SuperNet Training INFO: iter: 102840/144360  CE: 3.5795  
[04/03 07:36:00] SuperNet Training INFO: iter: 102960/144360  CE: 3.5625  
[04/03 07:37:06] SuperNet Training INFO: iter: 103080/144360  CE: 3.5870  
[04/03 07:38:12] SuperNet Training INFO: iter: 103200/144360  CE: 3.5997  
[04/03 07:39:18] SuperNet Training INFO: iter: 103320/144360  CE: 3.3291  
[04/03 07:40:24] SuperNet Training INFO: iter: 103440/144360  CE: 3.7190  
[04/03 07:40:33] SuperNet Training INFO: --> epoch:  86/120  avg CE: 3.5414  lr: 0.02224077653700959  
[04/03 07:42:08] SuperNet Training INFO: iter: 103560/144360  CE: 3.4118  
[04/03 07:43:15] SuperNet Training INFO: iter: 103680/144360  CE: 3.2311  
[04/03 07:44:22] SuperNet Training INFO: iter: 103800/144360  CE: 3.4308  
[04/03 07:45:29] SuperNet Training INFO: iter: 103920/144360  CE: 3.6928  
[04/03 07:46:36] SuperNet Training INFO: iter: 104040/144360  CE: 3.6569  
[04/03 07:47:44] SuperNet Training INFO: iter: 104160/144360  CE: 3.4945  
[04/03 07:48:51] SuperNet Training INFO: iter: 104280/144360  CE: 3.5763  
[04/03 07:49:57] SuperNet Training INFO: iter: 104400/144360  CE: 3.6038  
[04/03 07:51:04] SuperNet Training INFO: iter: 104520/144360  CE: 3.4858  
[04/03 07:52:10] SuperNet Training INFO: iter: 104640/144360  CE: 3.3329  
[04/03 07:52:21] SuperNet Training INFO: --> epoch:  87/120  avg CE: 3.5245  lr: 0.02103311710018879  
[04/03 07:53:53] SuperNet Training INFO: iter: 104760/144360  CE: 3.6023  
[04/03 07:54:59] SuperNet Training INFO: iter: 104880/144360  CE: 3.5741  
[04/03 07:56:06] SuperNet Training INFO: iter: 105000/144360  CE: 3.8967  
[04/03 07:57:13] SuperNet Training INFO: iter: 105120/144360  CE: 3.4036  
[04/03 07:58:19] SuperNet Training INFO: iter: 105240/144360  CE: 3.5953  
[04/03 07:59:25] SuperNet Training INFO: iter: 105360/144360  CE: 3.4451  
[04/03 08:00:32] SuperNet Training INFO: iter: 105480/144360  CE: 3.5156  
[04/03 08:01:38] SuperNet Training INFO: iter: 105600/144360  CE: 3.8037  
[04/03 08:02:43] SuperNet Training INFO: iter: 105720/144360  CE: 3.6336  
[04/03 08:03:50] SuperNet Training INFO: iter: 105840/144360  CE: 3.4824  
[04/03 08:04:02] SuperNet Training INFO: --> epoch:  88/120  avg CE: 3.5131  lr: 0.019852163618468272  
[04/03 08:05:33] SuperNet Training INFO: iter: 105960/144360  CE: 3.7151  
[04/03 08:06:40] SuperNet Training INFO: iter: 106080/144360  CE: 3.5605  
[04/03 08:07:46] SuperNet Training INFO: iter: 106200/144360  CE: 3.2562  
[04/03 08:08:53] SuperNet Training INFO: iter: 106320/144360  CE: 3.4405  
[04/03 08:10:00] SuperNet Training INFO: iter: 106440/144360  CE: 3.3880  
[04/03 08:11:05] SuperNet Training INFO: iter: 106560/144360  CE: 3.6676  
[04/03 08:12:12] SuperNet Training INFO: iter: 106680/144360  CE: 3.2933  
[04/03 08:13:18] SuperNet Training INFO: iter: 106800/144360  CE: 3.5517  
[04/03 08:14:24] SuperNet Training INFO: iter: 106920/144360  CE: 3.6327  
[04/03 08:15:30] SuperNet Training INFO: iter: 107040/144360  CE: 3.7403  
[04/03 08:15:44] SuperNet Training INFO: --> epoch:  89/120  avg CE: 3.5005  lr: 0.018698725458374543  
[04/03 08:17:13] SuperNet Training INFO: iter: 107160/144360  CE: 3.5699  
[04/03 08:18:20] SuperNet Training INFO: iter: 107280/144360  CE: 3.2367  
[04/03 08:19:27] SuperNet Training INFO: iter: 107400/144360  CE: 3.5572  
[04/03 08:20:34] SuperNet Training INFO: iter: 107520/144360  CE: 3.2735  
[04/03 08:21:40] SuperNet Training INFO: iter: 107640/144360  CE: 3.5992  
[04/03 08:22:46] SuperNet Training INFO: iter: 107760/144360  CE: 3.2440  
[04/03 08:23:53] SuperNet Training INFO: iter: 107880/144360  CE: 3.3363  
[04/03 08:25:01] SuperNet Training INFO: iter: 108000/144360  CE: 3.6987  
[04/03 08:26:07] SuperNet Training INFO: iter: 108120/144360  CE: 4.0124  
[04/03 08:27:14] SuperNet Training INFO: iter: 108240/144360  CE: 3.6642  
[04/03 08:27:29] SuperNet Training INFO: --> epoch:  90/120  avg CE: 3.4885  lr: 0.01757359312880703  
[04/03 08:28:56] SuperNet Training INFO: iter: 108360/144360  CE: 3.3284  
[04/03 08:30:02] SuperNet Training INFO: iter: 108480/144360  CE: 3.6855  
[04/03 08:31:09] SuperNet Training INFO: iter: 108600/144360  CE: 3.5598  
[04/03 08:32:16] SuperNet Training INFO: iter: 108720/144360  CE: 3.7536  
[04/03 08:33:23] SuperNet Training INFO: iter: 108840/144360  CE: 3.3637  
[04/03 08:34:30] SuperNet Training INFO: iter: 108960/144360  CE: 3.3333  
[04/03 08:35:36] SuperNet Training INFO: iter: 109080/144360  CE: 3.5294  
[04/03 08:36:41] SuperNet Training INFO: iter: 109200/144360  CE: 3.3768  
[04/03 08:37:47] SuperNet Training INFO: iter: 109320/144360  CE: 3.5046  
[04/03 08:38:53] SuperNet Training INFO: iter: 109440/144360  CE: 3.5298  
[04/03 08:39:10] SuperNet Training INFO: --> epoch:  91/120  avg CE: 3.4715  lr: 0.016477537739262627  
[04/03 08:40:35] SuperNet Training INFO: iter: 109560/144360  CE: 3.4828  
[04/03 08:41:42] SuperNet Training INFO: iter: 109680/144360  CE: 3.5184  
[04/03 08:42:50] SuperNet Training INFO: iter: 109800/144360  CE: 3.5715  
[04/03 08:43:56] SuperNet Training INFO: iter: 109920/144360  CE: 3.4270  
[04/03 08:45:03] SuperNet Training INFO: iter: 110040/144360  CE: 3.2542  
[04/03 08:46:09] SuperNet Training INFO: iter: 110160/144360  CE: 3.3361  
[04/03 08:47:16] SuperNet Training INFO: iter: 110280/144360  CE: 3.4963  
[04/03 08:48:22] SuperNet Training INFO: iter: 110400/144360  CE: 3.7095  
[04/03 08:49:28] SuperNet Training INFO: iter: 110520/144360  CE: 3.5958  
[04/03 08:50:35] SuperNet Training INFO: iter: 110640/144360  CE: 3.4389  
[04/03 08:50:54] SuperNet Training INFO: --> epoch:  92/120  avg CE: 3.4662  lr: 0.015411310471356233  
[04/03 08:52:17] SuperNet Training INFO: iter: 110760/144360  CE: 3.5911  
[04/03 08:53:25] SuperNet Training INFO: iter: 110880/144360  CE: 3.1999  
[04/03 08:54:32] SuperNet Training INFO: iter: 111000/144360  CE: 3.1944  
[04/03 08:55:39] SuperNet Training INFO: iter: 111120/144360  CE: 3.5929  
[04/03 08:56:45] SuperNet Training INFO: iter: 111240/144360  CE: 3.4769  
[04/03 08:57:52] SuperNet Training INFO: iter: 111360/144360  CE: 3.0698  
[04/03 08:58:57] SuperNet Training INFO: iter: 111480/144360  CE: 3.3149  
[04/03 09:00:04] SuperNet Training INFO: iter: 111600/144360  CE: 3.4736  
[04/03 09:01:11] SuperNet Training INFO: iter: 111720/144360  CE: 3.5127  
[04/03 09:02:17] SuperNet Training INFO: iter: 111840/144360  CE: 3.3510  
[04/03 09:02:38] SuperNet Training INFO: --> epoch:  93/120  avg CE: 3.4517  lr: 0.014375642063998082  
[04/03 09:03:59] SuperNet Training INFO: iter: 111960/144360  CE: 4.0408  
[04/03 09:05:06] SuperNet Training INFO: iter: 112080/144360  CE: 3.4605  
[04/03 09:06:13] SuperNet Training INFO: iter: 112200/144360  CE: 3.4310  
[04/03 09:07:20] SuperNet Training INFO: iter: 112320/144360  CE: 3.5141  
[04/03 09:08:27] SuperNet Training INFO: iter: 112440/144360  CE: 3.6373  
[04/03 09:09:32] SuperNet Training INFO: iter: 112560/144360  CE: 3.5510  
[04/03 09:10:38] SuperNet Training INFO: iter: 112680/144360  CE: 3.3826  
[04/03 09:11:44] SuperNet Training INFO: iter: 112800/144360  CE: 3.1816  
[04/03 09:12:51] SuperNet Training INFO: iter: 112920/144360  CE: 3.2055  
[04/03 09:13:57] SuperNet Training INFO: iter: 113040/144360  CE: 3.2705  
[04/03 09:14:19] SuperNet Training INFO: --> epoch:  94/120  avg CE: 3.4433  lr: 0.01337124231258167  
[04/03 09:15:39] SuperNet Training INFO: iter: 113160/144360  CE: 3.4235  
[04/03 09:16:47] SuperNet Training INFO: iter: 113280/144360  CE: 3.5246  
[04/03 09:17:54] SuperNet Training INFO: iter: 113400/144360  CE: 3.4308  
[04/03 09:19:02] SuperNet Training INFO: iter: 113520/144360  CE: 3.7256  
[04/03 09:20:09] SuperNet Training INFO: iter: 113640/144360  CE: 3.7065  
[04/03 09:21:16] SuperNet Training INFO: iter: 113760/144360  CE: 3.2360  
[04/03 09:22:23] SuperNet Training INFO: iter: 113880/144360  CE: 3.2073  
[04/03 09:23:31] SuperNet Training INFO: iter: 114000/144360  CE: 3.2779  
[04/03 09:24:38] SuperNet Training INFO: iter: 114120/144360  CE: 3.0933  
[04/03 09:25:44] SuperNet Training INFO: iter: 114240/144360  CE: 3.3888  
[04/03 09:26:09] SuperNet Training INFO: --> epoch:  95/120  avg CE: 3.4327  lr: 0.012398799582525794  
[04/03 09:27:27] SuperNet Training INFO: iter: 114360/144360  CE: 3.4045  
[04/03 09:28:38] SuperNet Training INFO: iter: 114480/144360  CE: 3.2543  
[04/03 09:29:46] SuperNet Training INFO: iter: 114600/144360  CE: 3.3440  
[04/03 09:30:53] SuperNet Training INFO: iter: 114720/144360  CE: 3.5720  
[04/03 09:31:59] SuperNet Training INFO: iter: 114840/144360  CE: 3.3341  
[04/03 09:33:05] SuperNet Training INFO: iter: 114960/144360  CE: 3.5223  
[04/03 09:34:11] SuperNet Training INFO: iter: 115080/144360  CE: 3.2993  
[04/03 09:35:16] SuperNet Training INFO: iter: 115200/144360  CE: 3.5975  
[04/03 09:36:21] SuperNet Training INFO: iter: 115320/144360  CE: 3.3861  
[04/03 09:37:27] SuperNet Training INFO: iter: 115440/144360  CE: 3.2582  
[04/03 09:37:52] SuperNet Training INFO: --> epoch:  96/120  avg CE: 3.4192  lr: 0.01145898033750309  
[04/03 09:39:09] SuperNet Training INFO: iter: 115560/144360  CE: 3.6780  
[04/03 09:40:17] SuperNet Training INFO: iter: 115680/144360  CE: 3.2135  
[04/03 09:41:25] SuperNet Training INFO: iter: 115800/144360  CE: 3.5162  
[04/03 09:42:32] SuperNet Training INFO: iter: 115920/144360  CE: 3.3269  
[04/03 09:43:38] SuperNet Training INFO: iter: 116040/144360  CE: 3.4488  
[04/03 09:44:45] SuperNet Training INFO: iter: 116160/144360  CE: 3.1337  
[04/03 09:45:52] SuperNet Training INFO: iter: 116280/144360  CE: 3.1640  
[04/03 09:46:58] SuperNet Training INFO: iter: 116400/144360  CE: 3.1431  
[04/03 09:48:04] SuperNet Training INFO: iter: 116520/144360  CE: 3.5524  
[04/03 09:49:11] SuperNet Training INFO: iter: 116640/144360  CE: 3.2244  
[04/03 09:49:38] SuperNet Training INFO: --> epoch:  97/120  avg CE: 3.4047  lr: 0.01055242868267905  
[04/03 09:50:54] SuperNet Training INFO: iter: 116760/144360  CE: 3.2630  
[04/03 09:52:01] SuperNet Training INFO: iter: 116880/144360  CE: 3.2336  
[04/03 09:53:09] SuperNet Training INFO: iter: 117000/144360  CE: 3.2714  
[04/03 09:54:15] SuperNet Training INFO: iter: 117120/144360  CE: 3.3013  
[04/03 09:55:22] SuperNet Training INFO: iter: 117240/144360  CE: 3.4456  
[04/03 09:56:28] SuperNet Training INFO: iter: 117360/144360  CE: 3.4004  
[04/03 09:57:34] SuperNet Training INFO: iter: 117480/144360  CE: 3.2855  
[04/03 09:58:41] SuperNet Training INFO: iter: 117600/144360  CE: 3.4595  
[04/03 09:59:47] SuperNet Training INFO: iter: 117720/144360  CE: 3.4983  
[04/03 10:00:53] SuperNet Training INFO: iter: 117840/144360  CE: 3.4585  
[04/03 10:01:22] SuperNet Training INFO: --> epoch:  98/120  avg CE: 3.3975  lr: 0.009679765923274538  
[04/03 10:02:36] SuperNet Training INFO: iter: 117960/144360  CE: 3.7692  
[04/03 10:03:44] SuperNet Training INFO: iter: 118080/144360  CE: 3.5681  
[04/03 10:04:51] SuperNet Training INFO: iter: 118200/144360  CE: 3.4642  
[04/03 10:05:59] SuperNet Training INFO: iter: 118320/144360  CE: 3.1246  
[04/03 10:07:06] SuperNet Training INFO: iter: 118440/144360  CE: 3.4317  
[04/03 10:08:12] SuperNet Training INFO: iter: 118560/144360  CE: 3.0598  
[04/03 10:09:19] SuperNet Training INFO: iter: 118680/144360  CE: 3.2523  
[04/03 10:10:25] SuperNet Training INFO: iter: 118800/144360  CE: 3.2788  
[04/03 10:11:32] SuperNet Training INFO: iter: 118920/144360  CE: 3.3139  
[04/03 10:12:37] SuperNet Training INFO: iter: 119040/144360  CE: 3.2238  
[04/03 10:13:09] SuperNet Training INFO: --> epoch:  99/120  avg CE: 3.3813  lr: 0.008841590138754444  
[04/03 10:14:20] SuperNet Training INFO: iter: 119160/144360  CE: 3.5168  
[04/03 10:15:27] SuperNet Training INFO: iter: 119280/144360  CE: 3.4548  
[04/03 10:16:35] SuperNet Training INFO: iter: 119400/144360  CE: 3.4291  
[04/03 10:17:42] SuperNet Training INFO: iter: 119520/144360  CE: 3.3885  
[04/03 10:18:49] SuperNet Training INFO: iter: 119640/144360  CE: 3.5294  
[04/03 10:19:56] SuperNet Training INFO: iter: 119760/144360  CE: 3.2934  
[04/03 10:21:03] SuperNet Training INFO: iter: 119880/144360  CE: 3.3778  
[04/03 10:22:08] SuperNet Training INFO: iter: 120000/144360  CE: 3.4368  
[04/03 10:23:15] SuperNet Training INFO: iter: 120120/144360  CE: 3.3931  
[04/03 10:24:20] SuperNet Training INFO: iter: 120240/144360  CE: 3.1311  
[04/03 10:24:53] SuperNet Training INFO: --> epoch: 100/120  avg CE: 3.3710  lr: 0.00803847577293368  
[04/03 10:26:02] SuperNet Training INFO: iter: 120360/144360  CE: 3.3835  
[04/03 10:27:09] SuperNet Training INFO: iter: 120480/144360  CE: 2.9557  
[04/03 10:28:16] SuperNet Training INFO: iter: 120600/144360  CE: 3.5091  
[04/03 10:29:23] SuperNet Training INFO: iter: 120720/144360  CE: 3.6111  
[04/03 10:30:30] SuperNet Training INFO: iter: 120840/144360  CE: 3.5017  
[04/03 10:31:40] SuperNet Training INFO: iter: 120960/144360  CE: 3.2772  
[04/03 10:32:45] SuperNet Training INFO: iter: 121080/144360  CE: 3.5143  
[04/03 10:33:52] SuperNet Training INFO: iter: 121200/144360  CE: 3.5782  
[04/03 10:34:57] SuperNet Training INFO: iter: 121320/144360  CE: 3.6220  
[04/03 10:36:03] SuperNet Training INFO: iter: 121440/144360  CE: 3.2851  
[04/03 10:36:37] SuperNet Training INFO: --> epoch: 101/120  avg CE: 3.3684  lr: 0.007270973240282054  
[04/03 10:37:46] SuperNet Training INFO: iter: 121560/144360  CE: 3.4533  
[04/03 10:38:53] SuperNet Training INFO: iter: 121680/144360  CE: 3.1716  
[04/03 10:40:00] SuperNet Training INFO: iter: 121800/144360  CE: 3.2549  
[04/03 10:41:07] SuperNet Training INFO: iter: 121920/144360  CE: 3.2384  
[04/03 10:42:14] SuperNet Training INFO: iter: 122040/144360  CE: 3.4278  
[04/03 10:43:20] SuperNet Training INFO: iter: 122160/144360  CE: 3.7408  
[04/03 10:44:27] SuperNet Training INFO: iter: 122280/144360  CE: 3.1364  
[04/03 10:45:33] SuperNet Training INFO: iter: 122400/144360  CE: 3.3226  
[04/03 10:46:39] SuperNet Training INFO: iter: 122520/144360  CE: 3.5212  
[04/03 10:47:45] SuperNet Training INFO: iter: 122640/144360  CE: 3.2498  
[04/03 10:48:20] SuperNet Training INFO: --> epoch: 102/120  avg CE: 3.3563  lr: 0.006539608548697928  
[04/03 10:49:28] SuperNet Training INFO: iter: 122760/144360  CE: 3.6518  
[04/03 10:50:35] SuperNet Training INFO: iter: 122880/144360  CE: 3.3011  
[04/03 10:51:43] SuperNet Training INFO: iter: 123000/144360  CE: 3.3766  
[04/03 10:52:49] SuperNet Training INFO: iter: 123120/144360  CE: 3.3308  
[04/03 10:53:55] SuperNet Training INFO: iter: 123240/144360  CE: 3.2505  
[04/03 10:55:02] SuperNet Training INFO: iter: 123360/144360  CE: 3.3493  
[04/03 10:56:08] SuperNet Training INFO: iter: 123480/144360  CE: 3.4770  
[04/03 10:57:14] SuperNet Training INFO: iter: 123600/144360  CE: 3.5916  
[04/03 10:58:21] SuperNet Training INFO: iter: 123720/144360  CE: 3.5472  
[04/03 10:59:27] SuperNet Training INFO: iter: 123840/144360  CE: 3.4825  
[04/03 11:00:04] SuperNet Training INFO: --> epoch: 103/120  avg CE: 3.3636  lr: 0.00584488293900834  
[04/03 11:01:09] SuperNet Training INFO: iter: 123960/144360  CE: 3.3685  
[04/03 11:02:16] SuperNet Training INFO: iter: 124080/144360  CE: 3.1496  
[04/03 11:03:24] SuperNet Training INFO: iter: 124200/144360  CE: 3.4950  
[04/03 11:04:30] SuperNet Training INFO: iter: 124320/144360  CE: 3.2849  
[04/03 11:05:36] SuperNet Training INFO: iter: 124440/144360  CE: 3.1547  
[04/03 11:06:43] SuperNet Training INFO: iter: 124560/144360  CE: 3.1790  
[04/03 11:07:48] SuperNet Training INFO: iter: 124680/144360  CE: 3.5160  
[04/03 11:08:54] SuperNet Training INFO: iter: 124800/144360  CE: 3.3541  
[04/03 11:10:00] SuperNet Training INFO: iter: 124920/144360  CE: 3.4797  
[04/03 11:11:06] SuperNet Training INFO: iter: 125040/144360  CE: 3.2233  
[04/03 11:11:45] SuperNet Training INFO: --> epoch: 104/120  avg CE: 3.3420  lr: 0.005187272541443939  
[04/03 11:12:48] SuperNet Training INFO: iter: 125160/144360  CE: 3.4541  
[04/03 11:13:56] SuperNet Training INFO: iter: 125280/144360  CE: 3.3866  
[04/03 11:15:02] SuperNet Training INFO: iter: 125400/144360  CE: 3.2836  
[04/03 11:16:09] SuperNet Training INFO: iter: 125520/144360  CE: 3.5825  
[04/03 11:17:16] SuperNet Training INFO: iter: 125640/144360  CE: 3.8463  
[04/03 11:18:23] SuperNet Training INFO: iter: 125760/144360  CE: 3.3028  
[04/03 11:19:30] SuperNet Training INFO: iter: 125880/144360  CE: 3.3292  
[04/03 11:20:36] SuperNet Training INFO: iter: 126000/144360  CE: 3.3077  
[04/03 11:21:42] SuperNet Training INFO: iter: 126120/144360  CE: 3.0486  
[04/03 11:22:49] SuperNet Training INFO: iter: 126240/144360  CE: 3.3565  
[04/03 11:23:29] SuperNet Training INFO: --> epoch: 105/120  avg CE: 3.3375  lr: 0.00456722804932279  
[04/03 11:24:31] SuperNet Training INFO: iter: 126360/144360  CE: 3.1967  
[04/03 11:25:39] SuperNet Training INFO: iter: 126480/144360  CE: 3.1863  
[04/03 11:26:46] SuperNet Training INFO: iter: 126600/144360  CE: 3.1965  
[04/03 11:27:53] SuperNet Training INFO: iter: 126720/144360  CE: 3.1726  
[04/03 11:28:58] SuperNet Training INFO: iter: 126840/144360  CE: 3.3517  
[04/03 11:30:03] SuperNet Training INFO: iter: 126960/144360  CE: 3.3573  
[04/03 11:31:10] SuperNet Training INFO: iter: 127080/144360  CE: 3.0415  
[04/03 11:32:17] SuperNet Training INFO: iter: 127200/144360  CE: 3.1281  
[04/03 11:33:23] SuperNet Training INFO: iter: 127320/144360  CE: 3.4656  
[04/03 11:34:33] SuperNet Training INFO: iter: 127440/144360  CE: 3.3271  
[04/03 11:35:15] SuperNet Training INFO: --> epoch: 106/120  avg CE: 3.3298  lr: 0.003985174410167894  
[04/03 11:36:15] SuperNet Training INFO: iter: 127560/144360  CE: 2.9905  
[04/03 11:37:22] SuperNet Training INFO: iter: 127680/144360  CE: 3.3480  
[04/03 11:38:29] SuperNet Training INFO: iter: 127800/144360  CE: 3.4137  
[04/03 11:39:35] SuperNet Training INFO: iter: 127920/144360  CE: 3.3466  
[04/03 11:40:42] SuperNet Training INFO: iter: 128040/144360  CE: 3.2099  
[04/03 11:41:49] SuperNet Training INFO: iter: 128160/144360  CE: 3.4710  
[04/03 11:42:55] SuperNet Training INFO: iter: 128280/144360  CE: 3.1755  
[04/03 11:44:01] SuperNet Training INFO: iter: 128400/144360  CE: 3.3257  
[04/03 11:45:07] SuperNet Training INFO: iter: 128520/144360  CE: 3.2885  
[04/03 11:46:14] SuperNet Training INFO: iter: 128640/144360  CE: 3.4049  
[04/03 11:46:59] SuperNet Training INFO: --> epoch: 107/120  avg CE: 3.3225  lr: 0.003441510534469298  
[04/03 11:47:57] SuperNet Training INFO: iter: 128760/144360  CE: 3.2534  
[04/03 11:49:03] SuperNet Training INFO: iter: 128880/144360  CE: 3.0963  
[04/03 11:50:11] SuperNet Training INFO: iter: 129000/144360  CE: 3.2001  
[04/03 11:51:18] SuperNet Training INFO: iter: 129120/144360  CE: 3.4860  
[04/03 11:52:24] SuperNet Training INFO: iter: 129240/144360  CE: 3.2777  
[04/03 11:53:30] SuperNet Training INFO: iter: 129360/144360  CE: 3.0469  
[04/03 11:54:36] SuperNet Training INFO: iter: 129480/144360  CE: 3.3079  
[04/03 11:55:42] SuperNet Training INFO: iter: 129600/144360  CE: 3.4932  
[04/03 11:56:47] SuperNet Training INFO: iter: 129720/144360  CE: 3.0709  
[04/03 11:57:52] SuperNet Training INFO: iter: 129840/144360  CE: 3.7218  
[04/03 11:58:38] SuperNet Training INFO: --> epoch: 108/120  avg CE: 3.3185  lr: 0.002936609022290792  
[04/03 11:59:35] SuperNet Training INFO: iter: 129960/144360  CE: 3.3742  
[04/03 12:00:42] SuperNet Training INFO: iter: 130080/144360  CE: 3.3044  
[04/03 12:01:49] SuperNet Training INFO: iter: 130200/144360  CE: 3.5799  
[04/03 12:02:55] SuperNet Training INFO: iter: 130320/144360  CE: 3.1927  
[04/03 12:04:02] SuperNet Training INFO: iter: 130440/144360  CE: 3.3593  
[04/03 12:05:09] SuperNet Training INFO: iter: 130560/144360  CE: 3.3295  
[04/03 12:06:16] SuperNet Training INFO: iter: 130680/144360  CE: 3.6770  
[04/03 12:07:22] SuperNet Training INFO: iter: 130800/144360  CE: 3.3130  
[04/03 12:08:28] SuperNet Training INFO: iter: 130920/144360  CE: 3.4384  
[04/03 12:09:34] SuperNet Training INFO: iter: 131040/144360  CE: 3.3265  
[04/03 12:10:22] SuperNet Training INFO: --> epoch: 109/120  avg CE: 3.3145  lr: 0.0024708159079084185  
[04/03 12:11:16] SuperNet Training INFO: iter: 131160/144360  CE: 3.5926  
[04/03 12:12:23] SuperNet Training INFO: iter: 131280/144360  CE: 3.4353  
[04/03 12:13:30] SuperNet Training INFO: iter: 131400/144360  CE: 3.0273  
[04/03 12:14:37] SuperNet Training INFO: iter: 131520/144360  CE: 3.3131  
[04/03 12:15:43] SuperNet Training INFO: iter: 131640/144360  CE: 3.3912  
[04/03 12:16:49] SuperNet Training INFO: iter: 131760/144360  CE: 3.3327  
[04/03 12:17:55] SuperNet Training INFO: iter: 131880/144360  CE: 3.6467  
[04/03 12:19:01] SuperNet Training INFO: iter: 132000/144360  CE: 3.2469  
[04/03 12:20:07] SuperNet Training INFO: iter: 132120/144360  CE: 3.2821  
[04/03 12:21:14] SuperNet Training INFO: iter: 132240/144360  CE: 3.4542  
[04/03 12:22:03] SuperNet Training INFO: --> epoch: 110/120  avg CE: 3.3139  lr: 0.0020444504226559065  
[04/03 12:22:56] SuperNet Training INFO: iter: 132360/144360  CE: 3.5247  
[04/03 12:24:03] SuperNet Training INFO: iter: 132480/144360  CE: 3.8560  
[04/03 12:25:10] SuperNet Training INFO: iter: 132600/144360  CE: 3.2713  
[04/03 12:26:16] SuperNet Training INFO: iter: 132720/144360  CE: 3.2105  
[04/03 12:27:22] SuperNet Training INFO: iter: 132840/144360  CE: 3.3154  
[04/03 12:28:28] SuperNet Training INFO: iter: 132960/144360  CE: 3.3176  
[04/03 12:29:35] SuperNet Training INFO: iter: 133080/144360  CE: 3.4709  
[04/03 12:30:41] SuperNet Training INFO: iter: 133200/144360  CE: 3.2614  
[04/03 12:31:47] SuperNet Training INFO: iter: 133320/144360  CE: 3.2778  
[04/03 12:32:53] SuperNet Training INFO: iter: 133440/144360  CE: 3.5203  
[04/03 12:33:43] SuperNet Training INFO: --> epoch: 111/120  avg CE: 3.3072  lr: 0.0016578047761394107  
[04/03 12:34:35] SuperNet Training INFO: iter: 133560/144360  CE: 3.4830  
[04/03 12:35:43] SuperNet Training INFO: iter: 133680/144360  CE: 3.2908  
[04/03 12:36:51] SuperNet Training INFO: iter: 133800/144360  CE: 3.3140  
[04/03 12:38:00] SuperNet Training INFO: iter: 133920/144360  CE: 3.5549  
[04/03 12:39:07] SuperNet Training INFO: iter: 134040/144360  CE: 3.2225  
[04/03 12:40:12] SuperNet Training INFO: iter: 134160/144360  CE: 3.4560  
[04/03 12:41:19] SuperNet Training INFO: iter: 134280/144360  CE: 3.2777  
[04/03 12:42:24] SuperNet Training INFO: iter: 134400/144360  CE: 3.0851  
[04/03 12:43:31] SuperNet Training INFO: iter: 134520/144360  CE: 3.1177  
[04/03 12:44:37] SuperNet Training INFO: iter: 134640/144360  CE: 3.3702  
[04/03 12:45:29] SuperNet Training INFO: --> epoch: 112/120  avg CE: 3.3082  lr: 0.0013111439559716617  
[04/03 12:46:20] SuperNet Training INFO: iter: 134760/144360  CE: 3.2842  
[04/03 12:47:27] SuperNet Training INFO: iter: 134880/144360  CE: 3.6119  
[04/03 12:48:34] SuperNet Training INFO: iter: 135000/144360  CE: 3.0523  
[04/03 12:49:40] SuperNet Training INFO: iter: 135120/144360  CE: 3.2608  
[04/03 12:50:47] SuperNet Training INFO: iter: 135240/144360  CE: 3.2931  
[04/03 12:51:53] SuperNet Training INFO: iter: 135360/144360  CE: 3.6248  
[04/03 12:52:59] SuperNet Training INFO: iter: 135480/144360  CE: 3.4835  
[04/03 12:54:05] SuperNet Training INFO: iter: 135600/144360  CE: 3.5188  
[04/03 12:55:11] SuperNet Training INFO: iter: 135720/144360  CE: 3.1250  
[04/03 12:56:17] SuperNet Training INFO: iter: 135840/144360  CE: 2.9605  
[04/03 12:57:12] SuperNet Training INFO: --> epoch: 113/120  avg CE: 3.2997  lr: 0.0010047055461627253  
[04/03 12:58:00] SuperNet Training INFO: iter: 135960/144360  CE: 3.5404  
[04/03 12:59:07] SuperNet Training INFO: iter: 136080/144360  CE: 3.3910  
[04/03 13:00:14] SuperNet Training INFO: iter: 136200/144360  CE: 3.0990  
[04/03 13:01:21] SuperNet Training INFO: iter: 136320/144360  CE: 3.5472  
[04/03 13:02:27] SuperNet Training INFO: iter: 136440/144360  CE: 3.2035  
[04/03 13:03:33] SuperNet Training INFO: iter: 136560/144360  CE: 3.2263  
[04/03 13:04:39] SuperNet Training INFO: iter: 136680/144360  CE: 3.3611  
[04/03 13:05:45] SuperNet Training INFO: iter: 136800/144360  CE: 3.2429  
[04/03 13:06:50] SuperNet Training INFO: iter: 136920/144360  CE: 3.1455  
[04/03 13:07:55] SuperNet Training INFO: iter: 137040/144360  CE: 3.6672  
[04/03 13:08:49] SuperNet Training INFO: --> epoch: 114/120  avg CE: 3.3020  lr: 0.000738699564291742  
[04/03 13:09:37] SuperNet Training INFO: iter: 137160/144360  CE: 2.9174  
[04/03 13:10:44] SuperNet Training INFO: iter: 137280/144360  CE: 2.9994  
[04/03 13:11:51] SuperNet Training INFO: iter: 137400/144360  CE: 3.0925  
[04/03 13:12:58] SuperNet Training INFO: iter: 137520/144360  CE: 3.1662  
[04/03 13:14:05] SuperNet Training INFO: iter: 137640/144360  CE: 3.0012  
[04/03 13:15:12] SuperNet Training INFO: iter: 137760/144360  CE: 3.5335  
[04/03 13:16:20] SuperNet Training INFO: iter: 137880/144360  CE: 3.4387  
[04/03 13:17:27] SuperNet Training INFO: iter: 138000/144360  CE: 3.2981  
[04/03 13:18:34] SuperNet Training INFO: iter: 138120/144360  CE: 3.3538  
[04/03 13:19:41] SuperNet Training INFO: iter: 138240/144360  CE: 3.3859  
[04/03 13:20:40] SuperNet Training INFO: --> epoch: 115/120  avg CE: 3.2922  lr: 0.0005133083175713779  
[04/03 13:21:25] SuperNet Training INFO: iter: 138360/144360  CE: 3.1498  
[04/03 13:22:32] SuperNet Training INFO: iter: 138480/144360  CE: 3.5251  
[04/03 13:23:40] SuperNet Training INFO: iter: 138600/144360  CE: 3.3991  
[04/03 13:24:46] SuperNet Training INFO: iter: 138720/144360  CE: 3.4767  
[04/03 13:25:53] SuperNet Training INFO: iter: 138840/144360  CE: 3.1699  
[04/03 13:27:00] SuperNet Training INFO: iter: 138960/144360  CE: 3.3115  
[04/03 13:28:05] SuperNet Training INFO: iter: 139080/144360  CE: 3.4094  
[04/03 13:29:11] SuperNet Training INFO: iter: 139200/144360  CE: 3.1787  
[04/03 13:30:18] SuperNet Training INFO: iter: 139320/144360  CE: 3.1401  
[04/03 13:31:24] SuperNet Training INFO: iter: 139440/144360  CE: 3.2859  
[04/03 13:32:22] SuperNet Training INFO: --> epoch: 116/120  avg CE: 3.2881  lr: 0.00032868627790359544  
[04/03 13:33:05] SuperNet Training INFO: iter: 139560/144360  CE: 3.3385  
[04/03 13:34:12] SuperNet Training INFO: iter: 139680/144360  CE: 3.2497  
[04/03 13:35:19] SuperNet Training INFO: iter: 139800/144360  CE: 3.1341  
[04/03 13:36:27] SuperNet Training INFO: iter: 139920/144360  CE: 3.2440  
[04/03 13:37:34] SuperNet Training INFO: iter: 140040/144360  CE: 3.3045  
[04/03 13:38:40] SuperNet Training INFO: iter: 140160/144360  CE: 3.3489  
[04/03 13:39:48] SuperNet Training INFO: iter: 140280/144360  CE: 3.1604  
[04/03 13:40:56] SuperNet Training INFO: iter: 140400/144360  CE: 3.2547  
[04/03 13:42:03] SuperNet Training INFO: iter: 140520/144360  CE: 3.2730  
[04/03 13:43:09] SuperNet Training INFO: iter: 140640/144360  CE: 3.2208  
[04/03 13:44:10] SuperNet Training INFO: --> epoch: 117/120  avg CE: 3.2893  lr: 0.00018495997601232129  
[04/03 13:44:51] SuperNet Training INFO: iter: 140760/144360  CE: 3.2976  
[04/03 13:45:58] SuperNet Training INFO: iter: 140880/144360  CE: 3.7702  
[04/03 13:47:05] SuperNet Training INFO: iter: 141000/144360  CE: 3.3143  
[04/03 13:48:12] SuperNet Training INFO: iter: 141120/144360  CE: 3.3804  
[04/03 13:49:18] SuperNet Training INFO: iter: 141240/144360  CE: 3.5405  
[04/03 13:50:24] SuperNet Training INFO: iter: 141360/144360  CE: 3.3105  
[04/03 13:51:30] SuperNet Training INFO: iter: 141480/144360  CE: 3.1075  
[04/03 13:52:36] SuperNet Training INFO: iter: 141600/144360  CE: 3.2282  
[04/03 13:53:43] SuperNet Training INFO: iter: 141720/144360  CE: 2.8757  
[04/03 13:54:48] SuperNet Training INFO: iter: 141840/144360  CE: 3.1211  
[04/03 13:55:50] SuperNet Training INFO: --> epoch: 118/120  avg CE: 3.2839  lr: 8.222791472556962e-05  
[04/03 13:56:30] SuperNet Training INFO: iter: 141960/144360  CE: 3.2778  
[04/03 13:57:37] SuperNet Training INFO: iter: 142080/144360  CE: 3.3152  
[04/03 13:58:44] SuperNet Training INFO: iter: 142200/144360  CE: 3.3374  
[04/03 13:59:52] SuperNet Training INFO: iter: 142320/144360  CE: 3.2637  
[04/03 14:00:58] SuperNet Training INFO: iter: 142440/144360  CE: 3.1116  
[04/03 14:02:04] SuperNet Training INFO: iter: 142560/144360  CE: 3.1744  
[04/03 14:03:11] SuperNet Training INFO: iter: 142680/144360  CE: 3.2777  
[04/03 14:04:17] SuperNet Training INFO: iter: 142800/144360  CE: 3.2268  
[04/03 14:05:24] SuperNet Training INFO: iter: 142920/144360  CE: 3.3853  
[04/03 14:06:30] SuperNet Training INFO: iter: 143040/144360  CE: 3.2737  
[04/03 14:07:34] SuperNet Training INFO: --> epoch: 119/120  avg CE: 3.2758  lr: 2.0560501466564365e-05  
[04/03 14:08:13] SuperNet Training INFO: iter: 143160/144360  CE: 3.1712  
[04/03 14:09:20] SuperNet Training INFO: iter: 143280/144360  CE: 3.1981  
[04/03 14:10:27] SuperNet Training INFO: iter: 143400/144360  CE: 3.0545  
[04/03 14:11:33] SuperNet Training INFO: iter: 143520/144360  CE: 2.9706  
[04/03 14:12:39] SuperNet Training INFO: iter: 143640/144360  CE: 3.4165  
[04/03 14:13:46] SuperNet Training INFO: iter: 143760/144360  CE: 3.4356  
[04/03 14:14:52] SuperNet Training INFO: iter: 143880/144360  CE: 3.1943  
[04/03 14:15:58] SuperNet Training INFO: iter: 144000/144360  CE: 3.3461  
[04/03 14:17:04] SuperNet Training INFO: iter: 144120/144360  CE: 3.3108  
[04/03 14:18:10] SuperNet Training INFO: iter: 144240/144360  CE: 3.2928  
[04/03 14:19:15] SuperNet Training INFO: iter: 144360/144360  CE: 3.5916  
[04/03 14:19:15] SuperNet Training INFO: --> epoch: 120/120  avg CE: 3.2889  lr: 0.0  
[04/03 14:19:15] SuperNet Training INFO: --> END gss-mobile0bn-seed-0
[04/03 14:19:20] SuperNet Training INFO: ELAPSED TIME: 84382.5(s) = 23(h) 26(m)
