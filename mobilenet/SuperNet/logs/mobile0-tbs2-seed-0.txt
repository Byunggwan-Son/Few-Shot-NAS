[05/06 05:02:45] SuperNet Training INFO: tag                 : mobile0-tbs2
[05/06 05:02:45] SuperNet Training INFO: seed                : 0
[05/06 05:02:45] SuperNet Training INFO: thresholds          : [38, 40]
[05/06 05:02:45] SuperNet Training INFO: data_path           : ../../../dataset/ILSVRC2012
[05/06 05:02:45] SuperNet Training INFO: save_path           : ./SuperNet
[05/06 05:02:45] SuperNet Training INFO: search_space        : proxyless
[05/06 05:02:45] SuperNet Training INFO: valid_size          : 50000
[05/06 05:02:45] SuperNet Training INFO: num_gpus            : 8
[05/06 05:02:45] SuperNet Training INFO: workers             : 4
[05/06 05:02:45] SuperNet Training INFO: interval_ep_eval    : 8
[05/06 05:02:45] SuperNet Training INFO: train_batch_size    : 1024
[05/06 05:02:45] SuperNet Training INFO: test_batch_size     : 256
[05/06 05:02:45] SuperNet Training INFO: max_epoch           : 120
[05/06 05:02:45] SuperNet Training INFO: learning_rate       : 0.12
[05/06 05:02:45] SuperNet Training INFO: momentum            : 0.9
[05/06 05:02:45] SuperNet Training INFO: weight_decay        : 4e-05
[05/06 05:02:45] SuperNet Training INFO: nesterov            : True
[05/06 05:02:45] SuperNet Training INFO: lr_schedule_type    : cosine
[05/06 05:02:45] SuperNet Training INFO: warmup              : False
[05/06 05:02:45] SuperNet Training INFO: label_smooth        : 0.1
[05/06 05:02:45] SuperNet Training INFO: rank                : 0
[05/06 05:02:45] SuperNet Training INFO: gpu                 : 0
[05/06 05:02:45] SuperNet Training INFO: save_name           : mobile0-tbs2-seed-0
[05/06 05:02:45] SuperNet Training INFO: log_path            : ./SuperNet/logs/mobile0-tbs2-seed-0.txt
[05/06 05:02:45] SuperNet Training INFO: ckpt_path           : ./SuperNet/checkpoint/mobile0-tbs2-seed-0.pt
[05/06 05:02:45] SuperNet Training INFO: dist_url            : tcp://127.0.0.1:23456
[05/06 05:02:45] SuperNet Training INFO: world_size          : 8
[05/06 05:02:45] SuperNet Training INFO: distributed         : True
[05/06 05:02:45] SuperNet Training INFO: ['3x3_MBConv3', '3x3_MBConv6', '5x5_MBConv3', '5x5_MBConv6', '7x7_MBConv3', '7x7_MBConv6', 'Identity']
[05/06 05:03:23] SuperNet Training INFO: DistributedDataParallel(
  (module): SuperNet(
    (first_conv): Sequential(
      (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
      (2): ReLU6(inplace=True)
    )
    (first_block): InvertedResidual(
      (depth_conv): Sequential(
        (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)
        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
        (2): ReLU6(inplace=True)
      )
      (point_linear): Sequential(
        (0): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
      )
    )
    (blocks): ModuleList(
      (0): ModuleList(
        (0): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(16, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(48, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=48, bias=False)
            (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(48, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (1): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)
            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(96, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (2): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(16, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(48, 48, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=48, bias=False)
            (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(48, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (3): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(96, 96, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=96, bias=False)
            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(96, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (4): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(16, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(48, 48, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), groups=48, bias=False)
            (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(48, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (5): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(96, 96, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), groups=96, bias=False)
            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(96, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
      )
      (1): ModuleList(
        (0): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(32, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(96, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (1): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (2): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(32, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96, bias=False)
            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(96, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (3): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(192, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=192, bias=False)
            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (4): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(32, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96, bias=False)
            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(96, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (5): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192, bias=False)
            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (6): Identity()
      )
      (2): ModuleList(
        (0): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(32, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(96, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (1): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (2): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(32, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96, bias=False)
            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(96, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (3): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(192, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=192, bias=False)
            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (4): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(32, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96, bias=False)
            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(96, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (5): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192, bias=False)
            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (6): Identity()
      )
      (3): ModuleList(
        (0): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(32, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(96, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (1): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (2): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(32, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(96, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=96, bias=False)
            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(96, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (3): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(192, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=192, bias=False)
            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (4): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(32, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96, bias=False)
            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(96, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (5): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192, bias=False)
            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (6): Identity()
      )
      (4): ModuleList(
        (0): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(32, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)
            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(96, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (1): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)
            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(192, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (2): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(32, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(96, 96, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=96, bias=False)
            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(96, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (3): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(192, 192, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=192, bias=False)
            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(192, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (4): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(32, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(96, 96, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), groups=96, bias=False)
            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(96, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (5): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(192, 192, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), groups=192, bias=False)
            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(192, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
      )
      (5): ModuleList(
        (0): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(120, 120, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=120, bias=False)
            (1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(120, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (1): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(240, 240, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=240, bias=False)
            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (2): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(120, 120, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=120, bias=False)
            (1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(120, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (3): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(240, 240, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=240, bias=False)
            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (4): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(120, 120, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=120, bias=False)
            (1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(120, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (5): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(240, 240, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=240, bias=False)
            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (6): Identity()
      )
      (6): ModuleList(
        (0): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(120, 120, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=120, bias=False)
            (1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(120, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (1): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(240, 240, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=240, bias=False)
            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (2): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(120, 120, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=120, bias=False)
            (1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(120, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (3): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(240, 240, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=240, bias=False)
            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (4): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(120, 120, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=120, bias=False)
            (1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(120, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (5): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(240, 240, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=240, bias=False)
            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (6): Identity()
      )
      (7): ModuleList(
        (0): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(120, 120, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=120, bias=False)
            (1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(120, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (1): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(240, 240, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=240, bias=False)
            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (2): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(120, 120, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=120, bias=False)
            (1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(120, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (3): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(240, 240, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=240, bias=False)
            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (4): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(120, 120, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=120, bias=False)
            (1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(120, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (5): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(240, 240, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=240, bias=False)
            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (6): Identity()
      )
      (8): ModuleList(
        (0): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(120, 120, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=120, bias=False)
            (1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(120, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (1): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(240, 240, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=240, bias=False)
            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (2): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(120, 120, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=120, bias=False)
            (1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(120, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (3): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(240, 240, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=240, bias=False)
            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (4): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(120, 120, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), groups=120, bias=False)
            (1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(120, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (5): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(240, 240, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), groups=240, bias=False)
            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
      )
      (9): ModuleList(
        (0): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(80, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(240, 240, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=240, bias=False)
            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (1): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)
            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (2): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(80, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(240, 240, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=240, bias=False)
            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (3): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(480, 480, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=480, bias=False)
            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (4): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(80, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(240, 240, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=240, bias=False)
            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (5): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(480, 480, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=480, bias=False)
            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (6): Identity()
      )
      (10): ModuleList(
        (0): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(80, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(240, 240, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=240, bias=False)
            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (1): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)
            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (2): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(80, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(240, 240, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=240, bias=False)
            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (3): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(480, 480, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=480, bias=False)
            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (4): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(80, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(240, 240, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=240, bias=False)
            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (5): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(480, 480, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=480, bias=False)
            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (6): Identity()
      )
      (11): ModuleList(
        (0): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(80, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(240, 240, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=240, bias=False)
            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (1): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)
            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (2): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(80, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(240, 240, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=240, bias=False)
            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (3): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(480, 480, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=480, bias=False)
            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (4): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(80, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(240, 240, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=240, bias=False)
            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (5): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(480, 480, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=480, bias=False)
            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (6): Identity()
      )
      (12): ModuleList(
        (0): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(80, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(240, 240, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=240, bias=False)
            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(240, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (1): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)
            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(480, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (2): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(80, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(240, 240, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=240, bias=False)
            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(240, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (3): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(480, 480, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=480, bias=False)
            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(480, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (4): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(80, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(240, 240, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=240, bias=False)
            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(240, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (5): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(480, 480, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=480, bias=False)
            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(480, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
      )
      (13): ModuleList(
        (0): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(96, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
            (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(288, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (1): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)
            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (2): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(96, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(288, 288, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=288, bias=False)
            (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(288, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (3): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(576, 576, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=576, bias=False)
            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (4): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(96, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(288, 288, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=288, bias=False)
            (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(288, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (5): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(576, 576, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=576, bias=False)
            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (6): Identity()
      )
      (14): ModuleList(
        (0): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(96, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
            (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(288, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (1): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)
            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (2): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(96, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(288, 288, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=288, bias=False)
            (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(288, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (3): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(576, 576, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=576, bias=False)
            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (4): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(96, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(288, 288, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=288, bias=False)
            (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(288, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (5): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(576, 576, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=576, bias=False)
            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (6): Identity()
      )
      (15): ModuleList(
        (0): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(96, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
            (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(288, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (1): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)
            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (2): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(96, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(288, 288, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=288, bias=False)
            (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(288, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (3): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(576, 576, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=576, bias=False)
            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (4): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(96, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(288, 288, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=288, bias=False)
            (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(288, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (5): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(576, 576, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=576, bias=False)
            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (6): Identity()
      )
      (16): ModuleList(
        (0): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(96, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(288, 288, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=288, bias=False)
            (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(288, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (1): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=576, bias=False)
            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(576, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (2): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(96, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(288, 288, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=288, bias=False)
            (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(288, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (3): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(576, 576, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=576, bias=False)
            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(576, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (4): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(96, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(288, 288, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), groups=288, bias=False)
            (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(288, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (5): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(576, 576, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), groups=576, bias=False)
            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(576, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
      )
      (17): ModuleList(
        (0): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(192, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)
            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(576, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (1): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152, bias=False)
            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (2): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(192, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(576, 576, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=576, bias=False)
            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(576, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (3): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)
            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (4): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(192, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(576, 576, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=576, bias=False)
            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(576, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (5): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(1152, 1152, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1152, bias=False)
            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (6): Identity()
      )
      (18): ModuleList(
        (0): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(192, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)
            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(576, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (1): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152, bias=False)
            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (2): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(192, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(576, 576, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=576, bias=False)
            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(576, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (3): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)
            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (4): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(192, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(576, 576, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=576, bias=False)
            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(576, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (5): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(1152, 1152, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1152, bias=False)
            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (6): Identity()
      )
      (19): ModuleList(
        (0): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(192, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)
            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(576, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (1): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152, bias=False)
            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (2): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(192, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(576, 576, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=576, bias=False)
            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(576, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (3): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)
            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (4): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(192, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(576, 576, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=576, bias=False)
            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(576, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (5): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(1152, 1152, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1152, bias=False)
            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (6): Identity()
      )
      (20): ModuleList(
        (0): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(192, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)
            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(576, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (1): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152, bias=False)
            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(1152, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (2): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(192, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(576, 576, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=576, bias=False)
            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(576, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (3): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)
            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(1152, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (4): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(192, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(576, 576, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=576, bias=False)
            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(576, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (5): InvertedResidual(
          (inverted_bottleneck): Sequential(
            (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (depth_conv): Sequential(
            (0): Conv2d(1152, 1152, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1152, bias=False)
            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
            (2): ReLU6(inplace=True)
          )
          (point_linear): Sequential(
            (0): Conv2d(1152, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
      )
    )
    (feature_mix_layer): Sequential(
      (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
      (2): ReLU6(inplace=True)
    )
    (avgpool): AvgPool2d(kernel_size=7, stride=7, padding=0)
    (classifier): Sequential(
      (0): Linear(in_features=1280, out_features=1000, bias=True)
    )
  )
)
[05/06 05:03:49] SuperNet Training INFO: Trainset Size: 1231167
[05/06 05:03:49] SuperNet Training INFO: Validset Size:   50000
[05/06 05:03:49] SuperNet Training INFO: Compose(
    RandomResizedCrop(size=(224, 224), scale=(0.08, 1.0), ratio=(0.75, 1.3333), interpolation=bilinear)
    RandomHorizontalFlip(p=0.5)
    ToTensor()
    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
)
[05/06 05:03:49] SuperNet Training INFO: --> START mobile0-tbs2-seed-0
[05/06 05:03:49] SuperNet Training INFO: {0: 0, 1: 0}
[05/06 05:06:12] SuperNet Training INFO: iter:   120/144360  CE: 6.9142  
[05/06 05:07:15] SuperNet Training INFO: iter:   240/144360  CE: 6.8497  
[05/06 05:08:14] SuperNet Training INFO: iter:   360/144360  CE: 6.9095  
[05/06 05:09:14] SuperNet Training INFO: iter:   480/144360  CE: 6.9284  
[05/06 05:10:17] SuperNet Training INFO: iter:   600/144360  CE: 6.9189  
[05/06 05:11:19] SuperNet Training INFO: iter:   720/144360  CE: 6.9197  
[05/06 05:12:18] SuperNet Training INFO: iter:   840/144360  CE: 6.8994  
[05/06 05:13:18] SuperNet Training INFO: iter:   960/144360  CE: 6.8951  
[05/06 05:14:17] SuperNet Training INFO: iter:  1080/144360  CE: 6.8648  
[05/06 05:15:18] SuperNet Training INFO: iter:  1200/144360  CE: 6.9220  
[05/06 05:15:19] SuperNet Training INFO: --> epoch:   1/120  avg CE: 6.8950  lr: 0.11997943949853311  
[05/06 05:16:55] SuperNet Training INFO: iter:  1320/144360  CE: 6.7889  
[05/06 05:17:58] SuperNet Training INFO: iter:  1440/144360  CE: 6.7757  
[05/06 05:18:59] SuperNet Training INFO: iter:  1560/144360  CE: 6.8084  
[05/06 05:20:00] SuperNet Training INFO: iter:  1680/144360  CE: 6.6900  
[05/06 05:21:00] SuperNet Training INFO: iter:  1800/144360  CE: 6.7320  
[05/06 05:22:01] SuperNet Training INFO: iter:  1920/144360  CE: 6.7042  
[05/06 05:23:02] SuperNet Training INFO: iter:  2040/144360  CE: 6.7514  
[05/06 05:24:03] SuperNet Training INFO: iter:  2160/144360  CE: 6.7218  
[05/06 05:25:05] SuperNet Training INFO: iter:  2280/144360  CE: 6.6384  
[05/06 05:26:07] SuperNet Training INFO: iter:  2400/144360  CE: 6.6348  
[05/06 05:26:09] SuperNet Training INFO: --> epoch:   2/120  avg CE: 6.7687  lr: 0.11991777208527424  
[05/06 05:27:42] SuperNet Training INFO: iter:  2520/144360  CE: 6.6764  
[05/06 05:28:44] SuperNet Training INFO: iter:  2640/144360  CE: 6.7026  
[05/06 05:29:45] SuperNet Training INFO: iter:  2760/144360  CE: 6.5558  
[05/06 05:30:46] SuperNet Training INFO: iter:  2880/144360  CE: 6.5663  
[05/06 05:31:47] SuperNet Training INFO: iter:  3000/144360  CE: 6.7717  
[05/06 05:32:48] SuperNet Training INFO: iter:  3120/144360  CE: 6.5915  
[05/06 05:33:49] SuperNet Training INFO: iter:  3240/144360  CE: 6.6009  
[05/06 05:34:51] SuperNet Training INFO: iter:  3360/144360  CE: 6.7701  
[05/06 05:35:51] SuperNet Training INFO: iter:  3480/144360  CE: 6.6046  
[05/06 05:36:53] SuperNet Training INFO: iter:  3600/144360  CE: 6.4955  
[05/06 05:36:56] SuperNet Training INFO: --> epoch:   3/120  avg CE: 6.6306  lr: 0.11981504002398749  
[05/06 05:38:29] SuperNet Training INFO: iter:  3720/144360  CE: 6.5100  
[05/06 05:39:29] SuperNet Training INFO: iter:  3840/144360  CE: 6.6262  
[05/06 05:40:30] SuperNet Training INFO: iter:  3960/144360  CE: 6.5352  
[05/06 05:41:29] SuperNet Training INFO: iter:  4080/144360  CE: 6.4576  
[05/06 05:42:29] SuperNet Training INFO: iter:  4200/144360  CE: 6.4744  
[05/06 05:43:30] SuperNet Training INFO: iter:  4320/144360  CE: 6.3976  
[05/06 05:44:30] SuperNet Training INFO: iter:  4440/144360  CE: 6.5608  
[05/06 05:45:31] SuperNet Training INFO: iter:  4560/144360  CE: 6.4235  
[05/06 05:46:32] SuperNet Training INFO: iter:  4680/144360  CE: 6.2995  
[05/06 05:47:32] SuperNet Training INFO: iter:  4800/144360  CE: 6.4341  
[05/06 05:47:37] SuperNet Training INFO: --> epoch:   4/120  avg CE: 6.4663  lr: 0.11967131372209595  
[05/06 05:49:08] SuperNet Training INFO: iter:  4920/144360  CE: 6.3643  
[05/06 05:50:09] SuperNet Training INFO: iter:  5040/144360  CE: 6.2360  
[05/06 05:51:11] SuperNet Training INFO: iter:  5160/144360  CE: 6.2993  
[05/06 05:52:13] SuperNet Training INFO: iter:  5280/144360  CE: 6.2624  
[05/06 05:53:14] SuperNet Training INFO: iter:  5400/144360  CE: 6.3236  
[05/06 05:54:15] SuperNet Training INFO: iter:  5520/144360  CE: 6.3634  
[05/06 05:55:16] SuperNet Training INFO: iter:  5640/144360  CE: 6.2005  
[05/06 05:56:16] SuperNet Training INFO: iter:  5760/144360  CE: 6.1766  
[05/06 05:57:18] SuperNet Training INFO: iter:  5880/144360  CE: 6.1889  
[05/06 05:58:19] SuperNet Training INFO: iter:  6000/144360  CE: 6.2526  
[05/06 05:58:26] SuperNet Training INFO: --> epoch:   5/120  avg CE: 6.3057  lr: 0.11948669168242801  
[05/06 05:59:56] SuperNet Training INFO: iter:  6120/144360  CE: 6.1273  
[05/06 06:00:58] SuperNet Training INFO: iter:  6240/144360  CE: 6.3040  
[05/06 06:01:59] SuperNet Training INFO: iter:  6360/144360  CE: 6.0653  
[05/06 06:03:01] SuperNet Training INFO: iter:  6480/144360  CE: 6.2087  
[05/06 06:04:03] SuperNet Training INFO: iter:  6600/144360  CE: 6.2203  
[05/06 06:05:03] SuperNet Training INFO: iter:  6720/144360  CE: 6.1016  
[05/06 06:06:05] SuperNet Training INFO: iter:  6840/144360  CE: 6.3489  
[05/06 06:07:05] SuperNet Training INFO: iter:  6960/144360  CE: 6.1018  
[05/06 06:08:06] SuperNet Training INFO: iter:  7080/144360  CE: 6.1645  
[05/06 06:09:08] SuperNet Training INFO: iter:  7200/144360  CE: 6.1404  
[05/06 06:09:16] SuperNet Training INFO: --> epoch:   6/120  avg CE: 6.1556  lr: 0.1192613004357081  
[05/06 06:10:43] SuperNet Training INFO: iter:  7320/144360  CE: 6.2331  
[05/06 06:11:45] SuperNet Training INFO: iter:  7440/144360  CE: 5.9883  
[05/06 06:12:47] SuperNet Training INFO: iter:  7560/144360  CE: 6.0544  
[05/06 06:13:48] SuperNet Training INFO: iter:  7680/144360  CE: 6.1709  
[05/06 06:14:50] SuperNet Training INFO: iter:  7800/144360  CE: 5.9060  
[05/06 06:15:51] SuperNet Training INFO: iter:  7920/144360  CE: 6.0642  
[05/06 06:16:50] SuperNet Training INFO: iter:  8040/144360  CE: 6.0459  
[05/06 06:17:51] SuperNet Training INFO: iter:  8160/144360  CE: 5.8822  
[05/06 06:18:52] SuperNet Training INFO: iter:  8280/144360  CE: 5.9180  
[05/06 06:19:51] SuperNet Training INFO: iter:  8400/144360  CE: 6.0281  
[05/06 06:20:00] SuperNet Training INFO: --> epoch:   7/120  avg CE: 6.0172  lr: 0.11899529445383715  
[05/06 06:21:27] SuperNet Training INFO: iter:  8520/144360  CE: 6.0200  
[05/06 06:22:28] SuperNet Training INFO: iter:  8640/144360  CE: 5.9460  
[05/06 06:23:29] SuperNet Training INFO: iter:  8760/144360  CE: 5.8914  
[05/06 06:24:30] SuperNet Training INFO: iter:  8880/144360  CE: 5.9092  
[05/06 06:25:32] SuperNet Training INFO: iter:  9000/144360  CE: 5.7626  
[05/06 06:26:34] SuperNet Training INFO: iter:  9120/144360  CE: 5.9077  
[05/06 06:27:35] SuperNet Training INFO: iter:  9240/144360  CE: 5.8565  
[05/06 06:28:35] SuperNet Training INFO: iter:  9360/144360  CE: 5.7245  
[05/06 06:29:37] SuperNet Training INFO: iter:  9480/144360  CE: 5.9078  
[05/06 06:30:38] SuperNet Training INFO: iter:  9600/144360  CE: 5.8421  
[05/06 06:30:49] SuperNet Training INFO: --> epoch:   8/120  avg CE: 5.8688  lr: 0.11868885604402826  
[05/06 06:32:16] SuperNet Training INFO: iter:  9720/144360  CE: 5.8734  
[05/06 06:33:16] SuperNet Training INFO: iter:  9840/144360  CE: 5.8498  
[05/06 06:34:18] SuperNet Training INFO: iter:  9960/144360  CE: 5.6005  
[05/06 06:35:19] SuperNet Training INFO: iter: 10080/144360  CE: 5.6773  
[05/06 06:36:19] SuperNet Training INFO: iter: 10200/144360  CE: 5.8346  
[05/06 06:37:20] SuperNet Training INFO: iter: 10320/144360  CE: 5.6152  
[05/06 06:38:21] SuperNet Training INFO: iter: 10440/144360  CE: 5.6877  
[05/06 06:39:22] SuperNet Training INFO: iter: 10560/144360  CE: 5.7008  
[05/06 06:40:22] SuperNet Training INFO: iter: 10680/144360  CE: 5.7276  
[05/06 06:41:23] SuperNet Training INFO: iter: 10800/144360  CE: 5.5812  
[05/06 06:41:36] SuperNet Training INFO: --> epoch:   9/120  avg CE: 5.7393  lr: 0.11834219522386061  
[05/06 06:43:01] SuperNet Training INFO: iter: 10920/144360  CE: 5.6680  
[05/06 06:44:01] SuperNet Training INFO: iter: 11040/144360  CE: 5.4180  
[05/06 06:45:02] SuperNet Training INFO: iter: 11160/144360  CE: 5.6502  
[05/06 06:46:02] SuperNet Training INFO: iter: 11280/144360  CE: 5.7192  
[05/06 06:47:02] SuperNet Training INFO: iter: 11400/144360  CE: 5.4059  
[05/06 06:48:02] SuperNet Training INFO: iter: 11520/144360  CE: 5.7655  
[05/06 06:49:02] SuperNet Training INFO: iter: 11640/144360  CE: 5.9303  
[05/06 06:50:02] SuperNet Training INFO: iter: 11760/144360  CE: 5.6207  
[05/06 06:51:03] SuperNet Training INFO: iter: 11880/144360  CE: 5.4027  
[05/06 06:52:04] SuperNet Training INFO: iter: 12000/144360  CE: 5.4748  
[05/06 06:52:19] SuperNet Training INFO: --> epoch:  10/120  avg CE: 5.5977  lr: 0.1179555495773443  
[05/06 06:53:40] SuperNet Training INFO: iter: 12120/144360  CE: 5.5838  
[05/06 06:54:41] SuperNet Training INFO: iter: 12240/144360  CE: 5.3771  
[05/06 06:55:40] SuperNet Training INFO: iter: 12360/144360  CE: 5.3476  
[05/06 06:56:41] SuperNet Training INFO: iter: 12480/144360  CE: 5.7085  
[05/06 06:57:44] SuperNet Training INFO: iter: 12600/144360  CE: 5.4792  
[05/06 06:58:44] SuperNet Training INFO: iter: 12720/144360  CE: 5.7335  
[05/06 06:59:44] SuperNet Training INFO: iter: 12840/144360  CE: 5.3586  
[05/06 07:00:45] SuperNet Training INFO: iter: 12960/144360  CE: 5.2862  
[05/06 07:01:46] SuperNet Training INFO: iter: 13080/144360  CE: 5.3087  
[05/06 07:02:45] SuperNet Training INFO: iter: 13200/144360  CE: 5.5134  
[05/06 07:03:01] SuperNet Training INFO: --> epoch:  11/120  avg CE: 5.4682  lr: 0.11752918409209158  
[05/06 07:04:22] SuperNet Training INFO: iter: 13320/144360  CE: 5.3676  
[05/06 07:05:24] SuperNet Training INFO: iter: 13440/144360  CE: 5.0459  
[05/06 07:06:24] SuperNet Training INFO: iter: 13560/144360  CE: 5.4339  
[05/06 07:07:26] SuperNet Training INFO: iter: 13680/144360  CE: 5.6062  
[05/06 07:08:26] SuperNet Training INFO: iter: 13800/144360  CE: 5.3074  
[05/06 07:09:28] SuperNet Training INFO: iter: 13920/144360  CE: 5.3439  
[05/06 07:10:29] SuperNet Training INFO: iter: 14040/144360  CE: 5.3199  
[05/06 07:11:30] SuperNet Training INFO: iter: 14160/144360  CE: 5.3738  
[05/06 07:12:30] SuperNet Training INFO: iter: 14280/144360  CE: 5.2270  
[05/06 07:13:32] SuperNet Training INFO: iter: 14400/144360  CE: 5.4359  
[05/06 07:13:49] SuperNet Training INFO: --> epoch:  12/120  avg CE: 5.3337  lr: 0.11706339097770935  
[05/06 07:15:09] SuperNet Training INFO: iter: 14520/144360  CE: 5.3075  
[05/06 07:16:09] SuperNet Training INFO: iter: 14640/144360  CE: 4.9501  
[05/06 07:17:10] SuperNet Training INFO: iter: 14760/144360  CE: 5.3088  
[05/06 07:18:10] SuperNet Training INFO: iter: 14880/144360  CE: 5.3332  
[05/06 07:19:12] SuperNet Training INFO: iter: 15000/144360  CE: 5.2976  
[05/06 07:20:14] SuperNet Training INFO: iter: 15120/144360  CE: 5.0074  
[05/06 07:21:15] SuperNet Training INFO: iter: 15240/144360  CE: 5.2266  
[05/06 07:22:15] SuperNet Training INFO: iter: 15360/144360  CE: 5.0604  
[05/06 07:23:17] SuperNet Training INFO: iter: 15480/144360  CE: 4.9651  
[05/06 07:24:17] SuperNet Training INFO: iter: 15600/144360  CE: 4.9292  
[05/06 07:24:36] SuperNet Training INFO: --> epoch:  13/120  avg CE: 5.2078  lr: 0.11655848946553125  
[05/06 07:25:54] SuperNet Training INFO: iter: 15720/144360  CE: 5.2396  
[05/06 07:26:55] SuperNet Training INFO: iter: 15840/144360  CE: 4.9173  
[05/06 07:27:55] SuperNet Training INFO: iter: 15960/144360  CE: 4.9636  
[05/06 07:28:54] SuperNet Training INFO: iter: 16080/144360  CE: 4.9316  
[05/06 07:29:54] SuperNet Training INFO: iter: 16200/144360  CE: 5.2330  
[05/06 07:30:54] SuperNet Training INFO: iter: 16320/144360  CE: 5.0314  
[05/06 07:31:53] SuperNet Training INFO: iter: 16440/144360  CE: 5.0269  
[05/06 07:32:54] SuperNet Training INFO: iter: 16560/144360  CE: 5.2908  
[05/06 07:33:55] SuperNet Training INFO: iter: 16680/144360  CE: 5.3594  
[05/06 07:34:57] SuperNet Training INFO: iter: 16800/144360  CE: 5.0140  
[05/06 07:35:18] SuperNet Training INFO: --> epoch:  14/120  avg CE: 5.0866  lr: 0.11601482558983225  
[05/06 07:36:33] SuperNet Training INFO: iter: 16920/144360  CE: 4.9142  
[05/06 07:37:34] SuperNet Training INFO: iter: 17040/144360  CE: 5.0841  
[05/06 07:38:33] SuperNet Training INFO: iter: 17160/144360  CE: 5.0806  
[05/06 07:39:32] SuperNet Training INFO: iter: 17280/144360  CE: 4.7996  
[05/06 07:40:33] SuperNet Training INFO: iter: 17400/144360  CE: 5.1041  
[05/06 07:41:33] SuperNet Training INFO: iter: 17520/144360  CE: 4.8103  
[05/06 07:42:33] SuperNet Training INFO: iter: 17640/144360  CE: 4.8797  
[05/06 07:43:34] SuperNet Training INFO: iter: 17760/144360  CE: 4.9540  
[05/06 07:44:34] SuperNet Training INFO: iter: 17880/144360  CE: 4.8587  
[05/06 07:45:34] SuperNet Training INFO: iter: 18000/144360  CE: 4.9312  
[05/06 07:45:56] SuperNet Training INFO: --> epoch:  15/120  avg CE: 4.9817  lr: 0.11543277195067722  
[05/06 07:47:09] SuperNet Training INFO: iter: 18120/144360  CE: 4.6850  
[05/06 07:48:10] SuperNet Training INFO: iter: 18240/144360  CE: 5.2402  
[05/06 07:49:10] SuperNet Training INFO: iter: 18360/144360  CE: 4.9788  
[05/06 07:50:10] SuperNet Training INFO: iter: 18480/144360  CE: 4.8015  
[05/06 07:51:10] SuperNet Training INFO: iter: 18600/144360  CE: 5.0034  
[05/06 07:52:11] SuperNet Training INFO: iter: 18720/144360  CE: 4.6287  
[05/06 07:53:11] SuperNet Training INFO: iter: 18840/144360  CE: 4.8016  
[05/06 07:54:11] SuperNet Training INFO: iter: 18960/144360  CE: 4.8670  
[05/06 07:55:13] SuperNet Training INFO: iter: 19080/144360  CE: 4.8883  
[05/06 07:56:13] SuperNet Training INFO: iter: 19200/144360  CE: 5.2704  
[05/06 07:56:37] SuperNet Training INFO: --> epoch:  16/120  avg CE: 4.8823  lr: 0.1148127274585561  
[05/06 07:57:48] SuperNet Training INFO: iter: 19320/144360  CE: 4.8332  
[05/06 07:58:48] SuperNet Training INFO: iter: 19440/144360  CE: 4.7647  
[05/06 07:59:48] SuperNet Training INFO: iter: 19560/144360  CE: 4.9522  
[05/06 08:00:49] SuperNet Training INFO: iter: 19680/144360  CE: 4.9511  
[05/06 08:01:49] SuperNet Training INFO: iter: 19800/144360  CE: 4.3738  
[05/06 08:02:50] SuperNet Training INFO: iter: 19920/144360  CE: 4.6685  
[05/06 08:03:51] SuperNet Training INFO: iter: 20040/144360  CE: 4.9959  
[05/06 08:04:50] SuperNet Training INFO: iter: 20160/144360  CE: 4.6660  
[05/06 08:05:50] SuperNet Training INFO: iter: 20280/144360  CE: 4.8651  
[05/06 08:06:52] SuperNet Training INFO: iter: 20400/144360  CE: 4.7254  
[05/06 08:07:17] SuperNet Training INFO: --> epoch:  17/120  avg CE: 4.7979  lr: 0.11415511706099139  
[05/06 08:08:27] SuperNet Training INFO: iter: 20520/144360  CE: 4.9003  
[05/06 08:09:27] SuperNet Training INFO: iter: 20640/144360  CE: 4.7005  
[05/06 08:10:29] SuperNet Training INFO: iter: 20760/144360  CE: 4.5945  
[05/06 08:11:28] SuperNet Training INFO: iter: 20880/144360  CE: 4.9613  
[05/06 08:12:27] SuperNet Training INFO: iter: 21000/144360  CE: 4.9661  
[05/06 08:13:28] SuperNet Training INFO: iter: 21120/144360  CE: 5.0443  
[05/06 08:14:28] SuperNet Training INFO: iter: 21240/144360  CE: 4.9369  
[05/06 08:15:27] SuperNet Training INFO: iter: 21360/144360  CE: 4.6660  
[05/06 08:16:28] SuperNet Training INFO: iter: 21480/144360  CE: 4.7311  
[05/06 08:17:28] SuperNet Training INFO: iter: 21600/144360  CE: 4.7517  
[05/06 08:17:54] SuperNet Training INFO: --> epoch:  18/120  avg CE: 4.7256  lr: 0.11346039145130195  
[05/06 08:19:05] SuperNet Training INFO: iter: 21720/144360  CE: 4.5908  
[05/06 08:20:05] SuperNet Training INFO: iter: 21840/144360  CE: 4.5107  
[05/06 08:21:06] SuperNet Training INFO: iter: 21960/144360  CE: 4.6963  
[05/06 08:22:06] SuperNet Training INFO: iter: 22080/144360  CE: 4.3832  
[05/06 08:23:07] SuperNet Training INFO: iter: 22200/144360  CE: 4.8649  
[05/06 08:24:08] SuperNet Training INFO: iter: 22320/144360  CE: 4.4140  
[05/06 08:25:08] SuperNet Training INFO: iter: 22440/144360  CE: 4.7231  
[05/06 08:26:08] SuperNet Training INFO: iter: 22560/144360  CE: 4.7187  
[05/06 08:27:09] SuperNet Training INFO: iter: 22680/144360  CE: 4.7676  
[05/06 08:28:10] SuperNet Training INFO: iter: 22800/144360  CE: 4.8314  
[05/06 08:28:37] SuperNet Training INFO: --> epoch:  19/120  avg CE: 4.6500  lr: 0.11272902675971772  
[05/06 08:29:45] SuperNet Training INFO: iter: 22920/144360  CE: 4.6535  
[05/06 08:30:45] SuperNet Training INFO: iter: 23040/144360  CE: 4.3832  
[05/06 08:31:46] SuperNet Training INFO: iter: 23160/144360  CE: 4.5545  
[05/06 08:32:45] SuperNet Training INFO: iter: 23280/144360  CE: 4.8545  
[05/06 08:33:44] SuperNet Training INFO: iter: 23400/144360  CE: 4.7068  
[05/06 08:34:44] SuperNet Training INFO: iter: 23520/144360  CE: 4.6518  
[05/06 08:35:44] SuperNet Training INFO: iter: 23640/144360  CE: 4.7257  
[05/06 08:36:43] SuperNet Training INFO: iter: 23760/144360  CE: 4.6413  
[05/06 08:37:42] SuperNet Training INFO: iter: 23880/144360  CE: 4.5614  
[05/06 08:38:44] SuperNet Training INFO: iter: 24000/144360  CE: 4.5503  
[05/06 08:39:13] SuperNet Training INFO: --> epoch:  20/120  avg CE: 4.5782  lr: 0.11196152422706572  
[05/06 08:40:19] SuperNet Training INFO: iter: 24120/144360  CE: 4.5897  
[05/06 08:41:18] SuperNet Training INFO: iter: 24240/144360  CE: 5.1741  
[05/06 08:42:19] SuperNet Training INFO: iter: 24360/144360  CE: 4.5054  
[05/06 08:43:19] SuperNet Training INFO: iter: 24480/144360  CE: 4.1154  
[05/06 08:44:19] SuperNet Training INFO: iter: 24600/144360  CE: 4.6875  
[05/06 08:45:20] SuperNet Training INFO: iter: 24720/144360  CE: 4.1327  
[05/06 08:46:21] SuperNet Training INFO: iter: 24840/144360  CE: 4.3891  
[05/06 08:47:23] SuperNet Training INFO: iter: 24960/144360  CE: 4.5006  
[05/06 08:48:23] SuperNet Training INFO: iter: 25080/144360  CE: 4.5469  
[05/06 08:49:24] SuperNet Training INFO: iter: 25200/144360  CE: 4.6017  
[05/06 08:49:54] SuperNet Training INFO: --> epoch:  21/120  avg CE: 4.5131  lr: 0.11115840986124514  
[05/06 08:50:59] SuperNet Training INFO: iter: 25320/144360  CE: 4.4007  
[05/06 08:52:02] SuperNet Training INFO: iter: 25440/144360  CE: 4.6734  
[05/06 08:53:04] SuperNet Training INFO: iter: 25560/144360  CE: 4.9708  
[05/06 08:54:05] SuperNet Training INFO: iter: 25680/144360  CE: 4.4213  
[05/06 08:55:06] SuperNet Training INFO: iter: 25800/144360  CE: 4.4695  
[05/06 08:56:08] SuperNet Training INFO: iter: 25920/144360  CE: 4.4823  
[05/06 08:57:10] SuperNet Training INFO: iter: 26040/144360  CE: 4.5840  
[05/06 08:58:11] SuperNet Training INFO: iter: 26160/144360  CE: 4.5055  
[05/06 08:59:11] SuperNet Training INFO: iter: 26280/144360  CE: 4.7408  
[05/06 09:00:13] SuperNet Training INFO: iter: 26400/144360  CE: 4.4524  
[05/06 09:00:46] SuperNet Training INFO: --> epoch:  22/120  avg CE: 4.4531  lr: 0.11032023407672516  
[05/06 09:01:50] SuperNet Training INFO: iter: 26520/144360  CE: 4.2710  
[05/06 09:02:50] SuperNet Training INFO: iter: 26640/144360  CE: 4.4572  
[05/06 09:03:50] SuperNet Training INFO: iter: 26760/144360  CE: 4.4950  
[05/06 09:04:52] SuperNet Training INFO: iter: 26880/144360  CE: 4.2643  
[05/06 09:05:54] SuperNet Training INFO: iter: 27000/144360  CE: 4.2831  
[05/06 09:06:55] SuperNet Training INFO: iter: 27120/144360  CE: 4.5481  
[05/06 09:07:55] SuperNet Training INFO: iter: 27240/144360  CE: 4.4910  
[05/06 09:08:57] SuperNet Training INFO: iter: 27360/144360  CE: 4.2120  
[05/06 09:09:59] SuperNet Training INFO: iter: 27480/144360  CE: 4.2624  
[05/06 09:11:00] SuperNet Training INFO: iter: 27600/144360  CE: 4.6541  
[05/06 09:11:34] SuperNet Training INFO: --> epoch:  23/120  avg CE: 4.3959  lr: 0.10944757131732062  
[05/06 09:12:37] SuperNet Training INFO: iter: 27720/144360  CE: 4.3630  
[05/06 09:13:37] SuperNet Training INFO: iter: 27840/144360  CE: 4.3747  
[05/06 09:14:37] SuperNet Training INFO: iter: 27960/144360  CE: 4.5002  
[05/06 09:15:39] SuperNet Training INFO: iter: 28080/144360  CE: 4.3151  
[05/06 09:16:41] SuperNet Training INFO: iter: 28200/144360  CE: 4.2702  
[05/06 09:17:42] SuperNet Training INFO: iter: 28320/144360  CE: 4.0633  
[05/06 09:18:44] SuperNet Training INFO: iter: 28440/144360  CE: 4.3382  
[05/06 09:19:45] SuperNet Training INFO: iter: 28560/144360  CE: 4.1634  
[05/06 09:20:45] SuperNet Training INFO: iter: 28680/144360  CE: 4.2014  
[05/06 09:21:44] SuperNet Training INFO: iter: 28800/144360  CE: 4.4310  
[05/06 09:22:19] SuperNet Training INFO: --> epoch:  24/120  avg CE: 4.3505  lr: 0.10854101966249682  
[05/06 09:23:21] SuperNet Training INFO: iter: 28920/144360  CE: 4.7218  
[05/06 09:24:23] SuperNet Training INFO: iter: 29040/144360  CE: 4.1057  
[05/06 09:25:25] SuperNet Training INFO: iter: 29160/144360  CE: 4.4706  
[05/06 09:26:27] SuperNet Training INFO: iter: 29280/144360  CE: 4.2820  
[05/06 09:27:27] SuperNet Training INFO: iter: 29400/144360  CE: 4.3026  
[05/06 09:28:25] SuperNet Training INFO: iter: 29520/144360  CE: 3.8954  
[05/06 09:29:24] SuperNet Training INFO: iter: 29640/144360  CE: 4.2352  
[05/06 09:30:24] SuperNet Training INFO: iter: 29760/144360  CE: 4.1001  
[05/06 09:31:23] SuperNet Training INFO: iter: 29880/144360  CE: 4.1953  
[05/06 09:32:23] SuperNet Training INFO: iter: 30000/144360  CE: 4.1915  
[05/06 09:32:58] SuperNet Training INFO: --> epoch:  25/120  avg CE: 4.3075  lr: 0.10760120041747454  
[05/06 09:33:57] SuperNet Training INFO: iter: 30120/144360  CE: 4.3135  
[05/06 09:34:58] SuperNet Training INFO: iter: 30240/144360  CE: 4.1997  
[05/06 09:35:59] SuperNet Training INFO: iter: 30360/144360  CE: 4.2238  
[05/06 09:36:59] SuperNet Training INFO: iter: 30480/144360  CE: 3.9830  
[05/06 09:38:00] SuperNet Training INFO: iter: 30600/144360  CE: 4.1050  
[05/06 09:39:00] SuperNet Training INFO: iter: 30720/144360  CE: 4.1245  
[05/06 09:40:00] SuperNet Training INFO: iter: 30840/144360  CE: 4.2088  
[05/06 09:41:00] SuperNet Training INFO: iter: 30960/144360  CE: 4.3095  
[05/06 09:42:00] SuperNet Training INFO: iter: 31080/144360  CE: 4.0005  
[05/06 09:43:01] SuperNet Training INFO: iter: 31200/144360  CE: 4.3071  
[05/06 09:43:39] SuperNet Training INFO: --> epoch:  26/120  avg CE: 4.2546  lr: 0.10662875768741867  
[05/06 09:44:35] SuperNet Training INFO: iter: 31320/144360  CE: 4.0971  
[05/06 09:45:36] SuperNet Training INFO: iter: 31440/144360  CE: 4.4201  
[05/06 09:46:37] SuperNet Training INFO: iter: 31560/144360  CE: 4.1905  
[05/06 09:47:38] SuperNet Training INFO: iter: 31680/144360  CE: 4.2056  
[05/06 09:48:38] SuperNet Training INFO: iter: 31800/144360  CE: 3.9884  
[05/06 09:49:38] SuperNet Training INFO: iter: 31920/144360  CE: 4.3764  
[05/06 09:50:39] SuperNet Training INFO: iter: 32040/144360  CE: 4.3889  
[05/06 09:51:39] SuperNet Training INFO: iter: 32160/144360  CE: 4.1216  
[05/06 09:52:39] SuperNet Training INFO: iter: 32280/144360  CE: 4.3717  
[05/06 09:53:39] SuperNet Training INFO: iter: 32400/144360  CE: 4.1443  
[05/06 09:54:18] SuperNet Training INFO: --> epoch:  27/120  avg CE: 4.2109  lr: 0.10562435793600224  
[05/06 09:55:13] SuperNet Training INFO: iter: 32520/144360  CE: 4.3213  
[05/06 09:56:13] SuperNet Training INFO: iter: 32640/144360  CE: 4.5047  
[05/06 09:57:14] SuperNet Training INFO: iter: 32760/144360  CE: 4.2496  
[05/06 09:58:14] SuperNet Training INFO: iter: 32880/144360  CE: 4.0996  
[05/06 09:59:14] SuperNet Training INFO: iter: 33000/144360  CE: 4.0853  
[05/06 10:00:16] SuperNet Training INFO: iter: 33120/144360  CE: 4.4071  
[05/06 10:01:18] SuperNet Training INFO: iter: 33240/144360  CE: 4.3779  
[05/06 10:02:19] SuperNet Training INFO: iter: 33360/144360  CE: 4.4314  
[05/06 10:03:21] SuperNet Training INFO: iter: 33480/144360  CE: 3.8822  
[05/06 10:04:22] SuperNet Training INFO: iter: 33600/144360  CE: 3.6704  
[05/06 10:05:04] SuperNet Training INFO: --> epoch:  28/120  avg CE: 4.1748  lr: 0.10458868952864393  
[05/06 10:05:58] SuperNet Training INFO: iter: 33720/144360  CE: 3.8520  
[05/06 10:06:59] SuperNet Training INFO: iter: 33840/144360  CE: 4.0868  
[05/06 10:07:59] SuperNet Training INFO: iter: 33960/144360  CE: 3.9505  
[05/06 10:09:00] SuperNet Training INFO: iter: 34080/144360  CE: 4.4918  
[05/06 10:10:00] SuperNet Training INFO: iter: 34200/144360  CE: 4.3087  
[05/06 10:11:00] SuperNet Training INFO: iter: 34320/144360  CE: 4.1879  
[05/06 10:12:00] SuperNet Training INFO: iter: 34440/144360  CE: 4.0011  
[05/06 10:12:59] SuperNet Training INFO: iter: 34560/144360  CE: 4.4156  
[05/06 10:13:58] SuperNet Training INFO: iter: 34680/144360  CE: 4.1587  
[05/06 10:14:58] SuperNet Training INFO: iter: 34800/144360  CE: 3.9121  
[05/06 10:15:40] SuperNet Training INFO: --> epoch:  29/120  avg CE: 4.1421  lr: 0.10352246226073762  
[05/06 10:16:33] SuperNet Training INFO: iter: 34920/144360  CE: 3.9891  
[05/06 10:17:34] SuperNet Training INFO: iter: 35040/144360  CE: 4.0574  
[05/06 10:18:35] SuperNet Training INFO: iter: 35160/144360  CE: 4.4791  
[05/06 10:19:35] SuperNet Training INFO: iter: 35280/144360  CE: 3.9962  
[05/06 10:20:35] SuperNet Training INFO: iter: 35400/144360  CE: 4.0699  
[05/06 10:21:34] SuperNet Training INFO: iter: 35520/144360  CE: 4.3129  
[05/06 10:22:35] SuperNet Training INFO: iter: 35640/144360  CE: 4.2803  
[05/06 10:23:35] SuperNet Training INFO: iter: 35760/144360  CE: 4.1943  
[05/06 10:24:36] SuperNet Training INFO: iter: 35880/144360  CE: 4.4493  
[05/06 10:25:37] SuperNet Training INFO: iter: 36000/144360  CE: 3.9766  
[05/06 10:26:22] SuperNet Training INFO: --> epoch:  30/120  avg CE: 4.1053  lr: 0.10242640687119343  
[05/06 10:27:13] SuperNet Training INFO: iter: 36120/144360  CE: 4.2972  
[05/06 10:28:15] SuperNet Training INFO: iter: 36240/144360  CE: 4.4262  
[05/06 10:29:17] SuperNet Training INFO: iter: 36360/144360  CE: 4.1745  
[05/06 10:30:19] SuperNet Training INFO: iter: 36480/144360  CE: 4.1979  
[05/06 10:31:22] SuperNet Training INFO: iter: 36600/144360  CE: 4.1649  
[05/06 10:32:22] SuperNet Training INFO: iter: 36720/144360  CE: 3.9645  
[05/06 10:33:22] SuperNet Training INFO: iter: 36840/144360  CE: 4.0843  
[05/06 10:34:21] SuperNet Training INFO: iter: 36960/144360  CE: 4.4788  
[05/06 10:35:21] SuperNet Training INFO: iter: 37080/144360  CE: 4.2335  
[05/06 10:36:21] SuperNet Training INFO: iter: 37200/144360  CE: 3.8332  
[05/06 10:37:07] SuperNet Training INFO: --> epoch:  31/120  avg CE: 4.0687  lr: 0.10130127454162571  
[05/06 10:37:57] SuperNet Training INFO: iter: 37320/144360  CE: 4.0210  
[05/06 10:38:57] SuperNet Training INFO: iter: 37440/144360  CE: 3.9821  
[05/06 10:39:58] SuperNet Training INFO: iter: 37560/144360  CE: 4.1743  
[05/06 10:40:58] SuperNet Training INFO: iter: 37680/144360  CE: 4.1823  
[05/06 10:41:57] SuperNet Training INFO: iter: 37800/144360  CE: 4.1946  
[05/06 10:42:57] SuperNet Training INFO: iter: 37920/144360  CE: 4.0310  
[05/06 10:43:58] SuperNet Training INFO: iter: 38040/144360  CE: 4.1494  
[05/06 10:44:58] SuperNet Training INFO: iter: 38160/144360  CE: 4.0688  
[05/06 10:45:57] SuperNet Training INFO: iter: 38280/144360  CE: 4.1814  
[05/06 10:46:58] SuperNet Training INFO: iter: 38400/144360  CE: 3.8797  
[05/06 10:47:46] SuperNet Training INFO: --> epoch:  32/120  avg CE: 4.0476  lr: 0.10014783638153192  
[05/06 10:48:35] SuperNet Training INFO: iter: 38520/144360  CE: 4.1818  
[05/06 10:49:35] SuperNet Training INFO: iter: 38640/144360  CE: 4.8392  
[05/06 10:50:35] SuperNet Training INFO: iter: 38760/144360  CE: 4.0683  
[05/06 10:51:35] SuperNet Training INFO: iter: 38880/144360  CE: 4.0264  
[05/06 10:52:35] SuperNet Training INFO: iter: 39000/144360  CE: 3.8743  
[05/06 10:53:35] SuperNet Training INFO: iter: 39120/144360  CE: 3.7609  
[05/06 10:54:35] SuperNet Training INFO: iter: 39240/144360  CE: 3.9099  
[05/06 10:55:36] SuperNet Training INFO: iter: 39360/144360  CE: 4.0294  
[05/06 10:56:37] SuperNet Training INFO: iter: 39480/144360  CE: 4.0767  
[05/06 10:57:38] SuperNet Training INFO: iter: 39600/144360  CE: 4.3696  
[05/06 10:58:27] SuperNet Training INFO: --> epoch:  33/120  avg CE: 4.0157  lr: 0.09896688289981138  
[05/06 10:59:14] SuperNet Training INFO: iter: 39720/144360  CE: 3.9496  
[05/06 11:00:15] SuperNet Training INFO: iter: 39840/144360  CE: 3.8723  
[05/06 11:01:15] SuperNet Training INFO: iter: 39960/144360  CE: 4.0960  
[05/06 11:02:16] SuperNet Training INFO: iter: 40080/144360  CE: 3.8524  
[05/06 11:03:17] SuperNet Training INFO: iter: 40200/144360  CE: 4.3208  
[05/06 11:04:17] SuperNet Training INFO: iter: 40320/144360  CE: 3.8482  
[05/06 11:05:18] SuperNet Training INFO: iter: 40440/144360  CE: 4.2446  
[05/06 11:06:19] SuperNet Training INFO: iter: 40560/144360  CE: 4.0427  
[05/06 11:07:19] SuperNet Training INFO: iter: 40680/144360  CE: 3.9316  
[05/06 11:08:20] SuperNet Training INFO: iter: 40800/144360  CE: 4.0827  
[05/06 11:09:10] SuperNet Training INFO: --> epoch:  34/120  avg CE: 3.9801  lr: 0.09775922346299062  
[05/06 11:09:56] SuperNet Training INFO: iter: 40920/144360  CE: 4.3881  
[05/06 11:10:56] SuperNet Training INFO: iter: 41040/144360  CE: 3.7738  
[05/06 11:11:58] SuperNet Training INFO: iter: 41160/144360  CE: 3.7622  
[05/06 11:12:57] SuperNet Training INFO: iter: 41280/144360  CE: 3.9992  
[05/06 11:13:58] SuperNet Training INFO: iter: 41400/144360  CE: 4.0216  
[05/06 11:14:59] SuperNet Training INFO: iter: 41520/144360  CE: 3.7775  
[05/06 11:16:00] SuperNet Training INFO: iter: 41640/144360  CE: 3.7414  
[05/06 11:16:59] SuperNet Training INFO: iter: 41760/144360  CE: 3.9319  
[05/06 11:18:00] SuperNet Training INFO: iter: 41880/144360  CE: 4.1431  
[05/06 11:19:00] SuperNet Training INFO: iter: 42000/144360  CE: 3.9683  
[05/06 11:19:52] SuperNet Training INFO: --> epoch:  35/120  avg CE: 3.9569  lr: 0.09652568574052359  
[05/06 11:20:35] SuperNet Training INFO: iter: 42120/144360  CE: 4.1140  
[05/06 11:21:36] SuperNet Training INFO: iter: 42240/144360  CE: 4.1191  
[05/06 11:22:38] SuperNet Training INFO: iter: 42360/144360  CE: 4.0573  
[05/06 11:23:38] SuperNet Training INFO: iter: 42480/144360  CE: 4.0138  
[05/06 11:24:38] SuperNet Training INFO: iter: 42600/144360  CE: 4.0008  
[05/06 11:25:39] SuperNet Training INFO: iter: 42720/144360  CE: 3.8988  
[05/06 11:26:42] SuperNet Training INFO: iter: 42840/144360  CE: 4.0503  
[05/06 11:27:42] SuperNet Training INFO: iter: 42960/144360  CE: 3.8772  
[05/06 11:28:43] SuperNet Training INFO: iter: 43080/144360  CE: 3.7853  
[05/06 11:29:43] SuperNet Training INFO: iter: 43200/144360  CE: 3.8961  
[05/06 11:30:37] SuperNet Training INFO: --> epoch:  36/120  avg CE: 3.9242  lr: 0.09526711513754865  
[05/06 11:31:19] SuperNet Training INFO: iter: 43320/144360  CE: 4.0097  
[05/06 11:32:20] SuperNet Training INFO: iter: 43440/144360  CE: 3.9486  
[05/06 11:33:20] SuperNet Training INFO: iter: 43560/144360  CE: 3.9700  
[05/06 11:34:20] SuperNet Training INFO: iter: 43680/144360  CE: 3.9984  
[05/06 11:35:20] SuperNet Training INFO: iter: 43800/144360  CE: 3.6664  
[05/06 11:36:21] SuperNet Training INFO: iter: 43920/144360  CE: 3.9715  
[05/06 11:37:21] SuperNet Training INFO: iter: 44040/144360  CE: 3.9290  
[05/06 11:38:21] SuperNet Training INFO: iter: 44160/144360  CE: 3.6069  
[05/06 11:39:22] SuperNet Training INFO: iter: 44280/144360  CE: 3.7909  
[05/06 11:40:23] SuperNet Training INFO: iter: 44400/144360  CE: 4.1682  
[05/06 11:41:18] SuperNet Training INFO: --> epoch:  37/120  avg CE: 3.8924  lr: 0.0939843742154901  
[05/06 11:41:58] SuperNet Training INFO: iter: 44520/144360  CE: 3.9734  
[05/06 11:42:59] SuperNet Training INFO: iter: 44640/144360  CE: 3.9052  
[05/06 11:44:01] SuperNet Training INFO: iter: 44760/144360  CE: 4.1812  
[05/06 11:45:02] SuperNet Training INFO: iter: 44880/144360  CE: 3.8728  
[05/06 11:46:02] SuperNet Training INFO: iter: 45000/144360  CE: 3.9373  
[05/06 11:47:02] SuperNet Training INFO: iter: 45120/144360  CE: 4.0726  
[05/06 11:48:02] SuperNet Training INFO: iter: 45240/144360  CE: 3.8479  
[05/06 11:49:02] SuperNet Training INFO: iter: 45360/144360  CE: 4.0489  
[05/06 11:50:02] SuperNet Training INFO: iter: 45480/144360  CE: 3.6844  
[05/06 11:51:01] SuperNet Training INFO: iter: 45600/144360  CE: 4.1470  
[05/06 11:51:57] SuperNet Training INFO: --> epoch:  38/120  avg CE: 3.8765  lr: 0.0926783421009017  
[05/06 11:52:36] SuperNet Training INFO: iter: 45720/144360  CE: 3.9028  
[05/06 11:53:36] SuperNet Training INFO: iter: 45840/144360  CE: 3.8930  
[05/06 11:54:37] SuperNet Training INFO: iter: 45960/144360  CE: 3.7108  
[05/06 11:55:39] SuperNet Training INFO: iter: 46080/144360  CE: 3.7645  
[05/06 11:56:39] SuperNet Training INFO: iter: 46200/144360  CE: 4.1120  
[05/06 11:57:40] SuperNet Training INFO: iter: 46320/144360  CE: 3.5424  
[05/06 11:58:41] SuperNet Training INFO: iter: 46440/144360  CE: 3.5941  
[05/06 11:59:42] SuperNet Training INFO: iter: 46560/144360  CE: 4.0040  
[05/06 12:00:43] SuperNet Training INFO: iter: 46680/144360  CE: 3.8456  
[05/06 12:01:43] SuperNet Training INFO: iter: 46800/144360  CE: 3.8780  
[05/06 12:02:41] SuperNet Training INFO: --> epoch:  39/120  avg CE: 3.8567  lr: 0.09134991388295689  
[05/06 12:03:18] SuperNet Training INFO: iter: 46920/144360  CE: 3.7544  
[05/06 12:04:18] SuperNet Training INFO: iter: 47040/144360  CE: 3.9974  
[05/06 12:05:19] SuperNet Training INFO: iter: 47160/144360  CE: 4.0689  
[05/06 12:06:19] SuperNet Training INFO: iter: 47280/144360  CE: 3.8096  
[05/06 12:07:19] SuperNet Training INFO: iter: 47400/144360  CE: 3.8935  
[05/06 12:08:20] SuperNet Training INFO: iter: 47520/144360  CE: 3.8040  
[05/06 12:09:20] SuperNet Training INFO: iter: 47640/144360  CE: 3.9804  
[05/06 12:10:20] SuperNet Training INFO: iter: 47760/144360  CE: 3.5378  
[05/06 12:11:20] SuperNet Training INFO: iter: 47880/144360  CE: 3.5858  
[05/06 12:12:21] SuperNet Training INFO: iter: 48000/144360  CE: 3.7644  
[05/06 12:13:20] SuperNet Training INFO: iter: 48120/144360  CE: 4.1375  
[05/06 12:13:20] SuperNet Training INFO: --> epoch:  40/120  avg CE: 3.8278  lr: 0.0899999999999998  
[05/06 12:14:58] SuperNet Training INFO: iter: 48240/144360  CE: 3.6177  
[05/06 12:15:58] SuperNet Training INFO: iter: 48360/144360  CE: 3.8082  
[05/06 12:17:00] SuperNet Training INFO: iter: 48480/144360  CE: 3.9401  
[05/06 12:18:02] SuperNet Training INFO: iter: 48600/144360  CE: 3.5698  
[05/06 12:19:03] SuperNet Training INFO: iter: 48720/144360  CE: 4.0962  
[05/06 12:20:04] SuperNet Training INFO: iter: 48840/144360  CE: 4.1758  
[05/06 12:21:03] SuperNet Training INFO: iter: 48960/144360  CE: 3.7320  
[05/06 12:22:04] SuperNet Training INFO: iter: 49080/144360  CE: 4.0047  
[05/06 12:23:05] SuperNet Training INFO: iter: 49200/144360  CE: 3.9255  
[05/06 12:24:05] SuperNet Training INFO: iter: 49320/144360  CE: 3.6712  
[05/06 12:24:06] SuperNet Training INFO: --> epoch:  41/120  avg CE: 3.7978  lr: 0.08862952561557644  
[05/06 12:25:42] SuperNet Training INFO: iter: 49440/144360  CE: 3.7728  
[05/06 12:26:43] SuperNet Training INFO: iter: 49560/144360  CE: 3.7208  
[05/06 12:27:44] SuperNet Training INFO: iter: 49680/144360  CE: 3.8573  
[05/06 12:28:47] SuperNet Training INFO: iter: 49800/144360  CE: 3.6782  
[05/06 12:29:47] SuperNet Training INFO: iter: 49920/144360  CE: 3.7544  
[05/06 12:30:49] SuperNet Training INFO: iter: 50040/144360  CE: 3.9510  
[05/06 12:31:50] SuperNet Training INFO: iter: 50160/144360  CE: 3.7441  
[05/06 12:32:50] SuperNet Training INFO: iter: 50280/144360  CE: 3.7114  
[05/06 12:33:52] SuperNet Training INFO: iter: 50400/144360  CE: 3.9702  
[05/06 12:34:53] SuperNet Training INFO: iter: 50520/144360  CE: 3.7165  
[05/06 12:34:55] SuperNet Training INFO: --> epoch:  42/120  avg CE: 3.7748  lr: 0.08723942998437267  
[05/06 12:36:28] SuperNet Training INFO: iter: 50640/144360  CE: 3.7652  
[05/06 12:37:29] SuperNet Training INFO: iter: 50760/144360  CE: 3.7474  
[05/06 12:38:30] SuperNet Training INFO: iter: 50880/144360  CE: 3.8661  
[05/06 12:39:31] SuperNet Training INFO: iter: 51000/144360  CE: 3.7496  
[05/06 12:40:32] SuperNet Training INFO: iter: 51120/144360  CE: 3.9101  
[05/06 12:41:33] SuperNet Training INFO: iter: 51240/144360  CE: 3.6412  
[05/06 12:42:33] SuperNet Training INFO: iter: 51360/144360  CE: 3.4972  
[05/06 12:43:33] SuperNet Training INFO: iter: 51480/144360  CE: 3.8033  
[05/06 12:44:32] SuperNet Training INFO: iter: 51600/144360  CE: 3.5826  
[05/06 12:45:33] SuperNet Training INFO: iter: 51720/144360  CE: 3.3385  
[05/06 12:45:37] SuperNet Training INFO: --> epoch:  43/120  avg CE: 3.7619  lr: 0.08583066580849745  
[05/06 12:47:10] SuperNet Training INFO: iter: 51840/144360  CE: 3.8684  
[05/06 12:48:12] SuperNet Training INFO: iter: 51960/144360  CE: 3.5171  
[05/06 12:49:13] SuperNet Training INFO: iter: 52080/144360  CE: 3.6101  
[05/06 12:50:13] SuperNet Training INFO: iter: 52200/144360  CE: 3.8150  
[05/06 12:51:13] SuperNet Training INFO: iter: 52320/144360  CE: 3.8124  
[05/06 12:52:13] SuperNet Training INFO: iter: 52440/144360  CE: 3.8510  
[05/06 12:53:14] SuperNet Training INFO: iter: 52560/144360  CE: 3.5558  
[05/06 12:54:15] SuperNet Training INFO: iter: 52680/144360  CE: 3.7111  
[05/06 12:55:15] SuperNet Training INFO: iter: 52800/144360  CE: 3.9816  
[05/06 12:56:16] SuperNet Training INFO: iter: 52920/144360  CE: 3.7368  
[05/06 12:56:21] SuperNet Training INFO: --> epoch:  44/120  avg CE: 3.7356  lr: 0.08440419858454766  
[05/06 12:57:51] SuperNet Training INFO: iter: 53040/144360  CE: 3.8623  
[05/06 12:58:52] SuperNet Training INFO: iter: 53160/144360  CE: 3.9189  
[05/06 12:59:52] SuperNet Training INFO: iter: 53280/144360  CE: 3.8612  
[05/06 13:00:53] SuperNet Training INFO: iter: 53400/144360  CE: 3.7349  
[05/06 13:01:54] SuperNet Training INFO: iter: 53520/144360  CE: 3.7047  
[05/06 13:02:56] SuperNet Training INFO: iter: 53640/144360  CE: 3.5709  
[05/06 13:03:56] SuperNet Training INFO: iter: 53760/144360  CE: 3.6833  
[05/06 13:04:57] SuperNet Training INFO: iter: 53880/144360  CE: 3.9201  
[05/06 13:05:58] SuperNet Training INFO: iter: 54000/144360  CE: 3.4817  
[05/06 13:07:00] SuperNet Training INFO: iter: 54120/144360  CE: 3.9436  
[05/06 13:07:06] SuperNet Training INFO: --> epoch:  45/120  avg CE: 3.7200  lr: 0.0829610059419049  
[05/06 13:08:36] SuperNet Training INFO: iter: 54240/144360  CE: 3.8728  
[05/06 13:09:36] SuperNet Training INFO: iter: 54360/144360  CE: 3.7321  
[05/06 13:10:38] SuperNet Training INFO: iter: 54480/144360  CE: 3.6476  
[05/06 13:11:39] SuperNet Training INFO: iter: 54600/144360  CE: 3.7486  
[05/06 13:12:39] SuperNet Training INFO: iter: 54720/144360  CE: 3.4239  
[05/06 13:13:38] SuperNet Training INFO: iter: 54840/144360  CE: 3.7645  
[05/06 13:14:38] SuperNet Training INFO: iter: 54960/144360  CE: 3.6456  
[05/06 13:15:37] SuperNet Training INFO: iter: 55080/144360  CE: 4.0352  
[05/06 13:16:37] SuperNet Training INFO: iter: 55200/144360  CE: 3.7350  
[05/06 13:17:37] SuperNet Training INFO: iter: 55320/144360  CE: 3.5846  
[05/06 13:17:45] SuperNet Training INFO: --> epoch:  46/120  avg CE: 3.6999  lr: 0.08150207697271764  
[05/06 13:19:13] SuperNet Training INFO: iter: 55440/144360  CE: 3.8621  
[05/06 13:20:13] SuperNet Training INFO: iter: 55560/144360  CE: 3.6235  
[05/06 13:21:14] SuperNet Training INFO: iter: 55680/144360  CE: 3.4857  
[05/06 13:22:13] SuperNet Training INFO: iter: 55800/144360  CE: 3.8007  
[05/06 13:23:15] SuperNet Training INFO: iter: 55920/144360  CE: 3.9345  
[05/06 13:24:16] SuperNet Training INFO: iter: 56040/144360  CE: 3.6398  
[05/06 13:25:17] SuperNet Training INFO: iter: 56160/144360  CE: 3.5363  
[05/06 13:26:18] SuperNet Training INFO: iter: 56280/144360  CE: 3.1631  
[05/06 13:27:19] SuperNet Training INFO: iter: 56400/144360  CE: 3.1919  
[05/06 13:28:19] SuperNet Training INFO: iter: 56520/144360  CE: 3.5328  
[05/06 13:28:28] SuperNet Training INFO: --> epoch:  47/120  avg CE: 3.6716  lr: 0.08002841155402596  
[05/06 13:29:54] SuperNet Training INFO: iter: 56640/144360  CE: 3.5984  
[05/06 13:30:55] SuperNet Training INFO: iter: 56760/144360  CE: 3.6515  
[05/06 13:31:55] SuperNet Training INFO: iter: 56880/144360  CE: 3.8573  
[05/06 13:32:56] SuperNet Training INFO: iter: 57000/144360  CE: 3.7390  
[05/06 13:33:57] SuperNet Training INFO: iter: 57120/144360  CE: 3.7031  
[05/06 13:34:57] SuperNet Training INFO: iter: 57240/144360  CE: 3.5874  
[05/06 13:35:59] SuperNet Training INFO: iter: 57360/144360  CE: 3.9112  
[05/06 13:36:59] SuperNet Training INFO: iter: 57480/144360  CE: 3.5131  
[05/06 13:37:59] SuperNet Training INFO: iter: 57600/144360  CE: 3.8065  
[05/06 13:39:00] SuperNet Training INFO: iter: 57720/144360  CE: 3.4420  
[05/06 13:39:12] SuperNet Training INFO: --> epoch:  48/120  avg CE: 3.6629  lr: 0.07854101966249659  
[05/06 13:40:37] SuperNet Training INFO: iter: 57840/144360  CE: 3.6095  
[05/06 13:41:37] SuperNet Training INFO: iter: 57960/144360  CE: 3.6755  
[05/06 13:42:39] SuperNet Training INFO: iter: 58080/144360  CE: 3.5187  
[05/06 13:43:39] SuperNet Training INFO: iter: 58200/144360  CE: 3.5638  
[05/06 13:44:40] SuperNet Training INFO: iter: 58320/144360  CE: 3.5961  
[05/06 13:45:41] SuperNet Training INFO: iter: 58440/144360  CE: 3.9249  
[05/06 13:46:43] SuperNet Training INFO: iter: 58560/144360  CE: 4.0587  
[05/06 13:47:44] SuperNet Training INFO: iter: 58680/144360  CE: 3.8760  
[05/06 13:48:45] SuperNet Training INFO: iter: 58800/144360  CE: 3.4054  
[05/06 13:49:47] SuperNet Training INFO: iter: 58920/144360  CE: 3.6583  
[05/06 13:49:59] SuperNet Training INFO: --> epoch:  49/120  avg CE: 3.6442  lr: 0.07704092068223518  
[05/06 13:51:25] SuperNet Training INFO: iter: 59040/144360  CE: 3.5696  
[05/06 13:52:24] SuperNet Training INFO: iter: 59160/144360  CE: 3.4922  
[05/06 13:53:24] SuperNet Training INFO: iter: 59280/144360  CE: 3.5910  
[05/06 13:54:25] SuperNet Training INFO: iter: 59400/144360  CE: 3.6071  
[05/06 13:55:25] SuperNet Training INFO: iter: 59520/144360  CE: 3.6870  
[05/06 13:56:26] SuperNet Training INFO: iter: 59640/144360  CE: 3.5094  
[05/06 13:57:27] SuperNet Training INFO: iter: 59760/144360  CE: 3.6842  
[05/06 13:58:28] SuperNet Training INFO: iter: 59880/144360  CE: 3.3271  
[05/06 13:59:29] SuperNet Training INFO: iter: 60000/144360  CE: 3.8865  
[05/06 14:00:30] SuperNet Training INFO: iter: 60120/144360  CE: 3.7054  
[05/06 14:00:45] SuperNet Training INFO: --> epoch:  50/120  avg CE: 3.6204  lr: 0.07552914270615126  
[05/06 14:02:07] SuperNet Training INFO: iter: 60240/144360  CE: 3.5063  
[05/06 14:03:07] SuperNet Training INFO: iter: 60360/144360  CE: 3.4514  
[05/06 14:04:09] SuperNet Training INFO: iter: 60480/144360  CE: 3.8299  
[05/06 14:05:11] SuperNet Training INFO: iter: 60600/144360  CE: 3.2842  
[05/06 14:06:12] SuperNet Training INFO: iter: 60720/144360  CE: 3.7928  
[05/06 14:07:14] SuperNet Training INFO: iter: 60840/144360  CE: 3.4299  
[05/06 14:08:15] SuperNet Training INFO: iter: 60960/144360  CE: 3.7060  
[05/06 14:09:18] SuperNet Training INFO: iter: 61080/144360  CE: 3.7796  
[05/06 14:10:19] SuperNet Training INFO: iter: 61200/144360  CE: 3.6907  
[05/06 14:11:19] SuperNet Training INFO: iter: 61320/144360  CE: 3.8697  
[05/06 14:11:35] SuperNet Training INFO: --> epoch:  51/120  avg CE: 3.6154  lr: 0.0740067218313545  
[05/06 14:12:55] SuperNet Training INFO: iter: 61440/144360  CE: 3.8157  
[05/06 14:13:55] SuperNet Training INFO: iter: 61560/144360  CE: 3.7292  
[05/06 14:14:57] SuperNet Training INFO: iter: 61680/144360  CE: 3.6330  
[05/06 14:15:59] SuperNet Training INFO: iter: 61800/144360  CE: 3.6991  
[05/06 14:16:58] SuperNet Training INFO: iter: 61920/144360  CE: 3.6668  
[05/06 14:18:00] SuperNet Training INFO: iter: 62040/144360  CE: 3.3981  
[05/06 14:19:00] SuperNet Training INFO: iter: 62160/144360  CE: 3.6112  
[05/06 14:20:01] SuperNet Training INFO: iter: 62280/144360  CE: 3.2874  
[05/06 14:21:02] SuperNet Training INFO: iter: 62400/144360  CE: 3.7886  
[05/06 14:22:02] SuperNet Training INFO: iter: 62520/144360  CE: 3.7723  
[05/06 14:22:19] SuperNet Training INFO: --> epoch:  52/120  avg CE: 3.5873  lr: 0.07247470144906537  
[05/06 14:23:36] SuperNet Training INFO: iter: 62640/144360  CE: 3.5787  
[05/06 14:24:36] SuperNet Training INFO: iter: 62760/144360  CE: 3.4862  
[05/06 14:25:39] SuperNet Training INFO: iter: 62880/144360  CE: 3.6847  
[05/06 14:26:39] SuperNet Training INFO: iter: 63000/144360  CE: 3.8422  
[05/06 14:27:40] SuperNet Training INFO: iter: 63120/144360  CE: 3.5462  
[05/06 14:28:40] SuperNet Training INFO: iter: 63240/144360  CE: 3.4115  
[05/06 14:29:39] SuperNet Training INFO: iter: 63360/144360  CE: 3.3597  
[05/06 14:30:41] SuperNet Training INFO: iter: 63480/144360  CE: 3.1940  
[05/06 14:31:41] SuperNet Training INFO: iter: 63600/144360  CE: 3.6358  
[05/06 14:32:41] SuperNet Training INFO: iter: 63720/144360  CE: 3.6251  
[05/06 14:33:00] SuperNet Training INFO: --> epoch:  53/120  avg CE: 3.5709  lr: 0.07093413152952865  
[05/06 14:34:17] SuperNet Training INFO: iter: 63840/144360  CE: 3.5858  
[05/06 14:35:19] SuperNet Training INFO: iter: 63960/144360  CE: 3.9364  
[05/06 14:36:19] SuperNet Training INFO: iter: 64080/144360  CE: 3.3373  
[05/06 14:37:21] SuperNet Training INFO: iter: 64200/144360  CE: 3.7534  
[05/06 14:38:21] SuperNet Training INFO: iter: 64320/144360  CE: 3.3518  
[05/06 14:39:22] SuperNet Training INFO: iter: 64440/144360  CE: 3.6964  
[05/06 14:40:21] SuperNet Training INFO: iter: 64560/144360  CE: 3.6920  
[05/06 14:41:19] SuperNet Training INFO: iter: 64680/144360  CE: 3.5813  
[05/06 14:42:19] SuperNet Training INFO: iter: 64800/144360  CE: 3.3166  
[05/06 14:43:18] SuperNet Training INFO: iter: 64920/144360  CE: 3.4661  
[05/06 14:43:38] SuperNet Training INFO: --> epoch:  54/120  avg CE: 3.5589  lr: 0.06938606790241343  
[05/06 14:44:54] SuperNet Training INFO: iter: 65040/144360  CE: 4.0262  
[05/06 14:45:54] SuperNet Training INFO: iter: 65160/144360  CE: 3.3848  
[05/06 14:46:54] SuperNet Training INFO: iter: 65280/144360  CE: 3.5873  
[05/06 14:47:54] SuperNet Training INFO: iter: 65400/144360  CE: 3.3944  
[05/06 14:48:54] SuperNet Training INFO: iter: 65520/144360  CE: 3.6475  
[05/06 14:49:54] SuperNet Training INFO: iter: 65640/144360  CE: 3.3776  
[05/06 14:50:55] SuperNet Training INFO: iter: 65760/144360  CE: 3.4404  
[05/06 14:51:54] SuperNet Training INFO: iter: 65880/144360  CE: 3.4686  
[05/06 14:52:54] SuperNet Training INFO: iter: 66000/144360  CE: 3.5319  
[05/06 14:53:54] SuperNet Training INFO: iter: 66120/144360  CE: 3.7003  
[05/06 14:54:15] SuperNet Training INFO: --> epoch:  55/120  avg CE: 3.5457  lr: 0.06783157153320266  
[05/06 14:55:30] SuperNet Training INFO: iter: 66240/144360  CE: 3.4653  
[05/06 14:56:31] SuperNet Training INFO: iter: 66360/144360  CE: 3.3157  
[05/06 14:57:31] SuperNet Training INFO: iter: 66480/144360  CE: 3.3793  
[05/06 14:58:32] SuperNet Training INFO: iter: 66600/144360  CE: 3.6564  
[05/06 14:59:34] SuperNet Training INFO: iter: 66720/144360  CE: 3.6252  
[05/06 15:00:35] SuperNet Training INFO: iter: 66840/144360  CE: 3.0389  
[05/06 15:01:35] SuperNet Training INFO: iter: 66960/144360  CE: 3.3829  
[05/06 15:02:35] SuperNet Training INFO: iter: 67080/144360  CE: 3.8770  
[05/06 15:03:36] SuperNet Training INFO: iter: 67200/144360  CE: 3.6827  
[05/06 15:04:37] SuperNet Training INFO: iter: 67320/144360  CE: 3.8186  
[05/06 15:05:00] SuperNet Training INFO: --> epoch:  56/120  avg CE: 3.5258  lr: 0.06627170779605904  
[05/06 15:06:12] SuperNet Training INFO: iter: 67440/144360  CE: 3.7192  
[05/06 15:07:13] SuperNet Training INFO: iter: 67560/144360  CE: 3.6482  
[05/06 15:08:14] SuperNet Training INFO: iter: 67680/144360  CE: 3.1402  
[05/06 15:09:15] SuperNet Training INFO: iter: 67800/144360  CE: 3.5876  
[05/06 15:10:16] SuperNet Training INFO: iter: 67920/144360  CE: 3.6072  
[05/06 15:11:16] SuperNet Training INFO: iter: 68040/144360  CE: 3.5474  
[05/06 15:12:17] SuperNet Training INFO: iter: 68160/144360  CE: 3.3789  
[05/06 15:13:17] SuperNet Training INFO: iter: 68280/144360  CE: 3.3909  
[05/06 15:14:18] SuperNet Training INFO: iter: 68400/144360  CE: 3.2199  
[05/06 15:15:19] SuperNet Training INFO: iter: 68520/144360  CE: 3.8339  
[05/06 15:15:44] SuperNet Training INFO: --> epoch:  57/120  avg CE: 3.5121  lr: 0.06470754574367053  
[05/06 15:16:55] SuperNet Training INFO: iter: 68640/144360  CE: 3.6260  
[05/06 15:17:56] SuperNet Training INFO: iter: 68760/144360  CE: 3.4946  
[05/06 15:18:58] SuperNet Training INFO: iter: 68880/144360  CE: 3.3874  
[05/06 15:19:59] SuperNet Training INFO: iter: 69000/144360  CE: 3.5690  
[05/06 15:20:59] SuperNet Training INFO: iter: 69120/144360  CE: 3.3117  
[05/06 15:22:01] SuperNet Training INFO: iter: 69240/144360  CE: 3.5671  
[05/06 15:23:02] SuperNet Training INFO: iter: 69360/144360  CE: 3.5883  
[05/06 15:24:04] SuperNet Training INFO: iter: 69480/144360  CE: 3.2534  
[05/06 15:25:06] SuperNet Training INFO: iter: 69600/144360  CE: 3.8091  
[05/06 15:26:07] SuperNet Training INFO: iter: 69720/144360  CE: 3.3157  
[05/06 15:26:33] SuperNet Training INFO: --> epoch:  58/120  avg CE: 3.4993  lr: 0.06314015737457618  
[05/06 15:27:43] SuperNet Training INFO: iter: 69840/144360  CE: 3.3778  
[05/06 15:28:45] SuperNet Training INFO: iter: 69960/144360  CE: 3.6199  
[05/06 15:29:46] SuperNet Training INFO: iter: 70080/144360  CE: 3.7238  
[05/06 15:30:48] SuperNet Training INFO: iter: 70200/144360  CE: 3.3576  
[05/06 15:31:49] SuperNet Training INFO: iter: 70320/144360  CE: 3.6633  
[05/06 15:32:49] SuperNet Training INFO: iter: 70440/144360  CE: 3.3493  
[05/06 15:33:50] SuperNet Training INFO: iter: 70560/144360  CE: 3.5160  
[05/06 15:34:51] SuperNet Training INFO: iter: 70680/144360  CE: 3.4457  
[05/06 15:35:50] SuperNet Training INFO: iter: 70800/144360  CE: 3.6538  
[05/06 15:36:50] SuperNet Training INFO: iter: 70920/144360  CE: 3.0609  
[05/06 15:37:17] SuperNet Training INFO: --> epoch:  59/120  avg CE: 3.4832  lr: 0.061570616898472125  
[05/06 15:38:25] SuperNet Training INFO: iter: 71040/144360  CE: 3.3734  
[05/06 15:39:25] SuperNet Training INFO: iter: 71160/144360  CE: 3.6670  
[05/06 15:40:27] SuperNet Training INFO: iter: 71280/144360  CE: 3.3476  
[05/06 15:41:28] SuperNet Training INFO: iter: 71400/144360  CE: 3.4051  
[05/06 15:42:30] SuperNet Training INFO: iter: 71520/144360  CE: 3.7777  
[05/06 15:43:32] SuperNet Training INFO: iter: 71640/144360  CE: 3.3542  
[05/06 15:44:33] SuperNet Training INFO: iter: 71760/144360  CE: 3.4286  
[05/06 15:45:34] SuperNet Training INFO: iter: 71880/144360  CE: 3.3776  
[05/06 15:46:34] SuperNet Training INFO: iter: 72000/144360  CE: 3.4156  
[05/06 15:47:33] SuperNet Training INFO: iter: 72120/144360  CE: 3.2583  
[05/06 15:48:03] SuperNet Training INFO: --> epoch:  60/120  avg CE: 3.4679  lr: 0.05999999999999976  
[05/06 15:49:09] SuperNet Training INFO: iter: 72240/144360  CE: 3.6381  
[05/06 15:50:11] SuperNet Training INFO: iter: 72360/144360  CE: 3.4400  
[05/06 15:51:12] SuperNet Training INFO: iter: 72480/144360  CE: 3.4622  
[05/06 15:52:13] SuperNet Training INFO: iter: 72600/144360  CE: 3.9331  
[05/06 15:53:15] SuperNet Training INFO: iter: 72720/144360  CE: 3.2723  
[05/06 15:54:15] SuperNet Training INFO: iter: 72840/144360  CE: 3.2045  
[05/06 15:55:16] SuperNet Training INFO: iter: 72960/144360  CE: 3.4165  
[05/06 15:56:17] SuperNet Training INFO: iter: 73080/144360  CE: 3.7098  
[05/06 15:57:17] SuperNet Training INFO: iter: 73200/144360  CE: 3.3550  
[05/06 15:58:17] SuperNet Training INFO: iter: 73320/144360  CE: 3.6106  
[05/06 15:58:48] SuperNet Training INFO: --> epoch:  61/120  avg CE: 3.4532  lr: 0.058429383101527455  
[05/06 15:59:53] SuperNet Training INFO: iter: 73440/144360  CE: 3.3203  
[05/06 16:00:55] SuperNet Training INFO: iter: 73560/144360  CE: 3.1132  
[05/06 16:01:56] SuperNet Training INFO: iter: 73680/144360  CE: 3.3961  
[05/06 16:02:56] SuperNet Training INFO: iter: 73800/144360  CE: 3.4299  
[05/06 16:03:57] SuperNet Training INFO: iter: 73920/144360  CE: 3.4719  
[05/06 16:04:58] SuperNet Training INFO: iter: 74040/144360  CE: 3.6690  
[05/06 16:06:00] SuperNet Training INFO: iter: 74160/144360  CE: 3.5312  
[05/06 16:07:01] SuperNet Training INFO: iter: 74280/144360  CE: 3.4648  
[05/06 16:08:03] SuperNet Training INFO: iter: 74400/144360  CE: 3.4815  
[05/06 16:09:04] SuperNet Training INFO: iter: 74520/144360  CE: 3.2752  
[05/06 16:09:36] SuperNet Training INFO: --> epoch:  62/120  avg CE: 3.4370  lr: 0.05685984262542327  
[05/06 16:10:39] SuperNet Training INFO: iter: 74640/144360  CE: 3.8922  
[05/06 16:11:40] SuperNet Training INFO: iter: 74760/144360  CE: 3.7475  
[05/06 16:12:41] SuperNet Training INFO: iter: 74880/144360  CE: 3.6174  
[05/06 16:13:42] SuperNet Training INFO: iter: 75000/144360  CE: 3.5734  
[05/06 16:14:45] SuperNet Training INFO: iter: 75120/144360  CE: 3.1785  
[05/06 16:15:46] SuperNet Training INFO: iter: 75240/144360  CE: 3.2594  
[05/06 16:16:48] SuperNet Training INFO: iter: 75360/144360  CE: 3.6102  
[05/06 16:17:49] SuperNet Training INFO: iter: 75480/144360  CE: 3.6265  
[05/06 16:18:50] SuperNet Training INFO: iter: 75600/144360  CE: 3.3879  
[05/06 16:19:51] SuperNet Training INFO: iter: 75720/144360  CE: 3.2842  
[05/06 16:20:25] SuperNet Training INFO: --> epoch:  63/120  avg CE: 3.4198  lr: 0.05529245425632924  
[05/06 16:21:27] SuperNet Training INFO: iter: 75840/144360  CE: 3.2900  
[05/06 16:22:27] SuperNet Training INFO: iter: 75960/144360  CE: 3.3474  
[05/06 16:23:26] SuperNet Training INFO: iter: 76080/144360  CE: 3.3424  
[05/06 16:24:26] SuperNet Training INFO: iter: 76200/144360  CE: 3.3182  
[05/06 16:25:26] SuperNet Training INFO: iter: 76320/144360  CE: 3.4382  
[05/06 16:26:26] SuperNet Training INFO: iter: 76440/144360  CE: 3.4527  
[05/06 16:27:26] SuperNet Training INFO: iter: 76560/144360  CE: 3.4666  
[05/06 16:28:27] SuperNet Training INFO: iter: 76680/144360  CE: 3.4034  
[05/06 16:29:28] SuperNet Training INFO: iter: 76800/144360  CE: 3.3458  
[05/06 16:30:28] SuperNet Training INFO: iter: 76920/144360  CE: 3.4054  
[05/06 16:31:03] SuperNet Training INFO: --> epoch:  64/120  avg CE: 3.4113  lr: 0.05372829220394078  
[05/06 16:32:04] SuperNet Training INFO: iter: 77040/144360  CE: 3.1937  
[05/06 16:33:05] SuperNet Training INFO: iter: 77160/144360  CE: 3.4219  
[05/06 16:34:06] SuperNet Training INFO: iter: 77280/144360  CE: 3.3858  
[05/06 16:35:06] SuperNet Training INFO: iter: 77400/144360  CE: 3.4318  
[05/06 16:36:07] SuperNet Training INFO: iter: 77520/144360  CE: 3.7136  
[05/06 16:37:08] SuperNet Training INFO: iter: 77640/144360  CE: 3.4809  
[05/06 16:38:09] SuperNet Training INFO: iter: 77760/144360  CE: 3.2813  
[05/06 16:39:09] SuperNet Training INFO: iter: 77880/144360  CE: 3.5462  
[05/06 16:40:10] SuperNet Training INFO: iter: 78000/144360  CE: 3.8172  
[05/06 16:41:11] SuperNet Training INFO: iter: 78120/144360  CE: 3.4838  
[05/06 16:41:49] SuperNet Training INFO: --> epoch:  65/120  avg CE: 3.3959  lr: 0.05216842846679694  
[05/06 16:42:47] SuperNet Training INFO: iter: 78240/144360  CE: 3.4460  
[05/06 16:43:47] SuperNet Training INFO: iter: 78360/144360  CE: 3.2879  
[05/06 16:44:48] SuperNet Training INFO: iter: 78480/144360  CE: 3.2866  
[05/06 16:45:50] SuperNet Training INFO: iter: 78600/144360  CE: 3.2495  
[05/06 16:46:51] SuperNet Training INFO: iter: 78720/144360  CE: 3.3685  
[05/06 16:47:52] SuperNet Training INFO: iter: 78840/144360  CE: 3.3761  
[05/06 16:48:54] SuperNet Training INFO: iter: 78960/144360  CE: 3.5806  
[05/06 16:49:55] SuperNet Training INFO: iter: 79080/144360  CE: 3.4424  
[05/06 16:50:55] SuperNet Training INFO: iter: 79200/144360  CE: 3.3930  
[05/06 16:51:55] SuperNet Training INFO: iter: 79320/144360  CE: 3.4225  
[05/06 16:52:33] SuperNet Training INFO: --> epoch:  66/120  avg CE: 3.3895  lr: 0.05061393209758616  
[05/06 16:53:30] SuperNet Training INFO: iter: 79440/144360  CE: 3.5351  
[05/06 16:54:31] SuperNet Training INFO: iter: 79560/144360  CE: 3.3896  
[05/06 16:55:31] SuperNet Training INFO: iter: 79680/144360  CE: 3.5407  
[05/06 16:56:31] SuperNet Training INFO: iter: 79800/144360  CE: 3.3211  
[05/06 16:57:31] SuperNet Training INFO: iter: 79920/144360  CE: 3.5259  
[05/06 16:58:31] SuperNet Training INFO: iter: 80040/144360  CE: 3.3802  
[05/06 16:59:30] SuperNet Training INFO: iter: 80160/144360  CE: 3.4090  
[05/06 17:00:30] SuperNet Training INFO: iter: 80280/144360  CE: 3.5696  
[05/06 17:01:31] SuperNet Training INFO: iter: 80400/144360  CE: 3.4478  
[05/06 17:02:30] SuperNet Training INFO: iter: 80520/144360  CE: 3.3760  
[05/06 17:03:09] SuperNet Training INFO: --> epoch:  67/120  avg CE: 3.3731  lr: 0.04906586847047105  
[05/06 17:04:04] SuperNet Training INFO: iter: 80640/144360  CE: 3.1318  
[05/06 17:05:06] SuperNet Training INFO: iter: 80760/144360  CE: 3.4745  
[05/06 17:06:07] SuperNet Training INFO: iter: 80880/144360  CE: 2.9612  
[05/06 17:07:07] SuperNet Training INFO: iter: 81000/144360  CE: 3.5415  
[05/06 17:08:08] SuperNet Training INFO: iter: 81120/144360  CE: 3.1768  
[05/06 17:09:07] SuperNet Training INFO: iter: 81240/144360  CE: 3.5832  
[05/06 17:10:06] SuperNet Training INFO: iter: 81360/144360  CE: 3.3467  
[05/06 17:11:06] SuperNet Training INFO: iter: 81480/144360  CE: 3.6667  
[05/06 17:12:06] SuperNet Training INFO: iter: 81600/144360  CE: 3.5431  
[05/06 17:13:05] SuperNet Training INFO: iter: 81720/144360  CE: 3.4481  
[05/06 17:13:45] SuperNet Training INFO: --> epoch:  68/120  avg CE: 3.3555  lr: 0.04752529855093431  
[05/06 17:14:39] SuperNet Training INFO: iter: 81840/144360  CE: 3.2384  
[05/06 17:15:40] SuperNet Training INFO: iter: 81960/144360  CE: 3.3628  
[05/06 17:16:41] SuperNet Training INFO: iter: 82080/144360  CE: 3.4296  
[05/06 17:17:42] SuperNet Training INFO: iter: 82200/144360  CE: 3.3450  
[05/06 17:18:42] SuperNet Training INFO: iter: 82320/144360  CE: 3.2769  
[05/06 17:19:43] SuperNet Training INFO: iter: 82440/144360  CE: 3.9555  
[05/06 17:20:43] SuperNet Training INFO: iter: 82560/144360  CE: 3.3108  
[05/06 17:21:43] SuperNet Training INFO: iter: 82680/144360  CE: 3.4545  
[05/06 17:22:43] SuperNet Training INFO: iter: 82800/144360  CE: 3.5180  
[05/06 17:23:41] SuperNet Training INFO: iter: 82920/144360  CE: 3.4875  
[05/06 17:24:24] SuperNet Training INFO: --> epoch:  69/120  avg CE: 3.3444  lr: 0.04599327816864548  
[05/06 17:25:16] SuperNet Training INFO: iter: 83040/144360  CE: 3.2501  
[05/06 17:26:16] SuperNet Training INFO: iter: 83160/144360  CE: 3.6683  
[05/06 17:27:15] SuperNet Training INFO: iter: 83280/144360  CE: 3.3507  
[05/06 17:28:16] SuperNet Training INFO: iter: 83400/144360  CE: 3.3559  
[05/06 17:29:15] SuperNet Training INFO: iter: 83520/144360  CE: 3.4605  
[05/06 17:30:15] SuperNet Training INFO: iter: 83640/144360  CE: 3.1486  
[05/06 17:31:18] SuperNet Training INFO: iter: 83760/144360  CE: 3.3031  
[05/06 17:32:19] SuperNet Training INFO: iter: 83880/144360  CE: 3.4406  
[05/06 17:33:20] SuperNet Training INFO: iter: 84000/144360  CE: 3.5207  
[05/06 17:34:21] SuperNet Training INFO: iter: 84120/144360  CE: 3.1875  
[05/06 17:35:05] SuperNet Training INFO: --> epoch:  70/120  avg CE: 3.3352  lr: 0.04447085729384866  
[05/06 17:35:55] SuperNet Training INFO: iter: 84240/144360  CE: 3.2420  
[05/06 17:36:56] SuperNet Training INFO: iter: 84360/144360  CE: 2.9571  
[05/06 17:37:57] SuperNet Training INFO: iter: 84480/144360  CE: 3.4481  
[05/06 17:38:59] SuperNet Training INFO: iter: 84600/144360  CE: 3.5257  
[05/06 17:40:00] SuperNet Training INFO: iter: 84720/144360  CE: 3.0241  
[05/06 17:41:00] SuperNet Training INFO: iter: 84840/144360  CE: 3.3113  
[05/06 17:42:00] SuperNet Training INFO: iter: 84960/144360  CE: 3.6198  
[05/06 17:43:01] SuperNet Training INFO: iter: 85080/144360  CE: 3.3660  
[05/06 17:44:01] SuperNet Training INFO: iter: 85200/144360  CE: 3.3095  
[05/06 17:45:02] SuperNet Training INFO: iter: 85320/144360  CE: 3.1861  
[05/06 17:45:49] SuperNet Training INFO: --> epoch:  71/120  avg CE: 3.3227  lr: 0.04295907931776456  
[05/06 17:46:38] SuperNet Training INFO: iter: 85440/144360  CE: 3.1740  
[05/06 17:47:38] SuperNet Training INFO: iter: 85560/144360  CE: 3.5100  
[05/06 17:48:38] SuperNet Training INFO: iter: 85680/144360  CE: 3.2824  
[05/06 17:49:39] SuperNet Training INFO: iter: 85800/144360  CE: 3.0603  
[05/06 17:50:39] SuperNet Training INFO: iter: 85920/144360  CE: 3.2012  
[05/06 17:51:39] SuperNet Training INFO: iter: 86040/144360  CE: 3.2870  
[05/06 17:52:41] SuperNet Training INFO: iter: 86160/144360  CE: 3.5299  
[05/06 17:53:41] SuperNet Training INFO: iter: 86280/144360  CE: 3.4364  
[05/06 17:54:41] SuperNet Training INFO: iter: 86400/144360  CE: 3.3978  
[05/06 17:55:42] SuperNet Training INFO: iter: 86520/144360  CE: 3.4060  
[05/06 17:56:30] SuperNet Training INFO: --> epoch:  72/120  avg CE: 3.3168  lr: 0.04145898033750296  
[05/06 17:57:18] SuperNet Training INFO: iter: 86640/144360  CE: 3.1121  
[05/06 17:58:19] SuperNet Training INFO: iter: 86760/144360  CE: 3.2283  
[05/06 17:59:21] SuperNet Training INFO: iter: 86880/144360  CE: 3.1340  
[05/06 18:00:20] SuperNet Training INFO: iter: 87000/144360  CE: 3.1958  
[05/06 18:01:20] SuperNet Training INFO: iter: 87120/144360  CE: 2.8755  
[05/06 18:02:21] SuperNet Training INFO: iter: 87240/144360  CE: 3.1862  
[05/06 18:03:21] SuperNet Training INFO: iter: 87360/144360  CE: 3.2582  
[05/06 18:04:21] SuperNet Training INFO: iter: 87480/144360  CE: 3.2617  
[05/06 18:05:22] SuperNet Training INFO: iter: 87600/144360  CE: 3.2053  
[05/06 18:06:23] SuperNet Training INFO: iter: 87720/144360  CE: 3.2096  
[05/06 18:07:13] SuperNet Training INFO: --> epoch:  73/120  avg CE: 3.2956  lr: 0.03997158844597365  
[05/06 18:08:00] SuperNet Training INFO: iter: 87840/144360  CE: 3.6169  
[05/06 18:09:01] SuperNet Training INFO: iter: 87960/144360  CE: 3.2856  
[05/06 18:10:02] SuperNet Training INFO: iter: 88080/144360  CE: 3.1583  
[05/06 18:11:03] SuperNet Training INFO: iter: 88200/144360  CE: 3.2554  
[05/06 18:12:04] SuperNet Training INFO: iter: 88320/144360  CE: 3.4429  
[05/06 18:13:03] SuperNet Training INFO: iter: 88440/144360  CE: 3.1481  
[05/06 18:14:04] SuperNet Training INFO: iter: 88560/144360  CE: 3.1242  
[05/06 18:15:03] SuperNet Training INFO: iter: 88680/144360  CE: 2.8445  
[05/06 18:16:04] SuperNet Training INFO: iter: 88800/144360  CE: 3.1588  
[05/06 18:17:04] SuperNet Training INFO: iter: 88920/144360  CE: 3.5618  
[05/06 18:17:54] SuperNet Training INFO: --> epoch:  74/120  avg CE: 3.2811  lr: 0.03849792302728192  
[05/06 18:18:38] SuperNet Training INFO: iter: 89040/144360  CE: 3.2222  
[05/06 18:19:38] SuperNet Training INFO: iter: 89160/144360  CE: 3.2007  
[05/06 18:20:38] SuperNet Training INFO: iter: 89280/144360  CE: 3.0573  
[05/06 18:21:39] SuperNet Training INFO: iter: 89400/144360  CE: 3.1857  
[05/06 18:22:40] SuperNet Training INFO: iter: 89520/144360  CE: 3.3167  
[05/06 18:23:41] SuperNet Training INFO: iter: 89640/144360  CE: 3.2007  
[05/06 18:24:42] SuperNet Training INFO: iter: 89760/144360  CE: 3.4115  
[05/06 18:25:42] SuperNet Training INFO: iter: 89880/144360  CE: 3.2333  
[05/06 18:26:43] SuperNet Training INFO: iter: 90000/144360  CE: 3.3721  
[05/06 18:27:44] SuperNet Training INFO: iter: 90120/144360  CE: 3.0928  
[05/06 18:28:36] SuperNet Training INFO: --> epoch:  75/120  avg CE: 3.2765  lr: 0.03703899405809455  
[05/06 18:29:19] SuperNet Training INFO: iter: 90240/144360  CE: 3.2446  
[05/06 18:30:20] SuperNet Training INFO: iter: 90360/144360  CE: 3.5563  
[05/06 18:31:21] SuperNet Training INFO: iter: 90480/144360  CE: 3.9604  
[05/06 18:32:21] SuperNet Training INFO: iter: 90600/144360  CE: 3.0144  
[05/06 18:33:23] SuperNet Training INFO: iter: 90720/144360  CE: 2.9953  
[05/06 18:34:23] SuperNet Training INFO: iter: 90840/144360  CE: 3.1698  
[05/06 18:35:24] SuperNet Training INFO: iter: 90960/144360  CE: 3.1031  
[05/06 18:36:23] SuperNet Training INFO: iter: 91080/144360  CE: 3.6769  
[05/06 18:37:24] SuperNet Training INFO: iter: 91200/144360  CE: 3.3441  
[05/06 18:38:25] SuperNet Training INFO: iter: 91320/144360  CE: 3.0634  
[05/06 18:39:19] SuperNet Training INFO: --> epoch:  76/120  avg CE: 3.2697  lr: 0.035595801415451815  
[05/06 18:40:00] SuperNet Training INFO: iter: 91440/144360  CE: 3.4265  
[05/06 18:41:01] SuperNet Training INFO: iter: 91560/144360  CE: 3.6260  
[05/06 18:42:00] SuperNet Training INFO: iter: 91680/144360  CE: 3.3318  
[05/06 18:43:01] SuperNet Training INFO: iter: 91800/144360  CE: 3.2486  
[05/06 18:44:00] SuperNet Training INFO: iter: 91920/144360  CE: 3.2142  
[05/06 18:45:01] SuperNet Training INFO: iter: 92040/144360  CE: 3.5775  
[05/06 18:46:02] SuperNet Training INFO: iter: 92160/144360  CE: 3.2731  
[05/06 18:47:03] SuperNet Training INFO: iter: 92280/144360  CE: 3.0951  
[05/06 18:48:05] SuperNet Training INFO: iter: 92400/144360  CE: 3.2713  
[05/06 18:49:04] SuperNet Training INFO: iter: 92520/144360  CE: 3.2212  
[05/06 18:49:59] SuperNet Training INFO: --> epoch:  77/120  avg CE: 3.2590  lr: 0.03416933419150217  
[05/06 18:50:39] SuperNet Training INFO: iter: 92640/144360  CE: 3.1671  
[05/06 18:51:40] SuperNet Training INFO: iter: 92760/144360  CE: 3.3177  
[05/06 18:52:39] SuperNet Training INFO: iter: 92880/144360  CE: 3.6021  
[05/06 18:53:40] SuperNet Training INFO: iter: 93000/144360  CE: 3.3659  
[05/06 18:54:41] SuperNet Training INFO: iter: 93120/144360  CE: 3.8319  
[05/06 18:55:41] SuperNet Training INFO: iter: 93240/144360  CE: 3.0493  
[05/06 18:56:42] SuperNet Training INFO: iter: 93360/144360  CE: 3.2876  
[05/06 18:57:44] SuperNet Training INFO: iter: 93480/144360  CE: 3.2903  
[05/06 18:58:46] SuperNet Training INFO: iter: 93600/144360  CE: 3.3805  
[05/06 18:59:48] SuperNet Training INFO: iter: 93720/144360  CE: 3.4367  
[05/06 19:00:45] SuperNet Training INFO: --> epoch:  78/120  avg CE: 3.2586  lr: 0.03276057001562702  
[05/06 19:01:23] SuperNet Training INFO: iter: 93840/144360  CE: 3.4453  
[05/06 19:02:25] SuperNet Training INFO: iter: 93960/144360  CE: 3.0805  
[05/06 19:03:26] SuperNet Training INFO: iter: 94080/144360  CE: 3.2288  
[05/06 19:04:27] SuperNet Training INFO: iter: 94200/144360  CE: 3.0643  
[05/06 19:05:28] SuperNet Training INFO: iter: 94320/144360  CE: 3.1865  
[05/06 19:06:30] SuperNet Training INFO: iter: 94440/144360  CE: 3.0785  
[05/06 19:07:32] SuperNet Training INFO: iter: 94560/144360  CE: 3.1816  
[05/06 19:08:32] SuperNet Training INFO: iter: 94680/144360  CE: 3.4606  
[05/06 19:09:34] SuperNet Training INFO: iter: 94800/144360  CE: 3.3807  
[05/06 19:10:36] SuperNet Training INFO: iter: 94920/144360  CE: 3.3823  
[05/06 19:11:34] SuperNet Training INFO: --> epoch:  79/120  avg CE: 3.2291  lr: 0.031370474384423336  
[05/06 19:12:11] SuperNet Training INFO: iter: 95040/144360  CE: 3.5851  
[05/06 19:13:11] SuperNet Training INFO: iter: 95160/144360  CE: 3.3060  
[05/06 19:14:10] SuperNet Training INFO: iter: 95280/144360  CE: 3.0648  
[05/06 19:15:13] SuperNet Training INFO: iter: 95400/144360  CE: 3.4414  
[05/06 19:16:13] SuperNet Training INFO: iter: 95520/144360  CE: 2.8482  
[05/06 19:17:15] SuperNet Training INFO: iter: 95640/144360  CE: 3.3427  
[05/06 19:18:17] SuperNet Training INFO: iter: 95760/144360  CE: 3.2492  
[05/06 19:19:18] SuperNet Training INFO: iter: 95880/144360  CE: 3.3325  
[05/06 19:20:18] SuperNet Training INFO: iter: 96000/144360  CE: 3.4884  
[05/06 19:21:19] SuperNet Training INFO: iter: 96120/144360  CE: 3.1641  
[05/06 19:22:19] SuperNet Training INFO: iter: 96240/144360  CE: 3.0118  
[05/06 19:22:19] SuperNet Training INFO: --> epoch:  80/120  avg CE: 3.2164  lr: 0.02999999999999983  
[05/06 19:23:55] SuperNet Training INFO: iter: 96360/144360  CE: 3.0934  
[05/06 19:24:56] SuperNet Training INFO: iter: 96480/144360  CE: 3.0907  
[05/06 19:25:57] SuperNet Training INFO: iter: 96600/144360  CE: 3.0693  
[05/06 19:26:57] SuperNet Training INFO: iter: 96720/144360  CE: 3.5018  
[05/06 19:27:57] SuperNet Training INFO: iter: 96840/144360  CE: 3.3484  
[05/06 19:28:58] SuperNet Training INFO: iter: 96960/144360  CE: 3.3897  
[05/06 19:30:00] SuperNet Training INFO: iter: 97080/144360  CE: 3.1429  
[05/06 19:31:01] SuperNet Training INFO: iter: 97200/144360  CE: 3.3527  
[05/06 19:32:02] SuperNet Training INFO: iter: 97320/144360  CE: 3.5461  
[05/06 19:33:01] SuperNet Training INFO: iter: 97440/144360  CE: 3.0524  
[05/06 19:33:02] SuperNet Training INFO: --> epoch:  81/120  avg CE: 3.2016  lr: 0.028650086117042926  
[05/06 19:34:38] SuperNet Training INFO: iter: 97560/144360  CE: 3.5135  
[05/06 19:35:39] SuperNet Training INFO: iter: 97680/144360  CE: 3.1868  
[05/06 19:36:38] SuperNet Training INFO: iter: 97800/144360  CE: 3.1876  
[05/06 19:37:39] SuperNet Training INFO: iter: 97920/144360  CE: 3.2317  
[05/06 19:38:39] SuperNet Training INFO: iter: 98040/144360  CE: 2.9786  
[05/06 19:39:41] SuperNet Training INFO: iter: 98160/144360  CE: 3.3977  
[05/06 19:40:42] SuperNet Training INFO: iter: 98280/144360  CE: 3.3688  
[05/06 19:41:42] SuperNet Training INFO: iter: 98400/144360  CE: 3.0894  
[05/06 19:42:44] SuperNet Training INFO: iter: 98520/144360  CE: 3.3897  
[05/06 19:43:44] SuperNet Training INFO: iter: 98640/144360  CE: 3.4196  
[05/06 19:43:46] SuperNet Training INFO: --> epoch:  82/120  avg CE: 3.1943  lr: 0.027321657899098132  
[05/06 19:45:18] SuperNet Training INFO: iter: 98760/144360  CE: 3.0193  
[05/06 19:46:20] SuperNet Training INFO: iter: 98880/144360  CE: 3.0801  
[05/06 19:47:21] SuperNet Training INFO: iter: 99000/144360  CE: 3.2065  
[05/06 19:48:22] SuperNet Training INFO: iter: 99120/144360  CE: 3.3636  
[05/06 19:49:22] SuperNet Training INFO: iter: 99240/144360  CE: 3.2921  
[05/06 19:50:23] SuperNet Training INFO: iter: 99360/144360  CE: 3.6056  
[05/06 19:51:23] SuperNet Training INFO: iter: 99480/144360  CE: 3.4815  
[05/06 19:52:25] SuperNet Training INFO: iter: 99600/144360  CE: 3.3584  
[05/06 19:53:26] SuperNet Training INFO: iter: 99720/144360  CE: 3.1541  
[05/06 19:54:26] SuperNet Training INFO: iter: 99840/144360  CE: 3.4628  
[05/06 19:54:30] SuperNet Training INFO: --> epoch:  83/120  avg CE: 3.1862  lr: 0.0260156257845098  
[05/06 19:56:01] SuperNet Training INFO: iter: 99960/144360  CE: 3.1922  
[05/06 19:57:02] SuperNet Training INFO: iter: 100080/144360  CE: 3.2151  
[05/06 19:58:02] SuperNet Training INFO: iter: 100200/144360  CE: 3.5198  
[05/06 19:59:02] SuperNet Training INFO: iter: 100320/144360  CE: 3.3733  
[05/06 20:00:02] SuperNet Training INFO: iter: 100440/144360  CE: 3.2039  
[05/06 20:01:02] SuperNet Training INFO: iter: 100560/144360  CE: 3.1512  
[05/06 20:02:03] SuperNet Training INFO: iter: 100680/144360  CE: 2.9659  
[05/06 20:03:02] SuperNet Training INFO: iter: 100800/144360  CE: 3.1970  
[05/06 20:04:02] SuperNet Training INFO: iter: 100920/144360  CE: 3.1282  
[05/06 20:05:02] SuperNet Training INFO: iter: 101040/144360  CE: 3.5491  
[05/06 20:05:06] SuperNet Training INFO: --> epoch:  84/120  avg CE: 3.1705  lr: 0.02473288486245143  
[05/06 20:06:39] SuperNet Training INFO: iter: 101160/144360  CE: 3.0970  
[05/06 20:07:40] SuperNet Training INFO: iter: 101280/144360  CE: 3.1035  
[05/06 20:08:42] SuperNet Training INFO: iter: 101400/144360  CE: 3.0809  
[05/06 20:09:43] SuperNet Training INFO: iter: 101520/144360  CE: 3.1715  
[05/06 20:10:45] SuperNet Training INFO: iter: 101640/144360  CE: 3.1289  
[05/06 20:11:46] SuperNet Training INFO: iter: 101760/144360  CE: 3.2032  
[05/06 20:12:48] SuperNet Training INFO: iter: 101880/144360  CE: 3.2210  
[05/06 20:13:49] SuperNet Training INFO: iter: 102000/144360  CE: 3.1624  
[05/06 20:14:50] SuperNet Training INFO: iter: 102120/144360  CE: 3.0873  
[05/06 20:15:52] SuperNet Training INFO: iter: 102240/144360  CE: 3.0008  
[05/06 20:15:58] SuperNet Training INFO: --> epoch:  85/120  avg CE: 3.1609  lr: 0.023474314259476523  
[05/06 20:17:27] SuperNet Training INFO: iter: 102360/144360  CE: 3.2109  
[05/06 20:18:28] SuperNet Training INFO: iter: 102480/144360  CE: 3.2056  
[05/06 20:19:30] SuperNet Training INFO: iter: 102600/144360  CE: 3.3047  
[05/06 20:20:30] SuperNet Training INFO: iter: 102720/144360  CE: 3.0467  
[05/06 20:21:30] SuperNet Training INFO: iter: 102840/144360  CE: 3.2136  
[05/06 20:22:31] SuperNet Training INFO: iter: 102960/144360  CE: 3.4683  
[05/06 20:23:30] SuperNet Training INFO: iter: 103080/144360  CE: 3.2729  
[05/06 20:24:30] SuperNet Training INFO: iter: 103200/144360  CE: 3.0752  
[05/06 20:25:29] SuperNet Training INFO: iter: 103320/144360  CE: 3.3157  
[05/06 20:26:28] SuperNet Training INFO: iter: 103440/144360  CE: 3.1867  
[05/06 20:26:36] SuperNet Training INFO: --> epoch:  86/120  avg CE: 3.1593  lr: 0.02224077653700959  
[05/06 20:28:02] SuperNet Training INFO: iter: 103560/144360  CE: 3.2043  
[05/06 20:29:02] SuperNet Training INFO: iter: 103680/144360  CE: 2.9106  
[05/06 20:30:03] SuperNet Training INFO: iter: 103800/144360  CE: 2.9716  
[05/06 20:31:03] SuperNet Training INFO: iter: 103920/144360  CE: 3.0693  
[05/06 20:32:04] SuperNet Training INFO: iter: 104040/144360  CE: 3.3312  
[05/06 20:33:04] SuperNet Training INFO: iter: 104160/144360  CE: 3.3330  
[05/06 20:34:06] SuperNet Training INFO: iter: 104280/144360  CE: 3.2188  
[05/06 20:35:05] SuperNet Training INFO: iter: 104400/144360  CE: 3.1751  
[05/06 20:36:05] SuperNet Training INFO: iter: 104520/144360  CE: 3.3489  
[05/06 20:37:06] SuperNet Training INFO: iter: 104640/144360  CE: 3.1546  
[05/06 20:37:16] SuperNet Training INFO: --> epoch:  87/120  avg CE: 3.1434  lr: 0.02103311710018879  
[05/06 20:38:41] SuperNet Training INFO: iter: 104760/144360  CE: 3.2888  
[05/06 20:39:41] SuperNet Training INFO: iter: 104880/144360  CE: 3.0492  
[05/06 20:40:41] SuperNet Training INFO: iter: 105000/144360  CE: 3.3126  
[05/06 20:41:41] SuperNet Training INFO: iter: 105120/144360  CE: 2.9498  
[05/06 20:42:40] SuperNet Training INFO: iter: 105240/144360  CE: 3.4337  
[05/06 20:43:41] SuperNet Training INFO: iter: 105360/144360  CE: 3.1938  
[05/06 20:44:42] SuperNet Training INFO: iter: 105480/144360  CE: 3.4742  
[05/06 20:45:43] SuperNet Training INFO: iter: 105600/144360  CE: 3.4855  
[05/06 20:46:46] SuperNet Training INFO: iter: 105720/144360  CE: 3.1766  
[05/06 20:47:46] SuperNet Training INFO: iter: 105840/144360  CE: 3.3686  
[05/06 20:47:58] SuperNet Training INFO: --> epoch:  88/120  avg CE: 3.1383  lr: 0.019852163618468272  
[05/06 20:49:23] SuperNet Training INFO: iter: 105960/144360  CE: 3.4019  
[05/06 20:50:24] SuperNet Training INFO: iter: 106080/144360  CE: 2.8342  
[05/06 20:51:26] SuperNet Training INFO: iter: 106200/144360  CE: 2.6951  
[05/06 20:52:28] SuperNet Training INFO: iter: 106320/144360  CE: 3.1957  
[05/06 20:53:29] SuperNet Training INFO: iter: 106440/144360  CE: 3.0963  
[05/06 20:54:30] SuperNet Training INFO: iter: 106560/144360  CE: 3.2552  
[05/06 20:55:31] SuperNet Training INFO: iter: 106680/144360  CE: 2.9657  
[05/06 20:56:33] SuperNet Training INFO: iter: 106800/144360  CE: 3.8922  
[05/06 20:57:34] SuperNet Training INFO: iter: 106920/144360  CE: 3.0230  
[05/06 20:58:34] SuperNet Training INFO: iter: 107040/144360  CE: 3.0907  
[05/06 20:58:47] SuperNet Training INFO: --> epoch:  89/120  avg CE: 3.1194  lr: 0.018698725458374543  
[05/06 21:00:10] SuperNet Training INFO: iter: 107160/144360  CE: 3.5921  
[05/06 21:01:11] SuperNet Training INFO: iter: 107280/144360  CE: 3.3646  
[05/06 21:02:12] SuperNet Training INFO: iter: 107400/144360  CE: 3.1380  
[05/06 21:03:13] SuperNet Training INFO: iter: 107520/144360  CE: 2.9093  
[05/06 21:04:14] SuperNet Training INFO: iter: 107640/144360  CE: 3.3969  
[05/06 21:05:15] SuperNet Training INFO: iter: 107760/144360  CE: 2.8671  
[05/06 21:06:16] SuperNet Training INFO: iter: 107880/144360  CE: 3.0398  
[05/06 21:07:16] SuperNet Training INFO: iter: 108000/144360  CE: 2.8797  
[05/06 21:08:16] SuperNet Training INFO: iter: 108120/144360  CE: 3.0599  
[05/06 21:09:15] SuperNet Training INFO: iter: 108240/144360  CE: 3.1901  
[05/06 21:09:30] SuperNet Training INFO: --> epoch:  90/120  avg CE: 3.1149  lr: 0.01757359312880703  
[05/06 21:10:49] SuperNet Training INFO: iter: 108360/144360  CE: 3.0289  
[05/06 21:11:48] SuperNet Training INFO: iter: 108480/144360  CE: 3.2059  
[05/06 21:12:48] SuperNet Training INFO: iter: 108600/144360  CE: 2.9707  
[05/06 21:13:47] SuperNet Training INFO: iter: 108720/144360  CE: 3.1955  
[05/06 21:14:46] SuperNet Training INFO: iter: 108840/144360  CE: 3.0393  
[05/06 21:15:45] SuperNet Training INFO: iter: 108960/144360  CE: 2.8856  
[05/06 21:16:44] SuperNet Training INFO: iter: 109080/144360  CE: 3.1437  
[05/06 21:17:43] SuperNet Training INFO: iter: 109200/144360  CE: 2.8856  
[05/06 21:18:43] SuperNet Training INFO: iter: 109320/144360  CE: 3.3713  
[05/06 21:19:41] SuperNet Training INFO: iter: 109440/144360  CE: 3.1440  
[05/06 21:19:56] SuperNet Training INFO: --> epoch:  91/120  avg CE: 3.0990  lr: 0.016477537739262627  
[05/06 21:21:15] SuperNet Training INFO: iter: 109560/144360  CE: 2.9460  
[05/06 21:22:16] SuperNet Training INFO: iter: 109680/144360  CE: 3.0713  
[05/06 21:23:16] SuperNet Training INFO: iter: 109800/144360  CE: 3.0619  
[05/06 21:24:17] SuperNet Training INFO: iter: 109920/144360  CE: 2.9633  
[05/06 21:25:17] SuperNet Training INFO: iter: 110040/144360  CE: 3.0516  
[05/06 21:26:18] SuperNet Training INFO: iter: 110160/144360  CE: 2.9103  
[05/06 21:27:18] SuperNet Training INFO: iter: 110280/144360  CE: 3.0497  
[05/06 21:28:19] SuperNet Training INFO: iter: 110400/144360  CE: 3.1701  
[05/06 21:29:20] SuperNet Training INFO: iter: 110520/144360  CE: 3.4042  
[05/06 21:30:20] SuperNet Training INFO: iter: 110640/144360  CE: 3.1599  
[05/06 21:30:38] SuperNet Training INFO: --> epoch:  92/120  avg CE: 3.1031  lr: 0.015411310471356233  
[05/06 21:31:57] SuperNet Training INFO: iter: 110760/144360  CE: 3.3520  
[05/06 21:32:58] SuperNet Training INFO: iter: 110880/144360  CE: 3.1095  
[05/06 21:33:59] SuperNet Training INFO: iter: 111000/144360  CE: 2.8479  
[05/06 21:35:01] SuperNet Training INFO: iter: 111120/144360  CE: 3.5538  
[05/06 21:36:03] SuperNet Training INFO: iter: 111240/144360  CE: 3.2692  
[05/06 21:37:03] SuperNet Training INFO: iter: 111360/144360  CE: 2.5418  
[05/06 21:38:04] SuperNet Training INFO: iter: 111480/144360  CE: 2.9803  
[05/06 21:39:04] SuperNet Training INFO: iter: 111600/144360  CE: 3.0888  
[05/06 21:40:04] SuperNet Training INFO: iter: 111720/144360  CE: 3.0702  
[05/06 21:41:04] SuperNet Training INFO: iter: 111840/144360  CE: 2.9431  
[05/06 21:41:22] SuperNet Training INFO: --> epoch:  93/120  avg CE: 3.0827  lr: 0.014375642063998082  
[05/06 21:42:39] SuperNet Training INFO: iter: 111960/144360  CE: 3.1758  
[05/06 21:43:39] SuperNet Training INFO: iter: 112080/144360  CE: 3.2091  
[05/06 21:44:39] SuperNet Training INFO: iter: 112200/144360  CE: 3.0365  
[05/06 21:45:40] SuperNet Training INFO: iter: 112320/144360  CE: 3.3136  
[05/06 21:46:41] SuperNet Training INFO: iter: 112440/144360  CE: 3.2093  
[05/06 21:47:41] SuperNet Training INFO: iter: 112560/144360  CE: 3.2951  
[05/06 21:48:40] SuperNet Training INFO: iter: 112680/144360  CE: 3.2334  
[05/06 21:49:40] SuperNet Training INFO: iter: 112800/144360  CE: 2.8695  
[05/06 21:50:41] SuperNet Training INFO: iter: 112920/144360  CE: 3.0158  
[05/06 21:51:41] SuperNet Training INFO: iter: 113040/144360  CE: 3.0322  
[05/06 21:52:02] SuperNet Training INFO: --> epoch:  94/120  avg CE: 3.0780  lr: 0.01337124231258167  
[05/06 21:53:16] SuperNet Training INFO: iter: 113160/144360  CE: 3.1556  
[05/06 21:54:16] SuperNet Training INFO: iter: 113280/144360  CE: 3.0080  
[05/06 21:55:17] SuperNet Training INFO: iter: 113400/144360  CE: 3.0500  
[05/06 21:56:16] SuperNet Training INFO: iter: 113520/144360  CE: 3.3972  
[05/06 21:57:16] SuperNet Training INFO: iter: 113640/144360  CE: 3.1695  
[05/06 21:58:18] SuperNet Training INFO: iter: 113760/144360  CE: 2.9630  
[05/06 21:59:19] SuperNet Training INFO: iter: 113880/144360  CE: 2.9975  
[05/06 22:00:19] SuperNet Training INFO: iter: 114000/144360  CE: 2.9292  
[05/06 22:01:21] SuperNet Training INFO: iter: 114120/144360  CE: 3.0658  
[05/06 22:02:22] SuperNet Training INFO: iter: 114240/144360  CE: 3.2741  
[05/06 22:02:43] SuperNet Training INFO: --> epoch:  95/120  avg CE: 3.0616  lr: 0.012398799582525794  
[05/06 22:03:58] SuperNet Training INFO: iter: 114360/144360  CE: 2.9362  
[05/06 22:04:57] SuperNet Training INFO: iter: 114480/144360  CE: 3.0076  
[05/06 22:05:57] SuperNet Training INFO: iter: 114600/144360  CE: 3.1008  
[05/06 22:06:58] SuperNet Training INFO: iter: 114720/144360  CE: 3.1480  
[05/06 22:07:58] SuperNet Training INFO: iter: 114840/144360  CE: 3.1022  
[05/06 22:08:58] SuperNet Training INFO: iter: 114960/144360  CE: 3.2304  
[05/06 22:10:00] SuperNet Training INFO: iter: 115080/144360  CE: 2.8352  
[05/06 22:11:01] SuperNet Training INFO: iter: 115200/144360  CE: 3.0848  
[05/06 22:12:02] SuperNet Training INFO: iter: 115320/144360  CE: 3.1347  
[05/06 22:13:03] SuperNet Training INFO: iter: 115440/144360  CE: 2.9010  
[05/06 22:13:26] SuperNet Training INFO: --> epoch:  96/120  avg CE: 3.0617  lr: 0.01145898033750309  
[05/06 22:14:38] SuperNet Training INFO: iter: 115560/144360  CE: 3.1740  
[05/06 22:15:39] SuperNet Training INFO: iter: 115680/144360  CE: 3.0852  
[05/06 22:16:39] SuperNet Training INFO: iter: 115800/144360  CE: 3.0206  
[05/06 22:17:39] SuperNet Training INFO: iter: 115920/144360  CE: 3.0040  
[05/06 22:18:39] SuperNet Training INFO: iter: 116040/144360  CE: 3.2460  
[05/06 22:19:40] SuperNet Training INFO: iter: 116160/144360  CE: 2.6767  
[05/06 22:20:41] SuperNet Training INFO: iter: 116280/144360  CE: 3.0029  
[05/06 22:21:40] SuperNet Training INFO: iter: 116400/144360  CE: 3.0835  
[05/06 22:22:39] SuperNet Training INFO: iter: 116520/144360  CE: 3.2040  
[05/06 22:23:41] SuperNet Training INFO: iter: 116640/144360  CE: 3.0523  
[05/06 22:24:04] SuperNet Training INFO: --> epoch:  97/120  avg CE: 3.0397  lr: 0.01055242868267905  
[05/06 22:25:15] SuperNet Training INFO: iter: 116760/144360  CE: 3.5366  
[05/06 22:26:15] SuperNet Training INFO: iter: 116880/144360  CE: 2.8416  
[05/06 22:27:15] SuperNet Training INFO: iter: 117000/144360  CE: 2.9316  
[05/06 22:28:16] SuperNet Training INFO: iter: 117120/144360  CE: 3.1607  
[05/06 22:29:16] SuperNet Training INFO: iter: 117240/144360  CE: 3.1266  
[05/06 22:30:17] SuperNet Training INFO: iter: 117360/144360  CE: 3.2659  
[05/06 22:31:15] SuperNet Training INFO: iter: 117480/144360  CE: 2.9728  
[05/06 22:32:14] SuperNet Training INFO: iter: 117600/144360  CE: 3.0728  
[05/06 22:33:13] SuperNet Training INFO: iter: 117720/144360  CE: 3.2288  
[05/06 22:34:12] SuperNet Training INFO: iter: 117840/144360  CE: 3.1902  
[05/06 22:34:38] SuperNet Training INFO: --> epoch:  98/120  avg CE: 3.0330  lr: 0.009679765923274538  
[05/06 22:35:48] SuperNet Training INFO: iter: 117960/144360  CE: 3.0898  
[05/06 22:36:49] SuperNet Training INFO: iter: 118080/144360  CE: 3.1568  
[05/06 22:37:50] SuperNet Training INFO: iter: 118200/144360  CE: 3.1720  
[05/06 22:38:50] SuperNet Training INFO: iter: 118320/144360  CE: 3.2070  
[05/06 22:39:50] SuperNet Training INFO: iter: 118440/144360  CE: 3.3676  
[05/06 22:40:50] SuperNet Training INFO: iter: 118560/144360  CE: 2.6657  
[05/06 22:41:52] SuperNet Training INFO: iter: 118680/144360  CE: 2.9353  
[05/06 22:42:51] SuperNet Training INFO: iter: 118800/144360  CE: 2.9662  
[05/06 22:43:50] SuperNet Training INFO: iter: 118920/144360  CE: 3.0452  
[05/06 22:44:49] SuperNet Training INFO: iter: 119040/144360  CE: 3.0117  
[05/06 22:45:17] SuperNet Training INFO: --> epoch:  99/120  avg CE: 3.0265  lr: 0.008841590138754444  
[05/06 22:46:24] SuperNet Training INFO: iter: 119160/144360  CE: 2.7630  
[05/06 22:47:24] SuperNet Training INFO: iter: 119280/144360  CE: 3.0130  
[05/06 22:48:25] SuperNet Training INFO: iter: 119400/144360  CE: 3.1102  
[05/06 22:49:26] SuperNet Training INFO: iter: 119520/144360  CE: 3.0925  
[05/06 22:50:26] SuperNet Training INFO: iter: 119640/144360  CE: 3.1240  
[05/06 22:51:27] SuperNet Training INFO: iter: 119760/144360  CE: 2.7863  
[05/06 22:52:27] SuperNet Training INFO: iter: 119880/144360  CE: 3.0408  
[05/06 22:53:26] SuperNet Training INFO: iter: 120000/144360  CE: 2.9525  
[05/06 22:54:27] SuperNet Training INFO: iter: 120120/144360  CE: 3.1709  
[05/06 22:55:27] SuperNet Training INFO: iter: 120240/144360  CE: 3.0491  
[05/06 22:55:56] SuperNet Training INFO: --> epoch: 100/120  avg CE: 3.0224  lr: 0.00803847577293368  
[05/06 22:57:03] SuperNet Training INFO: iter: 120360/144360  CE: 3.0321  
[05/06 22:58:02] SuperNet Training INFO: iter: 120480/144360  CE: 2.8042  
[05/06 22:59:01] SuperNet Training INFO: iter: 120600/144360  CE: 3.0742  
[05/06 23:00:01] SuperNet Training INFO: iter: 120720/144360  CE: 3.5357  
[05/06 23:01:02] SuperNet Training INFO: iter: 120840/144360  CE: 3.2874  
[05/06 23:02:03] SuperNet Training INFO: iter: 120960/144360  CE: 3.1220  
[05/06 23:03:02] SuperNet Training INFO: iter: 121080/144360  CE: 3.0386  
[05/06 23:04:03] SuperNet Training INFO: iter: 121200/144360  CE: 3.1275  
[05/06 23:05:03] SuperNet Training INFO: iter: 121320/144360  CE: 3.0465  
[05/06 23:06:05] SuperNet Training INFO: iter: 121440/144360  CE: 2.9049  
[05/06 23:06:36] SuperNet Training INFO: --> epoch: 101/120  avg CE: 3.0154  lr: 0.007270973240282054  
[05/06 23:07:42] SuperNet Training INFO: iter: 121560/144360  CE: 2.9159  
[05/06 23:08:43] SuperNet Training INFO: iter: 121680/144360  CE: 2.9902  
[05/06 23:09:43] SuperNet Training INFO: iter: 121800/144360  CE: 2.8162  
[05/06 23:10:43] SuperNet Training INFO: iter: 121920/144360  CE: 3.0973  
[05/06 23:11:42] SuperNet Training INFO: iter: 122040/144360  CE: 3.1013  
[05/06 23:12:42] SuperNet Training INFO: iter: 122160/144360  CE: 3.3669  
[05/06 23:13:42] SuperNet Training INFO: iter: 122280/144360  CE: 2.9975  
[05/06 23:14:44] SuperNet Training INFO: iter: 122400/144360  CE: 2.9702  
[05/06 23:15:45] SuperNet Training INFO: iter: 122520/144360  CE: 2.9021  
[05/06 23:16:46] SuperNet Training INFO: iter: 122640/144360  CE: 3.3930  
[05/06 23:17:18] SuperNet Training INFO: --> epoch: 102/120  avg CE: 3.0000  lr: 0.006539608548697928  
[05/06 23:18:21] SuperNet Training INFO: iter: 122760/144360  CE: 3.1061  
[05/06 23:19:22] SuperNet Training INFO: iter: 122880/144360  CE: 2.9257  
[05/06 23:20:22] SuperNet Training INFO: iter: 123000/144360  CE: 3.0047  
[05/06 23:21:21] SuperNet Training INFO: iter: 123120/144360  CE: 3.0177  
[05/06 23:22:22] SuperNet Training INFO: iter: 123240/144360  CE: 3.0418  
[05/06 23:23:23] SuperNet Training INFO: iter: 123360/144360  CE: 3.0924  
[05/06 23:24:23] SuperNet Training INFO: iter: 123480/144360  CE: 2.9612  
[05/06 23:25:24] SuperNet Training INFO: iter: 123600/144360  CE: 3.0357  
[05/06 23:26:25] SuperNet Training INFO: iter: 123720/144360  CE: 3.3277  
[05/06 23:27:27] SuperNet Training INFO: iter: 123840/144360  CE: 2.9050  
[05/06 23:28:00] SuperNet Training INFO: --> epoch: 103/120  avg CE: 3.0139  lr: 0.00584488293900834  
[05/06 23:29:01] SuperNet Training INFO: iter: 123960/144360  CE: 3.2355  
[05/06 23:30:00] SuperNet Training INFO: iter: 124080/144360  CE: 3.1541  
[05/06 23:31:01] SuperNet Training INFO: iter: 124200/144360  CE: 3.2436  
[05/06 23:32:02] SuperNet Training INFO: iter: 124320/144360  CE: 2.8778  
[05/06 23:33:03] SuperNet Training INFO: iter: 124440/144360  CE: 2.8487  
[05/06 23:34:05] SuperNet Training INFO: iter: 124560/144360  CE: 2.6853  
[05/06 23:35:05] SuperNet Training INFO: iter: 124680/144360  CE: 3.0589  
[05/06 23:36:06] SuperNet Training INFO: iter: 124800/144360  CE: 2.9120  
[05/06 23:37:07] SuperNet Training INFO: iter: 124920/144360  CE: 2.8297  
[05/06 23:38:07] SuperNet Training INFO: iter: 125040/144360  CE: 2.8545  
[05/06 23:38:42] SuperNet Training INFO: --> epoch: 104/120  avg CE: 2.9810  lr: 0.005187272541443939  
[05/06 23:39:43] SuperNet Training INFO: iter: 125160/144360  CE: 3.0056  
[05/06 23:40:44] SuperNet Training INFO: iter: 125280/144360  CE: 3.1792  
[05/06 23:41:43] SuperNet Training INFO: iter: 125400/144360  CE: 3.0538  
[05/06 23:42:45] SuperNet Training INFO: iter: 125520/144360  CE: 2.9488  
[05/06 23:43:46] SuperNet Training INFO: iter: 125640/144360  CE: 3.2170  
[05/06 23:44:47] SuperNet Training INFO: iter: 125760/144360  CE: 2.9092  
[05/06 23:45:48] SuperNet Training INFO: iter: 125880/144360  CE: 2.9896  
[05/06 23:46:48] SuperNet Training INFO: iter: 126000/144360  CE: 2.7839  
[05/06 23:47:49] SuperNet Training INFO: iter: 126120/144360  CE: 2.5587  
[05/06 23:48:50] SuperNet Training INFO: iter: 126240/144360  CE: 2.9598  
[05/06 23:49:27] SuperNet Training INFO: --> epoch: 105/120  avg CE: 2.9839  lr: 0.00456722804932279  
[05/06 23:50:25] SuperNet Training INFO: iter: 126360/144360  CE: 2.7312  
[05/06 23:51:26] SuperNet Training INFO: iter: 126480/144360  CE: 2.9747  
[05/06 23:52:27] SuperNet Training INFO: iter: 126600/144360  CE: 3.1383  
[05/06 23:53:27] SuperNet Training INFO: iter: 126720/144360  CE: 3.1245  
[05/06 23:54:29] SuperNet Training INFO: iter: 126840/144360  CE: 3.0284  
[05/06 23:55:29] SuperNet Training INFO: iter: 126960/144360  CE: 3.0115  
[05/06 23:56:29] SuperNet Training INFO: iter: 127080/144360  CE: 3.0062  
[05/06 23:57:31] SuperNet Training INFO: iter: 127200/144360  CE: 2.8169  
[05/06 23:58:31] SuperNet Training INFO: iter: 127320/144360  CE: 3.1325  
[05/06 23:59:29] SuperNet Training INFO: iter: 127440/144360  CE: 2.8344  
[05/07 00:00:07] SuperNet Training INFO: --> epoch: 106/120  avg CE: 2.9726  lr: 0.003985174410167894  
[05/07 00:01:05] SuperNet Training INFO: iter: 127560/144360  CE: 2.7627  
[05/07 00:02:06] SuperNet Training INFO: iter: 127680/144360  CE: 2.9531  
[05/07 00:03:07] SuperNet Training INFO: iter: 127800/144360  CE: 2.9687  
[05/07 00:04:08] SuperNet Training INFO: iter: 127920/144360  CE: 3.1918  
[05/07 00:05:09] SuperNet Training INFO: iter: 128040/144360  CE: 2.7784  
[05/07 00:06:09] SuperNet Training INFO: iter: 128160/144360  CE: 3.3314  
[05/07 00:07:10] SuperNet Training INFO: iter: 128280/144360  CE: 2.8461  
[05/07 00:08:11] SuperNet Training INFO: iter: 128400/144360  CE: 3.0155  
[05/07 00:09:12] SuperNet Training INFO: iter: 128520/144360  CE: 2.9108  
[05/07 00:10:13] SuperNet Training INFO: iter: 128640/144360  CE: 2.9211  
[05/07 00:10:53] SuperNet Training INFO: --> epoch: 107/120  avg CE: 2.9763  lr: 0.003441510534469298  
[05/07 00:11:50] SuperNet Training INFO: iter: 128760/144360  CE: 2.9000  
[05/07 00:12:52] SuperNet Training INFO: iter: 128880/144360  CE: 2.8101  
[05/07 00:13:54] SuperNet Training INFO: iter: 129000/144360  CE: 2.8580  
[05/07 00:14:55] SuperNet Training INFO: iter: 129120/144360  CE: 3.2288  
[05/07 00:15:57] SuperNet Training INFO: iter: 129240/144360  CE: 3.1678  
[05/07 00:16:57] SuperNet Training INFO: iter: 129360/144360  CE: 2.9937  
[05/07 00:17:57] SuperNet Training INFO: iter: 129480/144360  CE: 3.1747  
[05/07 00:18:55] SuperNet Training INFO: iter: 129600/144360  CE: 3.1818  
[05/07 00:19:54] SuperNet Training INFO: iter: 129720/144360  CE: 3.0484  
[05/07 00:20:53] SuperNet Training INFO: iter: 129840/144360  CE: 3.1567  
[05/07 00:21:33] SuperNet Training INFO: --> epoch: 108/120  avg CE: 2.9708  lr: 0.002936609022290792  
[05/07 00:22:27] SuperNet Training INFO: iter: 129960/144360  CE: 3.2140  
[05/07 00:23:28] SuperNet Training INFO: iter: 130080/144360  CE: 3.0644  
[05/07 00:24:28] SuperNet Training INFO: iter: 130200/144360  CE: 2.9212  
[05/07 00:25:28] SuperNet Training INFO: iter: 130320/144360  CE: 3.0148  
[05/07 00:26:28] SuperNet Training INFO: iter: 130440/144360  CE: 3.0416  
[05/07 00:27:28] SuperNet Training INFO: iter: 130560/144360  CE: 3.0242  
[05/07 00:28:27] SuperNet Training INFO: iter: 130680/144360  CE: 3.3634  
[05/07 00:29:27] SuperNet Training INFO: iter: 130800/144360  CE: 3.2851  
[05/07 00:30:27] SuperNet Training INFO: iter: 130920/144360  CE: 3.3499  
[05/07 00:31:28] SuperNet Training INFO: iter: 131040/144360  CE: 3.0979  
[05/07 00:32:11] SuperNet Training INFO: --> epoch: 109/120  avg CE: 2.9741  lr: 0.0024708159079084185  
[05/07 00:33:04] SuperNet Training INFO: iter: 131160/144360  CE: 2.9341  
[05/07 00:34:06] SuperNet Training INFO: iter: 131280/144360  CE: 3.1074  
[05/07 00:35:06] SuperNet Training INFO: iter: 131400/144360  CE: 2.7030  
[05/07 00:36:06] SuperNet Training INFO: iter: 131520/144360  CE: 2.9262  
[05/07 00:37:06] SuperNet Training INFO: iter: 131640/144360  CE: 3.1587  
[05/07 00:38:07] SuperNet Training INFO: iter: 131760/144360  CE: 3.1936  
[05/07 00:39:08] SuperNet Training INFO: iter: 131880/144360  CE: 3.2237  
[05/07 00:40:09] SuperNet Training INFO: iter: 132000/144360  CE: 2.9906  
[05/07 00:41:09] SuperNet Training INFO: iter: 132120/144360  CE: 3.2095  
[05/07 00:42:09] SuperNet Training INFO: iter: 132240/144360  CE: 3.4585  
[05/07 00:42:53] SuperNet Training INFO: --> epoch: 110/120  avg CE: 2.9715  lr: 0.0020444504226559065  
[05/07 00:43:45] SuperNet Training INFO: iter: 132360/144360  CE: 3.5187  
[05/07 00:44:45] SuperNet Training INFO: iter: 132480/144360  CE: 3.2596  
[05/07 00:45:45] SuperNet Training INFO: iter: 132600/144360  CE: 2.8559  
[05/07 00:46:45] SuperNet Training INFO: iter: 132720/144360  CE: 2.8049  
[05/07 00:47:46] SuperNet Training INFO: iter: 132840/144360  CE: 2.9348  
[05/07 00:48:48] SuperNet Training INFO: iter: 132960/144360  CE: 3.0232  
[05/07 00:49:46] SuperNet Training INFO: iter: 133080/144360  CE: 3.0711  
[05/07 00:50:45] SuperNet Training INFO: iter: 133200/144360  CE: 2.8159  
[05/07 00:51:45] SuperNet Training INFO: iter: 133320/144360  CE: 2.9772  
[05/07 00:52:45] SuperNet Training INFO: iter: 133440/144360  CE: 2.9518  
[05/07 00:53:29] SuperNet Training INFO: --> epoch: 111/120  avg CE: 2.9578  lr: 0.0016578047761394107  
[05/07 00:54:19] SuperNet Training INFO: iter: 133560/144360  CE: 3.0038  
[05/07 00:55:21] SuperNet Training INFO: iter: 133680/144360  CE: 2.9280  
[05/07 00:56:21] SuperNet Training INFO: iter: 133800/144360  CE: 3.1463  
[05/07 00:57:22] SuperNet Training INFO: iter: 133920/144360  CE: 3.1230  
[05/07 00:58:23] SuperNet Training INFO: iter: 134040/144360  CE: 2.9551  
[05/07 00:59:24] SuperNet Training INFO: iter: 134160/144360  CE: 2.9644  
[05/07 01:00:26] SuperNet Training INFO: iter: 134280/144360  CE: 3.5274  
[05/07 01:01:26] SuperNet Training INFO: iter: 134400/144360  CE: 2.9585  
[05/07 01:02:26] SuperNet Training INFO: iter: 134520/144360  CE: 2.8905  
[05/07 01:03:27] SuperNet Training INFO: iter: 134640/144360  CE: 2.9525  
[05/07 01:04:14] SuperNet Training INFO: --> epoch: 112/120  avg CE: 2.9655  lr: 0.0013111439559716617  
[05/07 01:05:03] SuperNet Training INFO: iter: 134760/144360  CE: 2.9155  
[05/07 01:06:04] SuperNet Training INFO: iter: 134880/144360  CE: 3.0782  
[05/07 01:07:06] SuperNet Training INFO: iter: 135000/144360  CE: 2.8228  
[05/07 01:08:08] SuperNet Training INFO: iter: 135120/144360  CE: 2.8382  
[05/07 01:09:10] SuperNet Training INFO: iter: 135240/144360  CE: 3.2169  
[05/07 01:10:11] SuperNet Training INFO: iter: 135360/144360  CE: 3.1912  
[05/07 01:11:11] SuperNet Training INFO: iter: 135480/144360  CE: 3.0250  
[05/07 01:12:12] SuperNet Training INFO: iter: 135600/144360  CE: 2.8831  
[05/07 01:13:13] SuperNet Training INFO: iter: 135720/144360  CE: 2.7103  
[05/07 01:14:16] SuperNet Training INFO: iter: 135840/144360  CE: 2.7386  
[05/07 01:15:06] SuperNet Training INFO: --> epoch: 113/120  avg CE: 2.9615  lr: 0.0010047055461627253  
[05/07 01:15:54] SuperNet Training INFO: iter: 135960/144360  CE: 2.9656  
[05/07 01:16:54] SuperNet Training INFO: iter: 136080/144360  CE: 2.9419  
[05/07 01:17:54] SuperNet Training INFO: iter: 136200/144360  CE: 2.7636  
[05/07 01:18:55] SuperNet Training INFO: iter: 136320/144360  CE: 3.0901  
[05/07 01:19:55] SuperNet Training INFO: iter: 136440/144360  CE: 2.8771  
[05/07 01:20:54] SuperNet Training INFO: iter: 136560/144360  CE: 2.7825  
[05/07 01:21:53] SuperNet Training INFO: iter: 136680/144360  CE: 3.0820  
[05/07 01:22:54] SuperNet Training INFO: iter: 136800/144360  CE: 2.9741  
[05/07 01:23:55] SuperNet Training INFO: iter: 136920/144360  CE: 2.9076  
[05/07 01:24:55] SuperNet Training INFO: iter: 137040/144360  CE: 3.1020  
[05/07 01:25:45] SuperNet Training INFO: --> epoch: 114/120  avg CE: 2.9512  lr: 0.000738699564291742  
[05/07 01:26:32] SuperNet Training INFO: iter: 137160/144360  CE: 2.7602  
[05/07 01:27:32] SuperNet Training INFO: iter: 137280/144360  CE: 2.5389  
[05/07 01:28:32] SuperNet Training INFO: iter: 137400/144360  CE: 2.8135  
[05/07 01:29:34] SuperNet Training INFO: iter: 137520/144360  CE: 2.9705  
[05/07 01:30:35] SuperNet Training INFO: iter: 137640/144360  CE: 2.8089  
[05/07 01:31:36] SuperNet Training INFO: iter: 137760/144360  CE: 2.9208  
[05/07 01:32:37] SuperNet Training INFO: iter: 137880/144360  CE: 2.7154  
[05/07 01:33:39] SuperNet Training INFO: iter: 138000/144360  CE: 3.0270  
[05/07 01:34:41] SuperNet Training INFO: iter: 138120/144360  CE: 2.8158  
[05/07 01:35:41] SuperNet Training INFO: iter: 138240/144360  CE: 3.0608  
[05/07 01:36:34] SuperNet Training INFO: --> epoch: 115/120  avg CE: 2.9425  lr: 0.0005133083175713779  
[05/07 01:37:18] SuperNet Training INFO: iter: 138360/144360  CE: 2.9432  
[05/07 01:38:18] SuperNet Training INFO: iter: 138480/144360  CE: 3.1588  
[05/07 01:39:19] SuperNet Training INFO: iter: 138600/144360  CE: 2.8283  
[05/07 01:40:20] SuperNet Training INFO: iter: 138720/144360  CE: 2.8719  
[05/07 01:41:21] SuperNet Training INFO: iter: 138840/144360  CE: 2.8650  
[05/07 01:42:22] SuperNet Training INFO: iter: 138960/144360  CE: 3.1453  
[05/07 01:43:23] SuperNet Training INFO: iter: 139080/144360  CE: 2.9616  
[05/07 01:44:23] SuperNet Training INFO: iter: 139200/144360  CE: 2.9362  
[05/07 01:45:23] SuperNet Training INFO: iter: 139320/144360  CE: 2.8342  
[05/07 01:46:23] SuperNet Training INFO: iter: 139440/144360  CE: 2.7570  
[05/07 01:47:16] SuperNet Training INFO: --> epoch: 116/120  avg CE: 2.9427  lr: 0.00032868627790359544  
[05/07 01:47:59] SuperNet Training INFO: iter: 139560/144360  CE: 3.1085  
[05/07 01:49:00] SuperNet Training INFO: iter: 139680/144360  CE: 2.8799  
[05/07 01:50:01] SuperNet Training INFO: iter: 139800/144360  CE: 2.9655  
[05/07 01:51:01] SuperNet Training INFO: iter: 139920/144360  CE: 2.8805  
[05/07 01:52:02] SuperNet Training INFO: iter: 140040/144360  CE: 2.8072  
[05/07 01:53:05] SuperNet Training INFO: iter: 140160/144360  CE: 3.0884  
[05/07 01:54:06] SuperNet Training INFO: iter: 140280/144360  CE: 2.5497  
[05/07 01:55:05] SuperNet Training INFO: iter: 140400/144360  CE: 2.8722  
[05/07 01:56:03] SuperNet Training INFO: iter: 140520/144360  CE: 3.0282  
[05/07 01:57:03] SuperNet Training INFO: iter: 140640/144360  CE: 2.6104  
[05/07 01:57:57] SuperNet Training INFO: --> epoch: 117/120  avg CE: 2.9497  lr: 0.00018495997601232129  
[05/07 01:58:38] SuperNet Training INFO: iter: 140760/144360  CE: 2.9463  
[05/07 01:59:40] SuperNet Training INFO: iter: 140880/144360  CE: 3.0255  
[05/07 02:00:43] SuperNet Training INFO: iter: 141000/144360  CE: 3.1198  
[05/07 02:01:46] SuperNet Training INFO: iter: 141120/144360  CE: 2.8663  
[05/07 02:02:45] SuperNet Training INFO: iter: 141240/144360  CE: 3.1920  
[05/07 02:03:48] SuperNet Training INFO: iter: 141360/144360  CE: 3.0109  
[05/07 02:04:49] SuperNet Training INFO: iter: 141480/144360  CE: 2.7965  
[05/07 02:05:49] SuperNet Training INFO: iter: 141600/144360  CE: 3.2602  
[05/07 02:06:51] SuperNet Training INFO: iter: 141720/144360  CE: 2.7993  
[05/07 02:07:52] SuperNet Training INFO: iter: 141840/144360  CE: 2.7807  
[05/07 02:08:49] SuperNet Training INFO: --> epoch: 118/120  avg CE: 2.9427  lr: 8.222791472556962e-05  
[05/07 02:09:28] SuperNet Training INFO: iter: 141960/144360  CE: 2.8789  
[05/07 02:10:29] SuperNet Training INFO: iter: 142080/144360  CE: 2.9661  
[05/07 02:11:29] SuperNet Training INFO: iter: 142200/144360  CE: 3.2095  
[05/07 02:12:30] SuperNet Training INFO: iter: 142320/144360  CE: 3.0446  
[05/07 02:13:31] SuperNet Training INFO: iter: 142440/144360  CE: 3.0328  
[05/07 02:14:31] SuperNet Training INFO: iter: 142560/144360  CE: 2.8755  
[05/07 02:15:34] SuperNet Training INFO: iter: 142680/144360  CE: 3.0962  
[05/07 02:16:33] SuperNet Training INFO: iter: 142800/144360  CE: 3.0735  
[05/07 02:17:34] SuperNet Training INFO: iter: 142920/144360  CE: 2.8562  
[05/07 02:18:36] SuperNet Training INFO: iter: 143040/144360  CE: 2.7997  
[05/07 02:19:34] SuperNet Training INFO: --> epoch: 119/120  avg CE: 2.9381  lr: 2.0560501466564365e-05  
[05/07 02:20:11] SuperNet Training INFO: iter: 143160/144360  CE: 2.7796  
[05/07 02:21:14] SuperNet Training INFO: iter: 143280/144360  CE: 2.7957  
[05/07 02:22:15] SuperNet Training INFO: iter: 143400/144360  CE: 2.7783  
[05/07 02:23:17] SuperNet Training INFO: iter: 143520/144360  CE: 2.8835  
[05/07 02:24:18] SuperNet Training INFO: iter: 143640/144360  CE: 3.1283  
[05/07 02:25:19] SuperNet Training INFO: iter: 143760/144360  CE: 3.0923  
[05/07 02:26:21] SuperNet Training INFO: iter: 143880/144360  CE: 3.1087  
[05/07 02:27:21] SuperNet Training INFO: iter: 144000/144360  CE: 3.1143  
[05/07 02:28:21] SuperNet Training INFO: iter: 144120/144360  CE: 3.0539  
[05/07 02:29:22] SuperNet Training INFO: iter: 144240/144360  CE: 3.0458  
[05/07 02:30:21] SuperNet Training INFO: iter: 144360/144360  CE: 2.7226  
[05/07 02:30:21] SuperNet Training INFO: --> epoch: 120/120  avg CE: 2.9425  lr: 0.0  
[05/07 02:30:21] SuperNet Training INFO: --> END mobile0-tbs2-seed-0
[05/07 02:30:21] SuperNet Training INFO: {0: 72180, 1: 72180}
[05/07 02:30:28] SuperNet Training INFO: ELAPSED TIME: 77224.9(s) = 21(h) 27(m)
