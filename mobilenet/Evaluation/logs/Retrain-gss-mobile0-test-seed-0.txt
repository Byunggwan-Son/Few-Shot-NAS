[04/15 09:28:40] Re-training INFO: tag                 : gss-mobile0-test
[04/15 09:28:40] Re-training INFO: arch                : [6, 4, 12, 6, 2, 4, 6, 8, 6, 4, 8, 10, 6, 0, 0, 4, 4, 6, 9, 0, 11]
[04/15 09:28:40] Re-training INFO: seed                : 0
[04/15 09:28:40] Re-training INFO: data_path           : ../../../dataset/ILSVRC2012
[04/15 09:28:40] Re-training INFO: save_path           : ./Evaluation
[04/15 09:28:40] Re-training INFO: search_space        : greedy
[04/15 09:28:40] Re-training INFO: valid_size          : 0
[04/15 09:28:40] Re-training INFO: num_gpus            : 8
[04/15 09:28:40] Re-training INFO: workers             : 4
[04/15 09:28:40] Re-training INFO: interval_ep_eval    : 20
[04/15 09:28:40] Re-training INFO: train_batch_size    : 1024
[04/15 09:28:40] Re-training INFO: test_batch_size     : 256
[04/15 09:28:40] Re-training INFO: max_epoch           : 240
[04/15 09:28:40] Re-training INFO: learning_rate       : 0.512
[04/15 09:28:40] Re-training INFO: momentum            : 0.9
[04/15 09:28:40] Re-training INFO: weight_decay        : 1e-05
[04/15 09:28:40] Re-training INFO: nesterov            : False
[04/15 09:28:40] Re-training INFO: lr_schedule_type    : cosine
[04/15 09:28:40] Re-training INFO: warmup              : True
[04/15 09:28:40] Re-training INFO: drop_out            : 0.2
[04/15 09:28:40] Re-training INFO: label_smooth        : 0.1
[04/15 09:28:40] Re-training INFO: rank                : 0
[04/15 09:28:40] Re-training INFO: gpu                 : 0
[04/15 09:28:40] Re-training INFO: save_name           : Retrain-gss-mobile0-test-seed-0
[04/15 09:28:40] Re-training INFO: log_path            : ./Evaluation/logs/Retrain-gss-mobile0-test-seed-0.txt
[04/15 09:28:40] Re-training INFO: ckpt_path           : ./Evaluation/checkpoint/Retrain-gss-mobile0-test-seed-0.pt
[04/15 09:28:40] Re-training INFO: dist_url            : tcp://127.0.0.1:23456
[04/15 09:28:40] Re-training INFO: world_size          : 8
[04/15 09:28:40] Re-training INFO: distributed         : True
[04/15 09:28:40] Re-training INFO: ['3x3_MBConv3', '3x3_MBConv6', '5x5_MBConv3', '5x5_MBConv6', '7x7_MBConv3', '7x7_MBConv6', '3x3_MBConv3_SE', '3x3_MBConv6_SE', '5x5_MBConv3_SE', '5x5_MBConv6_SE', '7x7_MBConv3_SE', '7x7_MBConv6_SE', 'Identity']
[04/15 09:29:17] Re-training INFO: CNN(
  (first_conv): Sequential(
    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU6(inplace=True)
  )
  (first_block): InvertedResidual(
    (depth_conv): Sequential(
      (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU6(inplace=True)
    )
    (point_linear): Sequential(
      (0): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (blocks): ModuleList(
    (0): InvertedResidual_SE(
      (inverted_bottleneck): Sequential(
        (0): Conv2d(16, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
      )
      (depth_conv): Sequential(
        (0): Conv2d(48, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=48, bias=False)
        (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
      )
      (point_linear): Sequential(
        (0): Conv2d(48, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (se): SqueezeExcite(
        (conv_reduce): Conv2d(48, 16, kernel_size=(1, 1), stride=(1, 1))
        (act1): ReLU6(inplace=True)
        (conv_expand): Conv2d(16, 48, kernel_size=(1, 1), stride=(1, 1))
        (gate): Sigmoid()
      )
    )
    (1): InvertedResidual(
      (inverted_bottleneck): Sequential(
        (0): Conv2d(32, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
      )
      (depth_conv): Sequential(
        (0): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96, bias=False)
        (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
      )
      (point_linear): Sequential(
        (0): Conv2d(96, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (2): Identity()
    (3): InvertedResidual_SE(
      (inverted_bottleneck): Sequential(
        (0): Conv2d(32, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
      )
      (depth_conv): Sequential(
        (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
        (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
      )
      (point_linear): Sequential(
        (0): Conv2d(96, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (se): SqueezeExcite(
        (conv_reduce): Conv2d(96, 32, kernel_size=(1, 1), stride=(1, 1))
        (act1): ReLU6(inplace=True)
        (conv_expand): Conv2d(32, 96, kernel_size=(1, 1), stride=(1, 1))
        (gate): Sigmoid()
      )
    )
    (4): InvertedResidual(
      (inverted_bottleneck): Sequential(
        (0): Conv2d(32, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
      )
      (depth_conv): Sequential(
        (0): Conv2d(96, 96, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=96, bias=False)
        (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
      )
      (point_linear): Sequential(
        (0): Conv2d(96, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (5): InvertedResidual(
      (inverted_bottleneck): Sequential(
        (0): Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
      )
      (depth_conv): Sequential(
        (0): Conv2d(120, 120, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=120, bias=False)
        (1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
      )
      (point_linear): Sequential(
        (0): Conv2d(120, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (6): InvertedResidual_SE(
      (inverted_bottleneck): Sequential(
        (0): Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
      )
      (depth_conv): Sequential(
        (0): Conv2d(120, 120, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=120, bias=False)
        (1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
      )
      (point_linear): Sequential(
        (0): Conv2d(120, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (se): SqueezeExcite(
        (conv_reduce): Conv2d(120, 40, kernel_size=(1, 1), stride=(1, 1))
        (act1): ReLU6(inplace=True)
        (conv_expand): Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1))
        (gate): Sigmoid()
      )
    )
    (7): InvertedResidual_SE(
      (inverted_bottleneck): Sequential(
        (0): Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
      )
      (depth_conv): Sequential(
        (0): Conv2d(120, 120, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=120, bias=False)
        (1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
      )
      (point_linear): Sequential(
        (0): Conv2d(120, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (se): SqueezeExcite(
        (conv_reduce): Conv2d(120, 40, kernel_size=(1, 1), stride=(1, 1))
        (act1): ReLU6(inplace=True)
        (conv_expand): Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1))
        (gate): Sigmoid()
      )
    )
    (8): InvertedResidual_SE(
      (inverted_bottleneck): Sequential(
        (0): Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
      )
      (depth_conv): Sequential(
        (0): Conv2d(120, 120, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=120, bias=False)
        (1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
      )
      (point_linear): Sequential(
        (0): Conv2d(120, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (se): SqueezeExcite(
        (conv_reduce): Conv2d(120, 40, kernel_size=(1, 1), stride=(1, 1))
        (act1): ReLU6(inplace=True)
        (conv_expand): Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1))
        (gate): Sigmoid()
      )
    )
    (9): InvertedResidual(
      (inverted_bottleneck): Sequential(
        (0): Conv2d(80, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
      )
      (depth_conv): Sequential(
        (0): Conv2d(240, 240, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=240, bias=False)
        (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
      )
      (point_linear): Sequential(
        (0): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (10): InvertedResidual_SE(
      (inverted_bottleneck): Sequential(
        (0): Conv2d(80, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
      )
      (depth_conv): Sequential(
        (0): Conv2d(240, 240, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=240, bias=False)
        (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
      )
      (point_linear): Sequential(
        (0): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (se): SqueezeExcite(
        (conv_reduce): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1))
        (act1): ReLU6(inplace=True)
        (conv_expand): Conv2d(80, 240, kernel_size=(1, 1), stride=(1, 1))
        (gate): Sigmoid()
      )
    )
    (11): InvertedResidual_SE(
      (inverted_bottleneck): Sequential(
        (0): Conv2d(80, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
      )
      (depth_conv): Sequential(
        (0): Conv2d(240, 240, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=240, bias=False)
        (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
      )
      (point_linear): Sequential(
        (0): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (se): SqueezeExcite(
        (conv_reduce): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1))
        (act1): ReLU6(inplace=True)
        (conv_expand): Conv2d(80, 240, kernel_size=(1, 1), stride=(1, 1))
        (gate): Sigmoid()
      )
    )
    (12): InvertedResidual_SE(
      (inverted_bottleneck): Sequential(
        (0): Conv2d(80, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
      )
      (depth_conv): Sequential(
        (0): Conv2d(240, 240, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=240, bias=False)
        (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
      )
      (point_linear): Sequential(
        (0): Conv2d(240, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (se): SqueezeExcite(
        (conv_reduce): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1))
        (act1): ReLU6(inplace=True)
        (conv_expand): Conv2d(80, 240, kernel_size=(1, 1), stride=(1, 1))
        (gate): Sigmoid()
      )
    )
    (13): InvertedResidual(
      (inverted_bottleneck): Sequential(
        (0): Conv2d(96, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
      )
      (depth_conv): Sequential(
        (0): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
        (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
      )
      (point_linear): Sequential(
        (0): Conv2d(288, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (14): InvertedResidual(
      (inverted_bottleneck): Sequential(
        (0): Conv2d(96, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
      )
      (depth_conv): Sequential(
        (0): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
        (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
      )
      (point_linear): Sequential(
        (0): Conv2d(288, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (15): InvertedResidual(
      (inverted_bottleneck): Sequential(
        (0): Conv2d(96, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
      )
      (depth_conv): Sequential(
        (0): Conv2d(288, 288, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=288, bias=False)
        (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
      )
      (point_linear): Sequential(
        (0): Conv2d(288, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (16): InvertedResidual(
      (inverted_bottleneck): Sequential(
        (0): Conv2d(96, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
      )
      (depth_conv): Sequential(
        (0): Conv2d(288, 288, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), groups=288, bias=False)
        (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
      )
      (point_linear): Sequential(
        (0): Conv2d(288, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (17): InvertedResidual_SE(
      (inverted_bottleneck): Sequential(
        (0): Conv2d(192, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
      )
      (depth_conv): Sequential(
        (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)
        (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
      )
      (point_linear): Sequential(
        (0): Conv2d(576, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (se): SqueezeExcite(
        (conv_reduce): Conv2d(576, 192, kernel_size=(1, 1), stride=(1, 1))
        (act1): ReLU6(inplace=True)
        (conv_expand): Conv2d(192, 576, kernel_size=(1, 1), stride=(1, 1))
        (gate): Sigmoid()
      )
    )
    (18): InvertedResidual_SE(
      (inverted_bottleneck): Sequential(
        (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
      )
      (depth_conv): Sequential(
        (0): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)
        (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
      )
      (point_linear): Sequential(
        (0): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (se): SqueezeExcite(
        (conv_reduce): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1))
        (act1): ReLU6(inplace=True)
        (conv_expand): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1))
        (gate): Sigmoid()
      )
    )
    (19): InvertedResidual(
      (inverted_bottleneck): Sequential(
        (0): Conv2d(192, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
      )
      (depth_conv): Sequential(
        (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)
        (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
      )
      (point_linear): Sequential(
        (0): Conv2d(576, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (20): InvertedResidual_SE(
      (inverted_bottleneck): Sequential(
        (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
      )
      (depth_conv): Sequential(
        (0): Conv2d(1152, 1152, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1152, bias=False)
        (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
      )
      (point_linear): Sequential(
        (0): Conv2d(1152, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (se): SqueezeExcite(
        (conv_reduce): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1))
        (act1): ReLU6(inplace=True)
        (conv_expand): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1))
        (gate): Sigmoid()
      )
    )
  )
  (feature_mix_layer): Sequential(
    (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (1): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU6(inplace=True)
  )
  (avgpool): AvgPool2d(kernel_size=7, stride=7, padding=0)
  (classifier): Sequential(
    (0): Dropout(p=0.2, inplace=False)
    (1): Linear(in_features=1280, out_features=1000, bias=True)
  )
)
[04/15 09:29:17] Re-training INFO: # of Params : 5.113
[04/15 09:29:29] Re-training INFO: Trainset Size: 1281167
[04/15 09:29:29] Re-training INFO: Validset Size:   50000
[04/15 09:29:29] Re-training INFO: Compose(
    RandomResizedCrop(size=(224, 224), scale=(0.08, 1.0), ratio=(0.75, 1.3333), interpolation=bilinear)
    RandomHorizontalFlip(p=0.5)
    ToTensor()
    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
)
[04/15 09:29:29] Re-training INFO: RMSprop (
Parameter Group 0
    alpha: 0.9
    centered: False
    eps: 1e-08
    initial_lr: 0.512
    lr: 0.512
    momentum: 0.9
    weight_decay: 1e-05

Parameter Group 1
    alpha: 0.9
    centered: False
    eps: 1e-08
    initial_lr: 0.512
    lr: 0.512
    momentum: 0.9
    weight_decay: 0.0
)
[04/15 09:29:29] Re-training INFO: --> START Retrain-gss-mobile0-test-seed-0
[04/15 09:30:04] Re-training INFO: tag                 : gss-mobile0-test
[04/15 09:30:04] Re-training INFO: arch                : [6, 4, 12, 6, 2, 4, 6, 8, 6, 4, 8, 10, 6, 0, 0, 4, 4, 6, 9, 0, 11]
[04/15 09:30:04] Re-training INFO: seed                : 0
[04/15 09:30:04] Re-training INFO: data_path           : ../../../dataset/ILSVRC2012
[04/15 09:30:04] Re-training INFO: save_path           : ./Evaluation
[04/15 09:30:04] Re-training INFO: search_space        : greedy
[04/15 09:30:04] Re-training INFO: valid_size          : 0
[04/15 09:30:04] Re-training INFO: num_gpus            : 8
[04/15 09:30:04] Re-training INFO: workers             : 4
[04/15 09:30:04] Re-training INFO: interval_ep_eval    : 20
[04/15 09:30:04] Re-training INFO: train_batch_size    : 1024
[04/15 09:30:04] Re-training INFO: test_batch_size     : 256
[04/15 09:30:04] Re-training INFO: max_epoch           : 240
[04/15 09:30:04] Re-training INFO: learning_rate       : 0.512
[04/15 09:30:04] Re-training INFO: momentum            : 0.9
[04/15 09:30:04] Re-training INFO: weight_decay        : 1e-05
[04/15 09:30:04] Re-training INFO: nesterov            : False
[04/15 09:30:04] Re-training INFO: lr_schedule_type    : cosine
[04/15 09:30:04] Re-training INFO: warmup              : True
[04/15 09:30:04] Re-training INFO: drop_out            : 0.2
[04/15 09:30:04] Re-training INFO: label_smooth        : 0.1
[04/15 09:30:04] Re-training INFO: rank                : 0
[04/15 09:30:04] Re-training INFO: gpu                 : 0
[04/15 09:30:04] Re-training INFO: save_name           : Retrain-gss-mobile0-test-seed-0
[04/15 09:30:04] Re-training INFO: log_path            : ./Evaluation/logs/Retrain-gss-mobile0-test-seed-0.txt
[04/15 09:30:04] Re-training INFO: ckpt_path           : ./Evaluation/checkpoint/Retrain-gss-mobile0-test-seed-0.pt
[04/15 09:30:04] Re-training INFO: dist_url            : tcp://127.0.0.1:23456
[04/15 09:30:04] Re-training INFO: world_size          : 8
[04/15 09:30:04] Re-training INFO: distributed         : True
[04/15 09:30:04] Re-training INFO: ['3x3_MBConv3', '3x3_MBConv6', '5x5_MBConv3', '5x5_MBConv6', '7x7_MBConv3', '7x7_MBConv6', '3x3_MBConv3_SE', '3x3_MBConv6_SE', '5x5_MBConv3_SE', '5x5_MBConv6_SE', '7x7_MBConv3_SE', '7x7_MBConv6_SE', 'Identity']
[04/15 09:30:46] Re-training INFO: CNN(
  (first_conv): Sequential(
    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU6(inplace=True)
  )
  (first_block): InvertedResidual(
    (depth_conv): Sequential(
      (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU6(inplace=True)
    )
    (point_linear): Sequential(
      (0): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (blocks): ModuleList(
    (0): InvertedResidual_SE(
      (inverted_bottleneck): Sequential(
        (0): Conv2d(16, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
      )
      (depth_conv): Sequential(
        (0): Conv2d(48, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=48, bias=False)
        (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
      )
      (point_linear): Sequential(
        (0): Conv2d(48, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (se): SqueezeExcite(
        (conv_reduce): Conv2d(48, 16, kernel_size=(1, 1), stride=(1, 1))
        (act1): ReLU6(inplace=True)
        (conv_expand): Conv2d(16, 48, kernel_size=(1, 1), stride=(1, 1))
        (gate): Sigmoid()
      )
    )
    (1): InvertedResidual(
      (inverted_bottleneck): Sequential(
        (0): Conv2d(32, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
      )
      (depth_conv): Sequential(
        (0): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96, bias=False)
        (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
      )
      (point_linear): Sequential(
        (0): Conv2d(96, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (2): Identity()
    (3): InvertedResidual_SE(
      (inverted_bottleneck): Sequential(
        (0): Conv2d(32, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
      )
      (depth_conv): Sequential(
        (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
        (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
      )
      (point_linear): Sequential(
        (0): Conv2d(96, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (se): SqueezeExcite(
        (conv_reduce): Conv2d(96, 32, kernel_size=(1, 1), stride=(1, 1))
        (act1): ReLU6(inplace=True)
        (conv_expand): Conv2d(32, 96, kernel_size=(1, 1), stride=(1, 1))
        (gate): Sigmoid()
      )
    )
    (4): InvertedResidual(
      (inverted_bottleneck): Sequential(
        (0): Conv2d(32, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
      )
      (depth_conv): Sequential(
        (0): Conv2d(96, 96, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=96, bias=False)
        (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
      )
      (point_linear): Sequential(
        (0): Conv2d(96, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (5): InvertedResidual(
      (inverted_bottleneck): Sequential(
        (0): Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
      )
      (depth_conv): Sequential(
        (0): Conv2d(120, 120, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=120, bias=False)
        (1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
      )
      (point_linear): Sequential(
        (0): Conv2d(120, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (6): InvertedResidual_SE(
      (inverted_bottleneck): Sequential(
        (0): Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
      )
      (depth_conv): Sequential(
        (0): Conv2d(120, 120, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=120, bias=False)
        (1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
      )
      (point_linear): Sequential(
        (0): Conv2d(120, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (se): SqueezeExcite(
        (conv_reduce): Conv2d(120, 40, kernel_size=(1, 1), stride=(1, 1))
        (act1): ReLU6(inplace=True)
        (conv_expand): Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1))
        (gate): Sigmoid()
      )
    )
    (7): InvertedResidual_SE(
      (inverted_bottleneck): Sequential(
        (0): Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
      )
      (depth_conv): Sequential(
        (0): Conv2d(120, 120, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=120, bias=False)
        (1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
      )
      (point_linear): Sequential(
        (0): Conv2d(120, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (se): SqueezeExcite(
        (conv_reduce): Conv2d(120, 40, kernel_size=(1, 1), stride=(1, 1))
        (act1): ReLU6(inplace=True)
        (conv_expand): Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1))
        (gate): Sigmoid()
      )
    )
    (8): InvertedResidual_SE(
      (inverted_bottleneck): Sequential(
        (0): Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
      )
      (depth_conv): Sequential(
        (0): Conv2d(120, 120, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=120, bias=False)
        (1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
      )
      (point_linear): Sequential(
        (0): Conv2d(120, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (se): SqueezeExcite(
        (conv_reduce): Conv2d(120, 40, kernel_size=(1, 1), stride=(1, 1))
        (act1): ReLU6(inplace=True)
        (conv_expand): Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1))
        (gate): Sigmoid()
      )
    )
    (9): InvertedResidual(
      (inverted_bottleneck): Sequential(
        (0): Conv2d(80, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
      )
      (depth_conv): Sequential(
        (0): Conv2d(240, 240, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=240, bias=False)
        (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
      )
      (point_linear): Sequential(
        (0): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (10): InvertedResidual_SE(
      (inverted_bottleneck): Sequential(
        (0): Conv2d(80, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
      )
      (depth_conv): Sequential(
        (0): Conv2d(240, 240, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=240, bias=False)
        (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
      )
      (point_linear): Sequential(
        (0): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (se): SqueezeExcite(
        (conv_reduce): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1))
        (act1): ReLU6(inplace=True)
        (conv_expand): Conv2d(80, 240, kernel_size=(1, 1), stride=(1, 1))
        (gate): Sigmoid()
      )
    )
    (11): InvertedResidual_SE(
      (inverted_bottleneck): Sequential(
        (0): Conv2d(80, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
      )
      (depth_conv): Sequential(
        (0): Conv2d(240, 240, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=240, bias=False)
        (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
      )
      (point_linear): Sequential(
        (0): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (se): SqueezeExcite(
        (conv_reduce): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1))
        (act1): ReLU6(inplace=True)
        (conv_expand): Conv2d(80, 240, kernel_size=(1, 1), stride=(1, 1))
        (gate): Sigmoid()
      )
    )
    (12): InvertedResidual_SE(
      (inverted_bottleneck): Sequential(
        (0): Conv2d(80, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
      )
      (depth_conv): Sequential(
        (0): Conv2d(240, 240, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=240, bias=False)
        (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
      )
      (point_linear): Sequential(
        (0): Conv2d(240, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (se): SqueezeExcite(
        (conv_reduce): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1))
        (act1): ReLU6(inplace=True)
        (conv_expand): Conv2d(80, 240, kernel_size=(1, 1), stride=(1, 1))
        (gate): Sigmoid()
      )
    )
    (13): InvertedResidual(
      (inverted_bottleneck): Sequential(
        (0): Conv2d(96, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
      )
      (depth_conv): Sequential(
        (0): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
        (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
      )
      (point_linear): Sequential(
        (0): Conv2d(288, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (14): InvertedResidual(
      (inverted_bottleneck): Sequential(
        (0): Conv2d(96, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
      )
      (depth_conv): Sequential(
        (0): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
        (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
      )
      (point_linear): Sequential(
        (0): Conv2d(288, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (15): InvertedResidual(
      (inverted_bottleneck): Sequential(
        (0): Conv2d(96, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
      )
      (depth_conv): Sequential(
        (0): Conv2d(288, 288, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=288, bias=False)
        (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
      )
      (point_linear): Sequential(
        (0): Conv2d(288, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (16): InvertedResidual(
      (inverted_bottleneck): Sequential(
        (0): Conv2d(96, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
      )
      (depth_conv): Sequential(
        (0): Conv2d(288, 288, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), groups=288, bias=False)
        (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
      )
      (point_linear): Sequential(
        (0): Conv2d(288, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (17): InvertedResidual_SE(
      (inverted_bottleneck): Sequential(
        (0): Conv2d(192, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
      )
      (depth_conv): Sequential(
        (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)
        (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
      )
      (point_linear): Sequential(
        (0): Conv2d(576, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (se): SqueezeExcite(
        (conv_reduce): Conv2d(576, 192, kernel_size=(1, 1), stride=(1, 1))
        (act1): ReLU6(inplace=True)
        (conv_expand): Conv2d(192, 576, kernel_size=(1, 1), stride=(1, 1))
        (gate): Sigmoid()
      )
    )
    (18): InvertedResidual_SE(
      (inverted_bottleneck): Sequential(
        (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
      )
      (depth_conv): Sequential(
        (0): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)
        (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
      )
      (point_linear): Sequential(
        (0): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (se): SqueezeExcite(
        (conv_reduce): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1))
        (act1): ReLU6(inplace=True)
        (conv_expand): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1))
        (gate): Sigmoid()
      )
    )
    (19): InvertedResidual(
      (inverted_bottleneck): Sequential(
        (0): Conv2d(192, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
      )
      (depth_conv): Sequential(
        (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)
        (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
      )
      (point_linear): Sequential(
        (0): Conv2d(576, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (20): InvertedResidual_SE(
      (inverted_bottleneck): Sequential(
        (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
      )
      (depth_conv): Sequential(
        (0): Conv2d(1152, 1152, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1152, bias=False)
        (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
      )
      (point_linear): Sequential(
        (0): Conv2d(1152, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (se): SqueezeExcite(
        (conv_reduce): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1))
        (act1): ReLU6(inplace=True)
        (conv_expand): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1))
        (gate): Sigmoid()
      )
    )
  )
  (feature_mix_layer): Sequential(
    (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (1): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU6(inplace=True)
  )
  (avgpool): AvgPool2d(kernel_size=7, stride=7, padding=0)
  (classifier): Sequential(
    (0): Dropout(p=0.2, inplace=False)
    (1): Linear(in_features=1280, out_features=1000, bias=True)
  )
)
[04/15 09:30:46] Re-training INFO: # of Params : 5.113
[04/15 09:30:58] Re-training INFO: Trainset Size: 1281167
[04/15 09:30:58] Re-training INFO: Validset Size:   50000
[04/15 09:30:58] Re-training INFO: Compose(
    RandomResizedCrop(size=(224, 224), scale=(0.08, 1.0), ratio=(0.75, 1.3333), interpolation=bilinear)
    RandomHorizontalFlip(p=0.5)
    ToTensor()
    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
)
[04/15 09:30:58] Re-training INFO: RMSprop (
Parameter Group 0
    alpha: 0.9
    centered: False
    eps: 1e-08
    initial_lr: 0.512
    lr: 0.512
    momentum: 0.9
    weight_decay: 1e-05

Parameter Group 1
    alpha: 0.9
    centered: False
    eps: 1e-08
    initial_lr: 0.512
    lr: 0.512
    momentum: 0.9
    weight_decay: 0.0
)
[04/15 09:30:58] Re-training INFO: --> START Retrain-gss-mobile0-test-seed-0
[04/15 09:33:30] Re-training INFO: iter:   125/300480  CE: 6.9897  
[04/15 09:34:35] Re-training INFO: iter:   250/300480  CE: 6.9114  
[04/15 09:35:40] Re-training INFO: iter:   375/300480  CE: 6.9216  
[04/15 09:36:45] Re-training INFO: iter:   500/300480  CE: 6.9959  
[04/15 09:37:48] Re-training INFO: iter:   625/300480  CE: 7.0665  
[04/15 09:38:53] Re-training INFO: iter:   750/300480  CE: 7.0275  
[04/15 09:39:57] Re-training INFO: iter:   875/300480  CE: 7.1179  
[04/15 09:41:01] Re-training INFO: iter:  1000/300480  CE: 7.0792  
[04/15 09:42:05] Re-training INFO: iter:  1125/300480  CE: 7.0580  
[04/15 09:43:08] Re-training INFO: iter:  1250/300480  CE: 7.0292  
[04/15 09:43:10] Re-training INFO: --> epoch:   1/240  avg CE: 6.9886  lr: 0.1024  
[04/15 09:44:31] Re-training INFO: iter:  1375/300480  CE: 9.1295  
[04/15 09:45:33] Re-training INFO: iter:  1500/300480  CE: 7.0671  
[04/15 09:46:37] Re-training INFO: iter:  1625/300480  CE: 7.2035  
[04/15 09:47:43] Re-training INFO: iter:  1750/300480  CE: 7.1193  
[04/15 09:48:50] Re-training INFO: iter:  1875/300480  CE: 7.1963  
[04/15 09:49:56] Re-training INFO: iter:  2000/300480  CE: 7.1461  
[04/15 09:51:02] Re-training INFO: iter:  2125/300480  CE: 8.7250  
[04/15 09:52:07] Re-training INFO: iter:  2250/300480  CE: 10.6486  
[04/15 09:53:13] Re-training INFO: iter:  2375/300480  CE: 7.2237  
[04/15 09:54:18] Re-training INFO: iter:  2500/300480  CE: 7.3099  
[04/15 09:54:19] Re-training INFO: --> epoch:   2/240  avg CE: 7.6004  lr: 0.2048  
[04/15 09:55:42] Re-training INFO: iter:  2625/300480  CE: 7.3977  
[04/15 09:56:49] Re-training INFO: iter:  2750/300480  CE: 7.2128  
[04/15 09:57:55] Re-training INFO: iter:  2875/300480  CE: 7.2834  
[04/15 09:59:01] Re-training INFO: iter:  3000/300480  CE: 7.2904  
[04/15 10:00:08] Re-training INFO: iter:  3125/300480  CE: 7.5265  
[04/15 10:01:14] Re-training INFO: iter:  3250/300480  CE: 7.4217  
[04/15 10:02:19] Re-training INFO: iter:  3375/300480  CE: 7.3647  
[04/15 10:03:25] Re-training INFO: iter:  3500/300480  CE: 7.3876  
[04/15 10:04:30] Re-training INFO: iter:  3625/300480  CE: 7.5319  
[04/15 10:05:36] Re-training INFO: iter:  3750/300480  CE: 7.4162  
[04/15 10:05:37] Re-training INFO: --> epoch:   3/240  avg CE: 7.4564  lr: 0.30720000000000003  
[04/15 10:06:59] Re-training INFO: iter:  3875/300480  CE: 7.3756  
[04/15 10:08:06] Re-training INFO: iter:  4000/300480  CE: 7.3913  
[04/15 10:09:15] Re-training INFO: iter:  4125/300480  CE: 7.5301  
[04/15 10:10:22] Re-training INFO: iter:  4250/300480  CE: 7.4280  
[04/15 10:11:29] Re-training INFO: iter:  4375/300480  CE: 7.9114  
[04/15 10:12:35] Re-training INFO: iter:  4500/300480  CE: 7.3970  
[04/15 10:13:41] Re-training INFO: iter:  4625/300480  CE: 7.5904  
[04/15 10:14:47] Re-training INFO: iter:  4750/300480  CE: 7.5741  
[04/15 10:15:52] Re-training INFO: iter:  4875/300480  CE: 8.4957  
[04/15 10:16:57] Re-training INFO: iter:  5000/300480  CE: 7.6073  
[04/15 10:17:00] Re-training INFO: --> epoch:   4/240  avg CE: 7.7366  lr: 0.4096  
[04/15 10:18:20] Re-training INFO: iter:  5125/300480  CE: 7.8431  
[04/15 10:19:26] Re-training INFO: iter:  5250/300480  CE: 7.5640  
[04/15 10:20:31] Re-training INFO: iter:  5375/300480  CE: 7.6937  
[04/15 10:21:37] Re-training INFO: iter:  5500/300480  CE: 7.7312  
[04/15 10:22:42] Re-training INFO: iter:  5625/300480  CE: 7.6041  
[04/15 10:23:47] Re-training INFO: iter:  5750/300480  CE: 7.7421  
[04/15 10:24:53] Re-training INFO: iter:  5875/300480  CE: 7.6700  
[04/15 10:25:59] Re-training INFO: iter:  6000/300480  CE: 7.9251  
[04/15 10:27:03] Re-training INFO: iter:  6125/300480  CE: 7.9977  
[04/15 10:28:08] Re-training INFO: iter:  6250/300480  CE: 7.7627  
[04/15 10:28:12] Re-training INFO: --> epoch:   5/240  avg CE: 7.8930  lr: 0.5119182108486301  
[04/15 10:29:32] Re-training INFO: iter:  6375/300480  CE: 8.1212  
[04/15 10:30:38] Re-training INFO: iter:  6500/300480  CE: 8.0424  
[04/15 10:31:43] Re-training INFO: iter:  6625/300480  CE: 7.7288  
[04/15 10:32:48] Re-training INFO: iter:  6750/300480  CE: 7.7063  
[04/15 10:33:54] Re-training INFO: iter:  6875/300480  CE: 7.6315  
[04/15 10:35:01] Re-training INFO: iter:  7000/300480  CE: 7.8870  
[04/15 10:36:06] Re-training INFO: iter:  7125/300480  CE: 7.9561  
[04/15 10:37:10] Re-training INFO: iter:  7250/300480  CE: 7.7757  
[04/15 10:38:14] Re-training INFO: iter:  7375/300480  CE: 7.7258  
[04/15 10:39:17] Re-training INFO: iter:  7500/300480  CE: 7.7644  
[04/15 10:39:22] Re-training INFO: --> epoch:   6/240  avg CE: 7.8555  lr: 0.5118962471818433  
[04/15 10:40:40] Re-training INFO: iter:  7625/300480  CE: 8.0124  
[04/15 10:41:46] Re-training INFO: iter:  7750/300480  CE: 8.0569  
[04/15 10:42:53] Re-training INFO: iter:  7875/300480  CE: 7.9719  
[04/15 10:43:59] Re-training INFO: iter:  8000/300480  CE: 8.2552  
[04/15 10:45:05] Re-training INFO: iter:  8125/300480  CE: 7.5631  
[04/15 10:46:10] Re-training INFO: iter:  8250/300480  CE: 7.7098  
[04/15 10:47:14] Re-training INFO: iter:  8375/300480  CE: 8.0099  
[04/15 10:48:19] Re-training INFO: iter:  8500/300480  CE: 7.7621  
[04/15 10:49:23] Re-training INFO: iter:  8625/300480  CE: 7.7978  
[04/15 10:50:27] Re-training INFO: iter:  8750/300480  CE: 7.8003  
[04/15 10:50:33] Re-training INFO: --> epoch:   7/240  avg CE: 7.8552  lr: 0.5118304300033839  
[04/15 10:51:50] Re-training INFO: iter:  8875/300480  CE: 7.8066  
[04/15 10:52:56] Re-training INFO: iter:  9000/300480  CE: 7.7477  
[04/15 10:54:01] Re-training INFO: iter:  9125/300480  CE: 7.8060  
[04/15 10:55:05] Re-training INFO: iter:  9250/300480  CE: 7.8373  
[04/15 10:56:08] Re-training INFO: iter:  9375/300480  CE: 7.9274  
[04/15 10:57:12] Re-training INFO: iter:  9500/300480  CE: 7.8269  
[04/15 10:58:15] Re-training INFO: iter:  9625/300480  CE: 7.9183  
[04/15 10:59:18] Re-training INFO: iter:  9750/300480  CE: 7.7133  
[04/15 11:00:21] Re-training INFO: iter:  9875/300480  CE: 7.7959  
[04/15 11:01:21] Re-training INFO: iter: 10000/300480  CE: 7.8209  
[04/15 11:01:27] Re-training INFO: --> epoch:   8/240  avg CE: 7.8526  lr: 0.5117207705906865  
[04/15 11:02:41] Re-training INFO: iter: 10125/300480  CE: 7.9659  
[04/15 11:03:45] Re-training INFO: iter: 10250/300480  CE: 7.8446  
[04/15 11:04:49] Re-training INFO: iter: 10375/300480  CE: 7.8190  
[04/15 11:05:53] Re-training INFO: iter: 10500/300480  CE: 7.5703  
[04/15 11:06:57] Re-training INFO: iter: 10625/300480  CE: 7.7996  
[04/15 11:08:01] Re-training INFO: iter: 10750/300480  CE: 7.7053  
[04/15 11:09:04] Re-training INFO: iter: 10875/300480  CE: 7.9163  
[04/15 11:10:08] Re-training INFO: iter: 11000/300480  CE: 7.9946  
[04/15 11:11:10] Re-training INFO: iter: 11125/300480  CE: 7.7528  
[04/15 11:12:13] Re-training INFO: iter: 11250/300480  CE: 7.7737  
[04/15 11:12:20] Re-training INFO: --> epoch:   9/240  avg CE: 7.8565  lr: 0.5115672877333283  
[04/15 11:13:34] Re-training INFO: iter: 11375/300480  CE: 7.9001  
[04/15 11:14:42] Re-training INFO: iter: 11500/300480  CE: 7.8592  
[04/15 11:15:46] Re-training INFO: iter: 11625/300480  CE: 7.8078  
[04/15 11:16:52] Re-training INFO: iter: 11750/300480  CE: 7.6901  
[04/15 11:17:54] Re-training INFO: iter: 11875/300480  CE: 7.8106  
[04/15 11:18:58] Re-training INFO: iter: 12000/300480  CE: 7.8242  
[04/15 11:20:01] Re-training INFO: iter: 12125/300480  CE: 8.0832  
[04/15 11:21:03] Re-training INFO: iter: 12250/300480  CE: 7.9285  
[04/15 11:22:06] Re-training INFO: iter: 12375/300480  CE: 7.8348  
[04/15 11:23:09] Re-training INFO: iter: 12500/300480  CE: 7.8206  
[04/15 11:23:18] Re-training INFO: --> epoch:  10/240  avg CE: 7.8570  lr: 0.5113700077298113  
[04/15 11:24:31] Re-training INFO: iter: 12625/300480  CE: 7.9443  
[04/15 11:25:36] Re-training INFO: iter: 12750/300480  CE: 8.0727  
[04/15 11:26:42] Re-training INFO: iter: 12875/300480  CE: 8.1590  
[04/15 11:27:47] Re-training INFO: iter: 13000/300480  CE: 7.9447  
[04/15 11:28:51] Re-training INFO: iter: 13125/300480  CE: 7.7301  
[04/15 11:29:56] Re-training INFO: iter: 13250/300480  CE: 7.8923  
[04/15 11:31:00] Re-training INFO: iter: 13375/300480  CE: 7.9685  
[04/15 11:32:02] Re-training INFO: iter: 13500/300480  CE: 7.6786  
[04/15 11:33:05] Re-training INFO: iter: 13625/300480  CE: 8.0522  
[04/15 11:34:06] Re-training INFO: iter: 13750/300480  CE: 7.9764  
[04/15 11:34:15] Re-training INFO: --> epoch:  11/240  avg CE: 7.8569  lr: 0.511128964383044  
[04/15 11:35:28] Re-training INFO: iter: 13875/300480  CE: 7.8535  
[04/15 11:36:33] Re-training INFO: iter: 14000/300480  CE: 7.7678  
[04/15 11:37:38] Re-training INFO: iter: 14125/300480  CE: 7.9167  
[04/15 11:38:44] Re-training INFO: iter: 14250/300480  CE: 8.1053  
[04/15 11:39:48] Re-training INFO: iter: 14375/300480  CE: 7.7451  
[04/15 11:40:52] Re-training INFO: iter: 14500/300480  CE: 7.7288  
[04/15 11:41:55] Re-training INFO: iter: 14625/300480  CE: 7.9810  
[04/15 11:42:58] Re-training INFO: iter: 14750/300480  CE: 7.7024  
[04/15 11:44:01] Re-training INFO: iter: 14875/300480  CE: 7.8117  
[04/15 11:45:04] Re-training INFO: iter: 15000/300480  CE: 7.9703  
[04/15 11:45:15] Re-training INFO: --> epoch:  12/240  avg CE: 7.8581  lr: 0.5108441989945636  
[04/15 11:46:26] Re-training INFO: iter: 15125/300480  CE: 7.6654  
[04/15 11:47:32] Re-training INFO: iter: 15250/300480  CE: 7.8942  
[04/15 11:48:37] Re-training INFO: iter: 15375/300480  CE: 7.6874  
[04/15 11:49:41] Re-training INFO: iter: 15500/300480  CE: 7.6595  
[04/15 11:50:45] Re-training INFO: iter: 15625/300480  CE: 7.6402  
[04/15 11:51:47] Re-training INFO: iter: 15750/300480  CE: 7.5788  
[04/15 11:52:50] Re-training INFO: iter: 15875/300480  CE: 7.8781  
[04/15 11:53:54] Re-training INFO: iter: 16000/300480  CE: 7.7382  
[04/15 11:54:57] Re-training INFO: iter: 16125/300480  CE: 7.9912  
[04/15 11:55:59] Re-training INFO: iter: 16250/300480  CE: 7.7347  
[04/15 11:56:11] Re-training INFO: --> epoch:  13/240  avg CE: 7.8589  lr: 0.5105157603574504  
[04/15 11:57:20] Re-training INFO: iter: 16375/300480  CE: 7.7683  
[04/15 11:58:25] Re-training INFO: iter: 16500/300480  CE: 8.1466  
[04/15 11:59:32] Re-training INFO: iter: 16625/300480  CE: 7.9827  
[04/15 12:00:35] Re-training INFO: iter: 16750/300480  CE: 7.9734  
[04/15 12:01:40] Re-training INFO: iter: 16875/300480  CE: 7.9044  
[04/15 12:02:45] Re-training INFO: iter: 17000/300480  CE: 7.7645  
[04/15 12:11:24] Re-training INFO: tag                 : gss-mobile0-test
[04/15 12:11:24] Re-training INFO: arch                : [6, 4, 12, 6, 2, 4, 6, 8, 6, 4, 8, 10, 6, 0, 0, 4, 4, 6, 9, 0, 11]
[04/15 12:11:24] Re-training INFO: seed                : 0
[04/15 12:11:24] Re-training INFO: data_path           : ../../../dataset/ILSVRC2012
[04/15 12:11:24] Re-training INFO: save_path           : ./Evaluation
[04/15 12:11:24] Re-training INFO: search_space        : greedy
[04/15 12:11:24] Re-training INFO: valid_size          : 0
[04/15 12:11:24] Re-training INFO: num_gpus            : 8
[04/15 12:11:24] Re-training INFO: workers             : 4
[04/15 12:11:24] Re-training INFO: interval_ep_eval    : 20
[04/15 12:11:24] Re-training INFO: train_batch_size    : 1024
[04/15 12:11:24] Re-training INFO: test_batch_size     : 256
[04/15 12:11:24] Re-training INFO: max_epoch           : 240
[04/15 12:11:24] Re-training INFO: learning_rate       : 0.512
[04/15 12:11:24] Re-training INFO: momentum            : 0.9
[04/15 12:11:24] Re-training INFO: weight_decay        : 1e-05
[04/15 12:11:24] Re-training INFO: nesterov            : False
[04/15 12:11:24] Re-training INFO: lr_schedule_type    : cosine
[04/15 12:11:24] Re-training INFO: warmup              : True
[04/15 12:11:24] Re-training INFO: drop_out            : 0.2
[04/15 12:11:24] Re-training INFO: label_smooth        : 0.1
[04/15 12:11:24] Re-training INFO: rank                : 0
[04/15 12:11:24] Re-training INFO: gpu                 : 0
[04/15 12:11:24] Re-training INFO: save_name           : Retrain-gss-mobile0-test-seed-0
[04/15 12:11:24] Re-training INFO: log_path            : ./Evaluation/logs/Retrain-gss-mobile0-test-seed-0.txt
[04/15 12:11:24] Re-training INFO: ckpt_path           : ./Evaluation/checkpoint/Retrain-gss-mobile0-test-seed-0.pt
[04/15 12:11:24] Re-training INFO: dist_url            : tcp://127.0.0.1:23456
[04/15 12:11:24] Re-training INFO: world_size          : 8
[04/15 12:11:24] Re-training INFO: distributed         : True
[04/15 12:11:24] Re-training INFO: ['3x3_MBConv3', '3x3_MBConv6', '5x5_MBConv3', '5x5_MBConv6', '7x7_MBConv3', '7x7_MBConv6', '3x3_MBConv3_SE', '3x3_MBConv6_SE', '5x5_MBConv3_SE', '5x5_MBConv6_SE', '7x7_MBConv3_SE', '7x7_MBConv6_SE', 'Identity']
[04/15 12:12:11] Re-training INFO: CNN(
  (first_conv): Sequential(
    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU6(inplace=True)
  )
  (first_block): InvertedResidual(
    (depth_conv): Sequential(
      (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU6(inplace=True)
    )
    (point_linear): Sequential(
      (0): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (blocks): ModuleList(
    (0): InvertedResidual_SE(
      (inverted_bottleneck): Sequential(
        (0): Conv2d(16, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
      )
      (depth_conv): Sequential(
        (0): Conv2d(48, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=48, bias=False)
        (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
      )
      (point_linear): Sequential(
        (0): Conv2d(48, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (se): SqueezeExcite(
        (conv_reduce): Conv2d(48, 16, kernel_size=(1, 1), stride=(1, 1))
        (act1): ReLU6(inplace=True)
        (conv_expand): Conv2d(16, 48, kernel_size=(1, 1), stride=(1, 1))
        (gate): Sigmoid()
      )
    )
    (1): InvertedResidual(
      (inverted_bottleneck): Sequential(
        (0): Conv2d(32, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
      )
      (depth_conv): Sequential(
        (0): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96, bias=False)
        (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
      )
      (point_linear): Sequential(
        (0): Conv2d(96, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (2): Identity()
    (3): InvertedResidual_SE(
      (inverted_bottleneck): Sequential(
        (0): Conv2d(32, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
      )
      (depth_conv): Sequential(
        (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
        (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
      )
      (point_linear): Sequential(
        (0): Conv2d(96, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (se): SqueezeExcite(
        (conv_reduce): Conv2d(96, 32, kernel_size=(1, 1), stride=(1, 1))
        (act1): ReLU6(inplace=True)
        (conv_expand): Conv2d(32, 96, kernel_size=(1, 1), stride=(1, 1))
        (gate): Sigmoid()
      )
    )
    (4): InvertedResidual(
      (inverted_bottleneck): Sequential(
        (0): Conv2d(32, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
      )
      (depth_conv): Sequential(
        (0): Conv2d(96, 96, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=96, bias=False)
        (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
      )
      (point_linear): Sequential(
        (0): Conv2d(96, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (5): InvertedResidual(
      (inverted_bottleneck): Sequential(
        (0): Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
      )
      (depth_conv): Sequential(
        (0): Conv2d(120, 120, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=120, bias=False)
        (1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
      )
      (point_linear): Sequential(
        (0): Conv2d(120, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (6): InvertedResidual_SE(
      (inverted_bottleneck): Sequential(
        (0): Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
      )
      (depth_conv): Sequential(
        (0): Conv2d(120, 120, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=120, bias=False)
        (1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
      )
      (point_linear): Sequential(
        (0): Conv2d(120, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (se): SqueezeExcite(
        (conv_reduce): Conv2d(120, 40, kernel_size=(1, 1), stride=(1, 1))
        (act1): ReLU6(inplace=True)
        (conv_expand): Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1))
        (gate): Sigmoid()
      )
    )
    (7): InvertedResidual_SE(
      (inverted_bottleneck): Sequential(
        (0): Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
      )
      (depth_conv): Sequential(
        (0): Conv2d(120, 120, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=120, bias=False)
        (1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
      )
      (point_linear): Sequential(
        (0): Conv2d(120, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (se): SqueezeExcite(
        (conv_reduce): Conv2d(120, 40, kernel_size=(1, 1), stride=(1, 1))
        (act1): ReLU6(inplace=True)
        (conv_expand): Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1))
        (gate): Sigmoid()
      )
    )
    (8): InvertedResidual_SE(
      (inverted_bottleneck): Sequential(
        (0): Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
      )
      (depth_conv): Sequential(
        (0): Conv2d(120, 120, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=120, bias=False)
        (1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
      )
      (point_linear): Sequential(
        (0): Conv2d(120, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (se): SqueezeExcite(
        (conv_reduce): Conv2d(120, 40, kernel_size=(1, 1), stride=(1, 1))
        (act1): ReLU6(inplace=True)
        (conv_expand): Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1))
        (gate): Sigmoid()
      )
    )
    (9): InvertedResidual(
      (inverted_bottleneck): Sequential(
        (0): Conv2d(80, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
      )
      (depth_conv): Sequential(
        (0): Conv2d(240, 240, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=240, bias=False)
        (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
      )
      (point_linear): Sequential(
        (0): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (10): InvertedResidual_SE(
      (inverted_bottleneck): Sequential(
        (0): Conv2d(80, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
      )
      (depth_conv): Sequential(
        (0): Conv2d(240, 240, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=240, bias=False)
        (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
      )
      (point_linear): Sequential(
        (0): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (se): SqueezeExcite(
        (conv_reduce): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1))
        (act1): ReLU6(inplace=True)
        (conv_expand): Conv2d(80, 240, kernel_size=(1, 1), stride=(1, 1))
        (gate): Sigmoid()
      )
    )
    (11): InvertedResidual_SE(
      (inverted_bottleneck): Sequential(
        (0): Conv2d(80, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
      )
      (depth_conv): Sequential(
        (0): Conv2d(240, 240, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=240, bias=False)
        (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
      )
      (point_linear): Sequential(
        (0): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (se): SqueezeExcite(
        (conv_reduce): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1))
        (act1): ReLU6(inplace=True)
        (conv_expand): Conv2d(80, 240, kernel_size=(1, 1), stride=(1, 1))
        (gate): Sigmoid()
      )
    )
    (12): InvertedResidual_SE(
      (inverted_bottleneck): Sequential(
        (0): Conv2d(80, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
      )
      (depth_conv): Sequential(
        (0): Conv2d(240, 240, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=240, bias=False)
        (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
      )
      (point_linear): Sequential(
        (0): Conv2d(240, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (se): SqueezeExcite(
        (conv_reduce): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1))
        (act1): ReLU6(inplace=True)
        (conv_expand): Conv2d(80, 240, kernel_size=(1, 1), stride=(1, 1))
        (gate): Sigmoid()
      )
    )
    (13): InvertedResidual(
      (inverted_bottleneck): Sequential(
        (0): Conv2d(96, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
      )
      (depth_conv): Sequential(
        (0): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
        (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
      )
      (point_linear): Sequential(
        (0): Conv2d(288, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (14): InvertedResidual(
      (inverted_bottleneck): Sequential(
        (0): Conv2d(96, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
      )
      (depth_conv): Sequential(
        (0): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
        (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
      )
      (point_linear): Sequential(
        (0): Conv2d(288, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (15): InvertedResidual(
      (inverted_bottleneck): Sequential(
        (0): Conv2d(96, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
      )
      (depth_conv): Sequential(
        (0): Conv2d(288, 288, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=288, bias=False)
        (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
      )
      (point_linear): Sequential(
        (0): Conv2d(288, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (16): InvertedResidual(
      (inverted_bottleneck): Sequential(
        (0): Conv2d(96, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
      )
      (depth_conv): Sequential(
        (0): Conv2d(288, 288, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), groups=288, bias=False)
        (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
      )
      (point_linear): Sequential(
        (0): Conv2d(288, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (17): InvertedResidual_SE(
      (inverted_bottleneck): Sequential(
        (0): Conv2d(192, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
      )
      (depth_conv): Sequential(
        (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)
        (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
      )
      (point_linear): Sequential(
        (0): Conv2d(576, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (se): SqueezeExcite(
        (conv_reduce): Conv2d(576, 192, kernel_size=(1, 1), stride=(1, 1))
        (act1): ReLU6(inplace=True)
        (conv_expand): Conv2d(192, 576, kernel_size=(1, 1), stride=(1, 1))
        (gate): Sigmoid()
      )
    )
    (18): InvertedResidual_SE(
      (inverted_bottleneck): Sequential(
        (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
      )
      (depth_conv): Sequential(
        (0): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)
        (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
      )
      (point_linear): Sequential(
        (0): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (se): SqueezeExcite(
        (conv_reduce): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1))
        (act1): ReLU6(inplace=True)
        (conv_expand): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1))
        (gate): Sigmoid()
      )
    )
    (19): InvertedResidual(
      (inverted_bottleneck): Sequential(
        (0): Conv2d(192, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
      )
      (depth_conv): Sequential(
        (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)
        (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
      )
      (point_linear): Sequential(
        (0): Conv2d(576, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (20): InvertedResidual_SE(
      (inverted_bottleneck): Sequential(
        (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
      )
      (depth_conv): Sequential(
        (0): Conv2d(1152, 1152, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1152, bias=False)
        (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
      )
      (point_linear): Sequential(
        (0): Conv2d(1152, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (se): SqueezeExcite(
        (conv_reduce): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1))
        (act1): ReLU6(inplace=True)
        (conv_expand): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1))
        (gate): Sigmoid()
      )
    )
  )
  (feature_mix_layer): Sequential(
    (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (1): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU6(inplace=True)
  )
  (avgpool): AvgPool2d(kernel_size=7, stride=7, padding=0)
  (classifier): Sequential(
    (0): Dropout(p=0.2, inplace=False)
    (1): Linear(in_features=1280, out_features=1000, bias=True)
  )
)
[04/15 12:12:11] Re-training INFO: # of Params : 5.113
[04/15 12:12:25] Re-training INFO: Trainset Size: 1281167
[04/15 12:12:25] Re-training INFO: Validset Size:   50000
[04/15 12:12:25] Re-training INFO: Compose(
    RandomResizedCrop(size=(224, 224), scale=(0.08, 1.0), ratio=(0.75, 1.3333), interpolation=bilinear)
    RandomHorizontalFlip(p=0.5)
    ToTensor()
    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
)
[04/15 12:13:45] Re-training INFO: tag                 : gss-mobile0-test
[04/15 12:13:45] Re-training INFO: arch                : [6, 4, 12, 6, 2, 4, 6, 8, 6, 4, 8, 10, 6, 0, 0, 4, 4, 6, 9, 0, 11]
[04/15 12:13:45] Re-training INFO: seed                : 0
[04/15 12:13:45] Re-training INFO: data_path           : ../../../dataset/ILSVRC2012
[04/15 12:13:45] Re-training INFO: save_path           : ./Evaluation
[04/15 12:13:45] Re-training INFO: search_space        : greedy
[04/15 12:13:45] Re-training INFO: valid_size          : 0
[04/15 12:13:45] Re-training INFO: num_gpus            : 8
[04/15 12:13:45] Re-training INFO: workers             : 4
[04/15 12:13:45] Re-training INFO: interval_ep_eval    : 20
[04/15 12:13:45] Re-training INFO: train_batch_size    : 1024
[04/15 12:13:45] Re-training INFO: test_batch_size     : 256
[04/15 12:13:45] Re-training INFO: max_epoch           : 240
[04/15 12:13:45] Re-training INFO: learning_rate       : 0.512
[04/15 12:13:45] Re-training INFO: momentum            : 0.9
[04/15 12:13:45] Re-training INFO: weight_decay        : 1e-05
[04/15 12:13:45] Re-training INFO: nesterov            : False
[04/15 12:13:45] Re-training INFO: lr_schedule_type    : cosine
[04/15 12:13:45] Re-training INFO: warmup              : True
[04/15 12:13:45] Re-training INFO: drop_out            : 0.2
[04/15 12:13:45] Re-training INFO: label_smooth        : 0.1
[04/15 12:13:45] Re-training INFO: rank                : 0
[04/15 12:13:45] Re-training INFO: gpu                 : 0
[04/15 12:13:45] Re-training INFO: save_name           : Retrain-gss-mobile0-test-seed-0
[04/15 12:13:45] Re-training INFO: log_path            : ./Evaluation/logs/Retrain-gss-mobile0-test-seed-0.txt
[04/15 12:13:45] Re-training INFO: ckpt_path           : ./Evaluation/checkpoint/Retrain-gss-mobile0-test-seed-0.pt
[04/15 12:13:45] Re-training INFO: dist_url            : tcp://127.0.0.1:23456
[04/15 12:13:45] Re-training INFO: world_size          : 8
[04/15 12:13:45] Re-training INFO: distributed         : True
[04/15 12:13:45] Re-training INFO: ['3x3_MBConv3', '3x3_MBConv6', '5x5_MBConv3', '5x5_MBConv6', '7x7_MBConv3', '7x7_MBConv6', '3x3_MBConv3_SE', '3x3_MBConv6_SE', '5x5_MBConv3_SE', '5x5_MBConv6_SE', '7x7_MBConv3_SE', '7x7_MBConv6_SE', 'Identity']
[04/15 12:14:31] Re-training INFO: CNN(
  (first_conv): Sequential(
    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU6(inplace=True)
  )
  (first_block): InvertedResidual(
    (depth_conv): Sequential(
      (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU6(inplace=True)
    )
    (point_linear): Sequential(
      (0): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (blocks): ModuleList(
    (0): InvertedResidual_SE(
      (inverted_bottleneck): Sequential(
        (0): Conv2d(16, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
      )
      (depth_conv): Sequential(
        (0): Conv2d(48, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=48, bias=False)
        (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
      )
      (point_linear): Sequential(
        (0): Conv2d(48, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (se): SqueezeExcite(
        (conv_reduce): Conv2d(48, 16, kernel_size=(1, 1), stride=(1, 1))
        (act1): ReLU6(inplace=True)
        (conv_expand): Conv2d(16, 48, kernel_size=(1, 1), stride=(1, 1))
        (gate): Sigmoid()
      )
    )
    (1): InvertedResidual(
      (inverted_bottleneck): Sequential(
        (0): Conv2d(32, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
      )
      (depth_conv): Sequential(
        (0): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96, bias=False)
        (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
      )
      (point_linear): Sequential(
        (0): Conv2d(96, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (2): Identity()
    (3): InvertedResidual_SE(
      (inverted_bottleneck): Sequential(
        (0): Conv2d(32, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
      )
      (depth_conv): Sequential(
        (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
        (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
      )
      (point_linear): Sequential(
        (0): Conv2d(96, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (se): SqueezeExcite(
        (conv_reduce): Conv2d(96, 32, kernel_size=(1, 1), stride=(1, 1))
        (act1): ReLU6(inplace=True)
        (conv_expand): Conv2d(32, 96, kernel_size=(1, 1), stride=(1, 1))
        (gate): Sigmoid()
      )
    )
    (4): InvertedResidual(
      (inverted_bottleneck): Sequential(
        (0): Conv2d(32, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
      )
      (depth_conv): Sequential(
        (0): Conv2d(96, 96, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=96, bias=False)
        (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
      )
      (point_linear): Sequential(
        (0): Conv2d(96, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (5): InvertedResidual(
      (inverted_bottleneck): Sequential(
        (0): Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
      )
      (depth_conv): Sequential(
        (0): Conv2d(120, 120, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=120, bias=False)
        (1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
      )
      (point_linear): Sequential(
        (0): Conv2d(120, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (6): InvertedResidual_SE(
      (inverted_bottleneck): Sequential(
        (0): Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
      )
      (depth_conv): Sequential(
        (0): Conv2d(120, 120, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=120, bias=False)
        (1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
      )
      (point_linear): Sequential(
        (0): Conv2d(120, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (se): SqueezeExcite(
        (conv_reduce): Conv2d(120, 40, kernel_size=(1, 1), stride=(1, 1))
        (act1): ReLU6(inplace=True)
        (conv_expand): Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1))
        (gate): Sigmoid()
      )
    )
    (7): InvertedResidual_SE(
      (inverted_bottleneck): Sequential(
        (0): Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
      )
      (depth_conv): Sequential(
        (0): Conv2d(120, 120, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=120, bias=False)
        (1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
      )
      (point_linear): Sequential(
        (0): Conv2d(120, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (se): SqueezeExcite(
        (conv_reduce): Conv2d(120, 40, kernel_size=(1, 1), stride=(1, 1))
        (act1): ReLU6(inplace=True)
        (conv_expand): Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1))
        (gate): Sigmoid()
      )
    )
    (8): InvertedResidual_SE(
      (inverted_bottleneck): Sequential(
        (0): Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
      )
      (depth_conv): Sequential(
        (0): Conv2d(120, 120, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=120, bias=False)
        (1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
      )
      (point_linear): Sequential(
        (0): Conv2d(120, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (se): SqueezeExcite(
        (conv_reduce): Conv2d(120, 40, kernel_size=(1, 1), stride=(1, 1))
        (act1): ReLU6(inplace=True)
        (conv_expand): Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1))
        (gate): Sigmoid()
      )
    )
    (9): InvertedResidual(
      (inverted_bottleneck): Sequential(
        (0): Conv2d(80, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
      )
      (depth_conv): Sequential(
        (0): Conv2d(240, 240, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=240, bias=False)
        (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
      )
      (point_linear): Sequential(
        (0): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (10): InvertedResidual_SE(
      (inverted_bottleneck): Sequential(
        (0): Conv2d(80, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
      )
      (depth_conv): Sequential(
        (0): Conv2d(240, 240, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=240, bias=False)
        (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
      )
      (point_linear): Sequential(
        (0): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (se): SqueezeExcite(
        (conv_reduce): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1))
        (act1): ReLU6(inplace=True)
        (conv_expand): Conv2d(80, 240, kernel_size=(1, 1), stride=(1, 1))
        (gate): Sigmoid()
      )
    )
    (11): InvertedResidual_SE(
      (inverted_bottleneck): Sequential(
        (0): Conv2d(80, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
      )
      (depth_conv): Sequential(
        (0): Conv2d(240, 240, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=240, bias=False)
        (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
      )
      (point_linear): Sequential(
        (0): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (se): SqueezeExcite(
        (conv_reduce): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1))
        (act1): ReLU6(inplace=True)
        (conv_expand): Conv2d(80, 240, kernel_size=(1, 1), stride=(1, 1))
        (gate): Sigmoid()
      )
    )
    (12): InvertedResidual_SE(
      (inverted_bottleneck): Sequential(
        (0): Conv2d(80, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
      )
      (depth_conv): Sequential(
        (0): Conv2d(240, 240, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=240, bias=False)
        (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
      )
      (point_linear): Sequential(
        (0): Conv2d(240, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (se): SqueezeExcite(
        (conv_reduce): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1))
        (act1): ReLU6(inplace=True)
        (conv_expand): Conv2d(80, 240, kernel_size=(1, 1), stride=(1, 1))
        (gate): Sigmoid()
      )
    )
    (13): InvertedResidual(
      (inverted_bottleneck): Sequential(
        (0): Conv2d(96, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
      )
      (depth_conv): Sequential(
        (0): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
        (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
      )
      (point_linear): Sequential(
        (0): Conv2d(288, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (14): InvertedResidual(
      (inverted_bottleneck): Sequential(
        (0): Conv2d(96, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
      )
      (depth_conv): Sequential(
        (0): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
        (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
      )
      (point_linear): Sequential(
        (0): Conv2d(288, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (15): InvertedResidual(
      (inverted_bottleneck): Sequential(
        (0): Conv2d(96, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
      )
      (depth_conv): Sequential(
        (0): Conv2d(288, 288, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=288, bias=False)
        (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
      )
      (point_linear): Sequential(
        (0): Conv2d(288, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (16): InvertedResidual(
      (inverted_bottleneck): Sequential(
        (0): Conv2d(96, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
      )
      (depth_conv): Sequential(
        (0): Conv2d(288, 288, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), groups=288, bias=False)
        (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
      )
      (point_linear): Sequential(
        (0): Conv2d(288, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (17): InvertedResidual_SE(
      (inverted_bottleneck): Sequential(
        (0): Conv2d(192, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
      )
      (depth_conv): Sequential(
        (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)
        (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
      )
      (point_linear): Sequential(
        (0): Conv2d(576, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (se): SqueezeExcite(
        (conv_reduce): Conv2d(576, 192, kernel_size=(1, 1), stride=(1, 1))
        (act1): ReLU6(inplace=True)
        (conv_expand): Conv2d(192, 576, kernel_size=(1, 1), stride=(1, 1))
        (gate): Sigmoid()
      )
    )
    (18): InvertedResidual_SE(
      (inverted_bottleneck): Sequential(
        (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
      )
      (depth_conv): Sequential(
        (0): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)
        (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
      )
      (point_linear): Sequential(
        (0): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (se): SqueezeExcite(
        (conv_reduce): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1))
        (act1): ReLU6(inplace=True)
        (conv_expand): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1))
        (gate): Sigmoid()
      )
    )
    (19): InvertedResidual(
      (inverted_bottleneck): Sequential(
        (0): Conv2d(192, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
      )
      (depth_conv): Sequential(
        (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)
        (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
      )
      (point_linear): Sequential(
        (0): Conv2d(576, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (20): InvertedResidual_SE(
      (inverted_bottleneck): Sequential(
        (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
      )
      (depth_conv): Sequential(
        (0): Conv2d(1152, 1152, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1152, bias=False)
        (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
      )
      (point_linear): Sequential(
        (0): Conv2d(1152, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (se): SqueezeExcite(
        (conv_reduce): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1))
        (act1): ReLU6(inplace=True)
        (conv_expand): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1))
        (gate): Sigmoid()
      )
    )
  )
  (feature_mix_layer): Sequential(
    (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (1): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU6(inplace=True)
  )
  (avgpool): AvgPool2d(kernel_size=7, stride=7, padding=0)
  (classifier): Sequential(
    (0): Dropout(p=0.2, inplace=False)
    (1): Linear(in_features=1280, out_features=1000, bias=True)
  )
)
[04/15 12:14:31] Re-training INFO: # of Params : 5.113
[04/15 12:14:44] Re-training INFO: Trainset Size: 1281167
[04/15 12:14:44] Re-training INFO: Validset Size:   50000
[04/15 12:14:44] Re-training INFO: Compose(
    RandomResizedCrop(size=(224, 224), scale=(0.08, 1.0), ratio=(0.75, 1.3333), interpolation=bilinear)
    RandomHorizontalFlip(p=0.5)
    ToTensor()
    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
)
[04/15 12:14:44] Re-training INFO: RMSpropTF (
Parameter Group 0
    alpha: 0.9
    centered: False
    decoupled_decay: False
    eps: 1e-10
    initial_lr: 0.512
    lr: 0.512
    lr_in_momentum: True
    momentum: 0.9
    weight_decay: 1e-05

Parameter Group 1
    alpha: 0.9
    centered: False
    decoupled_decay: False
    eps: 1e-10
    initial_lr: 0.512
    lr: 0.512
    lr_in_momentum: True
    momentum: 0.9
    weight_decay: 0.0
)
[04/15 12:14:44] Re-training INFO: --> START Retrain-gss-mobile0-test-seed-0
[04/15 12:17:18] Re-training INFO: iter:   125/300480  CE: 6.7750  
[04/15 12:18:22] Re-training INFO: iter:   250/300480  CE: 6.8180  
[04/15 12:19:25] Re-training INFO: iter:   375/300480  CE: 6.8276  
[04/15 12:20:30] Re-training INFO: iter:   500/300480  CE: 6.7650  
[04/15 12:21:35] Re-training INFO: iter:   625/300480  CE: 6.9970  
[04/15 12:22:39] Re-training INFO: iter:   750/300480  CE: 7.0507  
[04/15 12:23:44] Re-training INFO: iter:   875/300480  CE: 7.1059  
[04/15 12:24:49] Re-training INFO: iter:  1000/300480  CE: 7.0778  
[04/15 12:25:53] Re-training INFO: iter:  1125/300480  CE: 7.0191  
[04/15 12:26:56] Re-training INFO: iter:  1250/300480  CE: 7.0231  
[04/15 12:26:56] Re-training INFO: --> epoch:   1/240  avg CE: 6.9496  lr: 0.1024  
[04/15 12:28:19] Re-training INFO: iter:  1375/300480  CE: 7.1597  
[04/15 12:29:25] Re-training INFO: iter:  1500/300480  CE: 7.1063  
[04/15 12:30:32] Re-training INFO: iter:  1625/300480  CE: 7.1833  
[04/15 12:31:38] Re-training INFO: iter:  1750/300480  CE: 7.1489  
[04/15 12:32:45] Re-training INFO: iter:  1875/300480  CE: 7.1894  
[04/15 12:33:51] Re-training INFO: iter:  2000/300480  CE: 7.2297  
[04/15 12:34:57] Re-training INFO: iter:  2125/300480  CE: 7.1300  
[04/15 12:36:02] Re-training INFO: iter:  2250/300480  CE: 7.3626  
[04/15 12:37:09] Re-training INFO: iter:  2375/300480  CE: 7.2162  
[04/15 12:38:15] Re-training INFO: iter:  2500/300480  CE: 7.3709  
[04/15 12:38:16] Re-training INFO: --> epoch:   2/240  avg CE: 7.1853  lr: 0.2048  
[04/15 12:39:39] Re-training INFO: iter:  2625/300480  CE: 7.3680  
[04/15 12:40:46] Re-training INFO: iter:  2750/300480  CE: 7.2875  
[04/15 12:41:52] Re-training INFO: iter:  2875/300480  CE: 7.3825  
[04/15 12:42:58] Re-training INFO: iter:  3000/300480  CE: 7.3355  
[04/15 12:44:05] Re-training INFO: iter:  3125/300480  CE: 7.5363  
[04/15 12:45:13] Re-training INFO: iter:  3250/300480  CE: 7.4656  
[04/15 12:46:20] Re-training INFO: iter:  3375/300480  CE: 7.4416  
[04/15 12:47:27] Re-training INFO: iter:  3500/300480  CE: 7.3217  
[04/15 12:48:34] Re-training INFO: iter:  3625/300480  CE: 7.5272  
[04/15 12:49:41] Re-training INFO: iter:  3750/300480  CE: 7.3805  
[04/15 12:49:43] Re-training INFO: --> epoch:   3/240  avg CE: 7.3671  lr: 0.30720000000000003  
[04/15 12:51:05] Re-training INFO: iter:  3875/300480  CE: 7.3669  
[04/15 12:52:12] Re-training INFO: iter:  4000/300480  CE: 7.4958  
[04/15 12:53:19] Re-training INFO: iter:  4125/300480  CE: 7.5271  
[04/15 12:54:26] Re-training INFO: iter:  4250/300480  CE: 7.3972  
[04/15 12:55:33] Re-training INFO: iter:  4375/300480  CE: 7.4321  
[04/15 12:56:40] Re-training INFO: iter:  4500/300480  CE: 7.4137  
[04/15 12:57:47] Re-training INFO: iter:  4625/300480  CE: 7.5258  
[04/15 12:58:55] Re-training INFO: iter:  4750/300480  CE: 7.4440  
[04/15 13:00:04] Re-training INFO: iter:  4875/300480  CE: 7.7377  
[04/15 13:01:10] Re-training INFO: iter:  5000/300480  CE: 7.6193  
[04/15 13:01:13] Re-training INFO: --> epoch:   4/240  avg CE: 7.5456  lr: 0.4096  
[04/15 13:02:34] Re-training INFO: iter:  5125/300480  CE: 7.8200  
[04/15 13:03:41] Re-training INFO: iter:  5250/300480  CE: 7.5401  
[04/15 13:04:48] Re-training INFO: iter:  5375/300480  CE: 7.7488  
[04/15 13:05:55] Re-training INFO: iter:  5500/300480  CE: 7.7411  
[04/15 13:07:02] Re-training INFO: iter:  5625/300480  CE: 7.5866  
[04/15 13:08:11] Re-training INFO: iter:  5750/300480  CE: 7.7434  
[04/15 13:09:19] Re-training INFO: iter:  5875/300480  CE: 7.6613  
[04/15 13:10:26] Re-training INFO: iter:  6000/300480  CE: 7.9474  
[04/15 13:11:33] Re-training INFO: iter:  6125/300480  CE: 7.9638  
[04/15 13:12:40] Re-training INFO: iter:  6250/300480  CE: 7.7849  
[04/15 13:12:43] Re-training INFO: --> epoch:   5/240  avg CE: 7.7558  lr: 0.5119182108626198  
[04/15 13:14:03] Re-training INFO: iter:  6375/300480  CE: 8.0882  
[04/15 13:15:10] Re-training INFO: iter:  6500/300480  CE: 7.9664  
[04/15 13:16:19] Re-training INFO: iter:  6625/300480  CE: 7.7517  
[04/15 13:17:26] Re-training INFO: iter:  6750/300480  CE: 7.6968  
[04/15 13:18:35] Re-training INFO: iter:  6875/300480  CE: 7.5838  
[04/15 13:19:43] Re-training INFO: iter:  7000/300480  CE: 7.7737  
[04/15 13:20:51] Re-training INFO: iter:  7125/300480  CE: 7.9200  
[04/15 13:21:58] Re-training INFO: iter:  7250/300480  CE: 7.7272  
[04/15 13:23:05] Re-training INFO: iter:  7375/300480  CE: 7.7578  
[04/15 13:24:13] Re-training INFO: iter:  7500/300480  CE: 7.7860  
[04/15 13:24:18] Re-training INFO: --> epoch:   6/240  avg CE: 7.8508  lr: 0.5119182108626198  
[04/15 13:25:37] Re-training INFO: iter:  7625/300480  CE: 8.0395  
[04/15 13:26:45] Re-training INFO: iter:  7750/300480  CE: 8.0140  
[04/15 13:27:52] Re-training INFO: iter:  7875/300480  CE: 7.9572  
[04/15 13:29:01] Re-training INFO: iter:  8000/300480  CE: 8.1442  
[04/15 13:30:09] Re-training INFO: iter:  8125/300480  CE: 7.5639  
[04/15 13:31:17] Re-training INFO: iter:  8250/300480  CE: 7.6307  
[04/15 13:32:24] Re-training INFO: iter:  8375/300480  CE: 7.9785  
[04/15 13:33:32] Re-training INFO: iter:  8500/300480  CE: 7.7300  
[04/15 13:34:39] Re-training INFO: iter:  8625/300480  CE: 7.8459  
[04/15 13:35:47] Re-training INFO: iter:  8750/300480  CE: 7.7368  
[04/15 13:35:52] Re-training INFO: --> epoch:   7/240  avg CE: 7.8516  lr: 0.5119182108626198  
[04/15 13:37:11] Re-training INFO: iter:  8875/300480  CE: 7.7529  
[04/15 13:38:19] Re-training INFO: iter:  9000/300480  CE: 7.8340  
[04/15 13:39:27] Re-training INFO: iter:  9125/300480  CE: 7.8169  
[04/15 13:40:34] Re-training INFO: iter:  9250/300480  CE: 7.7711  
[04/15 13:41:42] Re-training INFO: iter:  9375/300480  CE: 7.8954  
[04/15 13:42:49] Re-training INFO: iter:  9500/300480  CE: 7.7654  
[04/15 13:43:58] Re-training INFO: iter:  9625/300480  CE: 7.9162  
[04/15 13:45:06] Re-training INFO: iter:  9750/300480  CE: 7.7053  
[04/15 13:46:14] Re-training INFO: iter:  9875/300480  CE: 7.7584  
[04/15 13:47:21] Re-training INFO: iter: 10000/300480  CE: 7.7582  
[04/15 13:47:28] Re-training INFO: --> epoch:   8/240  avg CE: 7.8293  lr: 0.49791062054266094  
[04/15 13:48:44] Re-training INFO: iter: 10125/300480  CE: 7.9040  
[04/15 13:49:52] Re-training INFO: iter: 10250/300480  CE: 7.9359  
[04/15 13:51:00] Re-training INFO: iter: 10375/300480  CE: 7.7917  
[04/15 13:52:06] Re-training INFO: iter: 10500/300480  CE: 7.5125  
[04/15 13:53:14] Re-training INFO: iter: 10625/300480  CE: 7.8047  
[04/15 13:54:22] Re-training INFO: iter: 10750/300480  CE: 7.7813  
[04/15 13:55:30] Re-training INFO: iter: 10875/300480  CE: 7.9016  
[04/15 13:56:38] Re-training INFO: iter: 11000/300480  CE: 7.9418  
[04/15 13:57:46] Re-training INFO: iter: 11125/300480  CE: 7.7009  
[04/15 13:58:55] Re-training INFO: iter: 11250/300480  CE: 7.8058  
[04/15 13:59:02] Re-training INFO: --> epoch:   9/240  avg CE: 7.8269  lr: 0.49791062054266094  
[04/15 14:00:18] Re-training INFO: iter: 11375/300480  CE: 7.8026  
[04/15 14:01:25] Re-training INFO: iter: 11500/300480  CE: 7.7671  
[04/15 14:02:32] Re-training INFO: iter: 11625/300480  CE: 7.7649  
[04/15 14:03:41] Re-training INFO: iter: 11750/300480  CE: 7.7440  
[04/15 14:04:50] Re-training INFO: iter: 11875/300480  CE: 7.7966  
[04/15 14:05:57] Re-training INFO: iter: 12000/300480  CE: 7.9513  
[04/15 14:07:06] Re-training INFO: iter: 12125/300480  CE: 8.0695  
[04/15 14:08:13] Re-training INFO: iter: 12250/300480  CE: 7.8220  
[04/15 14:09:20] Re-training INFO: iter: 12375/300480  CE: 7.9121  
[04/15 14:10:28] Re-training INFO: iter: 12500/300480  CE: 7.7325  
[04/15 14:10:38] Re-training INFO: --> epoch:  10/240  avg CE: 7.8143  lr: 0.48297330192638105  
[04/15 14:11:53] Re-training INFO: iter: 12625/300480  CE: 7.8194  
[04/15 14:13:02] Re-training INFO: iter: 12750/300480  CE: 7.9913  
[04/15 14:14:10] Re-training INFO: iter: 12875/300480  CE: 8.1134  
[04/15 14:15:19] Re-training INFO: iter: 13000/300480  CE: 7.9664  
[04/15 14:16:27] Re-training INFO: iter: 13125/300480  CE: 7.7222  
[04/15 14:17:36] Re-training INFO: iter: 13250/300480  CE: 7.8595  
[04/15 14:18:44] Re-training INFO: iter: 13375/300480  CE: 7.8092  
[04/15 14:19:50] Re-training INFO: iter: 13500/300480  CE: 7.5486  
[04/15 14:20:59] Re-training INFO: iter: 13625/300480  CE: 7.9060  
[04/15 14:22:07] Re-training INFO: iter: 13750/300480  CE: 7.9457  
[04/15 14:22:17] Re-training INFO: --> epoch:  11/240  avg CE: 7.7951  lr: 0.48297330192638105  
[04/15 14:23:32] Re-training INFO: iter: 13875/300480  CE: 7.6852  
[04/15 14:24:40] Re-training INFO: iter: 14000/300480  CE: 7.6637  
[04/15 14:25:48] Re-training INFO: iter: 14125/300480  CE: 7.9706  
[04/15 14:26:56] Re-training INFO: iter: 14250/300480  CE: 8.0487  
[04/15 14:28:05] Re-training INFO: iter: 14375/300480  CE: 7.7927  
[04/15 14:29:14] Re-training INFO: iter: 14500/300480  CE: 7.7612  
[04/15 14:30:21] Re-training INFO: iter: 14625/300480  CE: 7.8585  
[04/15 14:31:29] Re-training INFO: iter: 14750/300480  CE: 7.6381  
[04/15 14:32:36] Re-training INFO: iter: 14875/300480  CE: 7.7885  
[04/15 14:33:45] Re-training INFO: iter: 15000/300480  CE: 7.9655  
[04/15 14:33:56] Re-training INFO: --> epoch:  12/240  avg CE: 7.7998  lr: 0.4684841028685896  
[04/15 14:38:44] Re-training INFO: tag                 : gss-mobile0-test
[04/15 14:38:44] Re-training INFO: arch                : [6, 4, 12, 6, 2, 4, 6, 8, 6, 4, 8, 10, 6, 0, 0, 4, 4, 6, 9, 0, 11]
[04/15 14:38:44] Re-training INFO: seed                : 0
[04/15 14:38:44] Re-training INFO: data_path           : ../../../dataset/ILSVRC2012
[04/15 14:38:44] Re-training INFO: save_path           : ./Evaluation
[04/15 14:38:44] Re-training INFO: search_space        : greedy
[04/15 14:38:44] Re-training INFO: valid_size          : 0
[04/15 14:38:44] Re-training INFO: num_gpus            : 8
[04/15 14:38:44] Re-training INFO: workers             : 4
[04/15 14:38:44] Re-training INFO: interval_ep_eval    : 20
[04/15 14:38:44] Re-training INFO: train_batch_size    : 1024
[04/15 14:38:44] Re-training INFO: test_batch_size     : 256
[04/15 14:38:44] Re-training INFO: max_epoch           : 240
[04/15 14:38:44] Re-training INFO: learning_rate       : 0.512
[04/15 14:38:44] Re-training INFO: momentum            : 0.9
[04/15 14:38:44] Re-training INFO: weight_decay        : 1e-05
[04/15 14:38:44] Re-training INFO: nesterov            : True
[04/15 14:38:44] Re-training INFO: lr_schedule_type    : cosine
[04/15 14:38:44] Re-training INFO: warmup              : True
[04/15 14:38:44] Re-training INFO: drop_out            : 0.2
[04/15 14:38:44] Re-training INFO: label_smooth        : 0.1
[04/15 14:38:44] Re-training INFO: rank                : 0
[04/15 14:38:44] Re-training INFO: gpu                 : 0
[04/15 14:38:44] Re-training INFO: save_name           : Retrain-gss-mobile0-test-seed-0
[04/15 14:38:44] Re-training INFO: log_path            : ./Evaluation/logs/Retrain-gss-mobile0-test-seed-0.txt
[04/15 14:38:44] Re-training INFO: ckpt_path           : ./Evaluation/checkpoint/Retrain-gss-mobile0-test-seed-0.pt
[04/15 14:38:44] Re-training INFO: dist_url            : tcp://127.0.0.1:23456
[04/15 14:38:44] Re-training INFO: world_size          : 8
[04/15 14:38:44] Re-training INFO: distributed         : True
[04/15 14:38:44] Re-training INFO: ['3x3_MBConv3', '3x3_MBConv6', '5x5_MBConv3', '5x5_MBConv6', '7x7_MBConv3', '7x7_MBConv6', '3x3_MBConv3_SE', '3x3_MBConv6_SE', '5x5_MBConv3_SE', '5x5_MBConv6_SE', '7x7_MBConv3_SE', '7x7_MBConv6_SE', 'Identity']
[04/15 14:39:22] Re-training INFO: CNN(
  (first_conv): Sequential(
    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU6(inplace=True)
  )
  (first_block): InvertedResidual(
    (depth_conv): Sequential(
      (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU6(inplace=True)
    )
    (point_linear): Sequential(
      (0): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (blocks): ModuleList(
    (0): InvertedResidual_SE(
      (inverted_bottleneck): Sequential(
        (0): Conv2d(16, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
      )
      (depth_conv): Sequential(
        (0): Conv2d(48, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=48, bias=False)
        (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
      )
      (point_linear): Sequential(
        (0): Conv2d(48, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (se): SqueezeExcite(
        (conv_reduce): Conv2d(48, 16, kernel_size=(1, 1), stride=(1, 1))
        (act1): ReLU6(inplace=True)
        (conv_expand): Conv2d(16, 48, kernel_size=(1, 1), stride=(1, 1))
        (gate): Sigmoid()
      )
    )
    (1): InvertedResidual(
      (inverted_bottleneck): Sequential(
        (0): Conv2d(32, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
      )
      (depth_conv): Sequential(
        (0): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96, bias=False)
        (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
      )
      (point_linear): Sequential(
        (0): Conv2d(96, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (2): Identity()
    (3): InvertedResidual_SE(
      (inverted_bottleneck): Sequential(
        (0): Conv2d(32, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
      )
      (depth_conv): Sequential(
        (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
        (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
      )
      (point_linear): Sequential(
        (0): Conv2d(96, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (se): SqueezeExcite(
        (conv_reduce): Conv2d(96, 32, kernel_size=(1, 1), stride=(1, 1))
        (act1): ReLU6(inplace=True)
        (conv_expand): Conv2d(32, 96, kernel_size=(1, 1), stride=(1, 1))
        (gate): Sigmoid()
      )
    )
    (4): InvertedResidual(
      (inverted_bottleneck): Sequential(
        (0): Conv2d(32, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
      )
      (depth_conv): Sequential(
        (0): Conv2d(96, 96, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=96, bias=False)
        (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
      )
      (point_linear): Sequential(
        (0): Conv2d(96, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (5): InvertedResidual(
      (inverted_bottleneck): Sequential(
        (0): Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
      )
      (depth_conv): Sequential(
        (0): Conv2d(120, 120, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=120, bias=False)
        (1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
      )
      (point_linear): Sequential(
        (0): Conv2d(120, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (6): InvertedResidual_SE(
      (inverted_bottleneck): Sequential(
        (0): Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
      )
      (depth_conv): Sequential(
        (0): Conv2d(120, 120, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=120, bias=False)
        (1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
      )
      (point_linear): Sequential(
        (0): Conv2d(120, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (se): SqueezeExcite(
        (conv_reduce): Conv2d(120, 40, kernel_size=(1, 1), stride=(1, 1))
        (act1): ReLU6(inplace=True)
        (conv_expand): Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1))
        (gate): Sigmoid()
      )
    )
    (7): InvertedResidual_SE(
      (inverted_bottleneck): Sequential(
        (0): Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
      )
      (depth_conv): Sequential(
        (0): Conv2d(120, 120, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=120, bias=False)
        (1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
      )
      (point_linear): Sequential(
        (0): Conv2d(120, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (se): SqueezeExcite(
        (conv_reduce): Conv2d(120, 40, kernel_size=(1, 1), stride=(1, 1))
        (act1): ReLU6(inplace=True)
        (conv_expand): Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1))
        (gate): Sigmoid()
      )
    )
    (8): InvertedResidual_SE(
      (inverted_bottleneck): Sequential(
        (0): Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
      )
      (depth_conv): Sequential(
        (0): Conv2d(120, 120, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=120, bias=False)
        (1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
      )
      (point_linear): Sequential(
        (0): Conv2d(120, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (se): SqueezeExcite(
        (conv_reduce): Conv2d(120, 40, kernel_size=(1, 1), stride=(1, 1))
        (act1): ReLU6(inplace=True)
        (conv_expand): Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1))
        (gate): Sigmoid()
      )
    )
    (9): InvertedResidual(
      (inverted_bottleneck): Sequential(
        (0): Conv2d(80, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
      )
      (depth_conv): Sequential(
        (0): Conv2d(240, 240, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=240, bias=False)
        (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
      )
      (point_linear): Sequential(
        (0): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (10): InvertedResidual_SE(
      (inverted_bottleneck): Sequential(
        (0): Conv2d(80, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
      )
      (depth_conv): Sequential(
        (0): Conv2d(240, 240, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=240, bias=False)
        (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
      )
      (point_linear): Sequential(
        (0): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (se): SqueezeExcite(
        (conv_reduce): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1))
        (act1): ReLU6(inplace=True)
        (conv_expand): Conv2d(80, 240, kernel_size=(1, 1), stride=(1, 1))
        (gate): Sigmoid()
      )
    )
    (11): InvertedResidual_SE(
      (inverted_bottleneck): Sequential(
        (0): Conv2d(80, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
      )
      (depth_conv): Sequential(
        (0): Conv2d(240, 240, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=240, bias=False)
        (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
      )
      (point_linear): Sequential(
        (0): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (se): SqueezeExcite(
        (conv_reduce): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1))
        (act1): ReLU6(inplace=True)
        (conv_expand): Conv2d(80, 240, kernel_size=(1, 1), stride=(1, 1))
        (gate): Sigmoid()
      )
    )
    (12): InvertedResidual_SE(
      (inverted_bottleneck): Sequential(
        (0): Conv2d(80, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
      )
      (depth_conv): Sequential(
        (0): Conv2d(240, 240, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=240, bias=False)
        (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
      )
      (point_linear): Sequential(
        (0): Conv2d(240, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (se): SqueezeExcite(
        (conv_reduce): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1))
        (act1): ReLU6(inplace=True)
        (conv_expand): Conv2d(80, 240, kernel_size=(1, 1), stride=(1, 1))
        (gate): Sigmoid()
      )
    )
    (13): InvertedResidual(
      (inverted_bottleneck): Sequential(
        (0): Conv2d(96, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
      )
      (depth_conv): Sequential(
        (0): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
        (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
      )
      (point_linear): Sequential(
        (0): Conv2d(288, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (14): InvertedResidual(
      (inverted_bottleneck): Sequential(
        (0): Conv2d(96, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
      )
      (depth_conv): Sequential(
        (0): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288, bias=False)
        (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
      )
      (point_linear): Sequential(
        (0): Conv2d(288, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (15): InvertedResidual(
      (inverted_bottleneck): Sequential(
        (0): Conv2d(96, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
      )
      (depth_conv): Sequential(
        (0): Conv2d(288, 288, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=288, bias=False)
        (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
      )
      (point_linear): Sequential(
        (0): Conv2d(288, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (16): InvertedResidual(
      (inverted_bottleneck): Sequential(
        (0): Conv2d(96, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
      )
      (depth_conv): Sequential(
        (0): Conv2d(288, 288, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), groups=288, bias=False)
        (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
      )
      (point_linear): Sequential(
        (0): Conv2d(288, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (17): InvertedResidual_SE(
      (inverted_bottleneck): Sequential(
        (0): Conv2d(192, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
      )
      (depth_conv): Sequential(
        (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)
        (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
      )
      (point_linear): Sequential(
        (0): Conv2d(576, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (se): SqueezeExcite(
        (conv_reduce): Conv2d(576, 192, kernel_size=(1, 1), stride=(1, 1))
        (act1): ReLU6(inplace=True)
        (conv_expand): Conv2d(192, 576, kernel_size=(1, 1), stride=(1, 1))
        (gate): Sigmoid()
      )
    )
    (18): InvertedResidual_SE(
      (inverted_bottleneck): Sequential(
        (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
      )
      (depth_conv): Sequential(
        (0): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)
        (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
      )
      (point_linear): Sequential(
        (0): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (se): SqueezeExcite(
        (conv_reduce): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1))
        (act1): ReLU6(inplace=True)
        (conv_expand): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1))
        (gate): Sigmoid()
      )
    )
    (19): InvertedResidual(
      (inverted_bottleneck): Sequential(
        (0): Conv2d(192, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
      )
      (depth_conv): Sequential(
        (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)
        (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
      )
      (point_linear): Sequential(
        (0): Conv2d(576, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (20): InvertedResidual_SE(
      (inverted_bottleneck): Sequential(
        (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
      )
      (depth_conv): Sequential(
        (0): Conv2d(1152, 1152, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1152, bias=False)
        (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
      )
      (point_linear): Sequential(
        (0): Conv2d(1152, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (se): SqueezeExcite(
        (conv_reduce): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1))
        (act1): ReLU6(inplace=True)
        (conv_expand): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1))
        (gate): Sigmoid()
      )
    )
  )
  (feature_mix_layer): Sequential(
    (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (1): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU6(inplace=True)
  )
  (avgpool): AvgPool2d(kernel_size=7, stride=7, padding=0)
  (classifier): Sequential(
    (0): Dropout(p=0.2, inplace=False)
    (1): Linear(in_features=1280, out_features=1000, bias=True)
  )
)
[04/15 14:39:22] Re-training INFO: # of Params : 5.113
[04/15 14:39:34] Re-training INFO: Trainset Size: 1281167
[04/15 14:39:34] Re-training INFO: Validset Size:   50000
[04/15 14:39:34] Re-training INFO: Compose(
    RandomResizedCrop(size=(224, 224), scale=(0.08, 1.0), ratio=(0.75, 1.3333), interpolation=bilinear)
    RandomHorizontalFlip(p=0.5)
    ToTensor()
    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
)
[04/15 14:39:34] Re-training INFO: SGD (
Parameter Group 0
    dampening: 0
    initial_lr: 0.512
    lr: 0.512
    momentum: 0.9
    nesterov: True
    weight_decay: 1e-05

Parameter Group 1
    dampening: 0
    initial_lr: 0.512
    lr: 0.512
    momentum: 0.9
    nesterov: True
    weight_decay: 0.0
)
[04/15 14:39:34] Re-training INFO: --> START Retrain-gss-mobile0-test-seed-0
[04/15 14:42:06] Re-training INFO: iter:   125/300480  CE: 6.9218  
[04/15 14:43:09] Re-training INFO: iter:   250/300480  CE: 6.8352  
[04/15 14:44:13] Re-training INFO: iter:   375/300480  CE: 6.6068  
[04/15 14:45:16] Re-training INFO: iter:   500/300480  CE: 6.4125  
[04/15 14:46:21] Re-training INFO: iter:   625/300480  CE: 6.1046  
[04/15 14:47:24] Re-training INFO: iter:   750/300480  CE: 5.6783  
[04/15 14:48:28] Re-training INFO: iter:   875/300480  CE: 5.7975  
[04/15 14:49:31] Re-training INFO: iter:  1000/300480  CE: 5.5437  
[04/15 14:50:34] Re-training INFO: iter:  1125/300480  CE: 5.0362  
[04/15 14:51:36] Re-training INFO: iter:  1250/300480  CE: 5.3889  
[04/15 14:51:37] Re-training INFO: --> epoch:   1/240  avg CE: 6.1274  lr: 0.1024  
[04/15 14:52:59] Re-training INFO: iter:  1375/300480  CE: 5.1676  
[04/15 14:54:02] Re-training INFO: iter:  1500/300480  CE: 4.8502  
[04/15 14:55:06] Re-training INFO: iter:  1625/300480  CE: 4.7588  
[04/15 14:56:10] Re-training INFO: iter:  1750/300480  CE: 5.0868  
[04/15 14:57:15] Re-training INFO: iter:  1875/300480  CE: 4.4071  
[04/15 14:58:19] Re-training INFO: iter:  2000/300480  CE: 4.6245  
[04/15 14:59:23] Re-training INFO: iter:  2125/300480  CE: 4.4711  
[04/15 15:00:28] Re-training INFO: iter:  2250/300480  CE: 4.5503  
[04/15 15:01:31] Re-training INFO: iter:  2375/300480  CE: 4.3475  
[04/15 15:02:34] Re-training INFO: iter:  2500/300480  CE: 3.9964  
[04/15 15:02:35] Re-training INFO: --> epoch:   2/240  avg CE: 4.6314  lr: 0.2048  
[04/15 15:03:55] Re-training INFO: iter:  2625/300480  CE: 4.0781  
[04/15 15:05:00] Re-training INFO: iter:  2750/300480  CE: 3.7824  
[04/15 15:06:04] Re-training INFO: iter:  2875/300480  CE: 3.9797  
[04/15 15:07:08] Re-training INFO: iter:  3000/300480  CE: 4.1802  
[04/15 15:08:13] Re-training INFO: iter:  3125/300480  CE: 4.1218  
[04/15 15:09:16] Re-training INFO: iter:  3250/300480  CE: 3.7842  
[04/15 15:10:20] Re-training INFO: iter:  3375/300480  CE: 3.8399  
[04/15 15:11:24] Re-training INFO: iter:  3500/300480  CE: 3.8150  
[04/15 15:12:28] Re-training INFO: iter:  3625/300480  CE: 3.8398  
[04/15 15:13:31] Re-training INFO: iter:  3750/300480  CE: 4.0387  
[04/15 15:13:32] Re-training INFO: --> epoch:   3/240  avg CE: 3.9835  lr: 0.30720000000000003  
[04/15 15:14:51] Re-training INFO: iter:  3875/300480  CE: 3.7582  
[04/15 15:15:56] Re-training INFO: iter:  4000/300480  CE: 3.6582  
[04/15 15:17:02] Re-training INFO: iter:  4125/300480  CE: 3.9032  
[04/15 15:18:06] Re-training INFO: iter:  4250/300480  CE: 3.6582  
[04/15 15:19:09] Re-training INFO: iter:  4375/300480  CE: 3.6222  
[04/15 15:20:13] Re-training INFO: iter:  4500/300480  CE: 3.7534  
[04/15 15:21:17] Re-training INFO: iter:  4625/300480  CE: 3.3152  
[04/15 15:22:20] Re-training INFO: iter:  4750/300480  CE: 3.7758  
[04/15 15:23:24] Re-training INFO: iter:  4875/300480  CE: 3.6601  
[04/15 15:24:28] Re-training INFO: iter:  5000/300480  CE: 3.5645  
[04/15 15:24:31] Re-training INFO: --> epoch:   4/240  avg CE: 3.6527  lr: 0.4096  
[04/15 15:25:48] Re-training INFO: iter:  5125/300480  CE: 3.4869  
[04/15 15:26:52] Re-training INFO: iter:  5250/300480  CE: 3.0856  
[04/15 15:27:56] Re-training INFO: iter:  5375/300480  CE: 3.3448  
[04/15 15:29:00] Re-training INFO: iter:  5500/300480  CE: 3.4284  
[04/15 15:30:04] Re-training INFO: iter:  5625/300480  CE: 3.3491  
[04/15 15:31:08] Re-training INFO: iter:  5750/300480  CE: 3.2317  
[04/15 15:32:12] Re-training INFO: iter:  5875/300480  CE: 3.4272  
[04/15 15:33:16] Re-training INFO: iter:  6000/300480  CE: 3.2580  
[04/15 15:34:20] Re-training INFO: iter:  6125/300480  CE: 3.3386  
[04/15 15:35:23] Re-training INFO: iter:  6250/300480  CE: 3.3386  
[04/15 15:35:26] Re-training INFO: --> epoch:   5/240  avg CE: 3.4560  lr: 0.5119182108626198  
[04/15 15:36:43] Re-training INFO: iter:  6375/300480  CE: 3.3511  
[04/15 15:37:46] Re-training INFO: iter:  6500/300480  CE: 2.9569  
[04/15 15:38:50] Re-training INFO: iter:  6625/300480  CE: 3.0064  
[04/15 15:39:54] Re-training INFO: iter:  6750/300480  CE: 3.1570  
[04/15 15:40:58] Re-training INFO: iter:  6875/300480  CE: 3.2644  
[04/15 15:42:02] Re-training INFO: iter:  7000/300480  CE: 3.2967  
[04/15 15:43:05] Re-training INFO: iter:  7125/300480  CE: 3.2828  
[04/15 15:44:09] Re-training INFO: iter:  7250/300480  CE: 3.1223  
[04/15 15:45:14] Re-training INFO: iter:  7375/300480  CE: 3.4216  
[04/15 15:46:18] Re-training INFO: iter:  7500/300480  CE: 3.0227  
[04/15 15:46:22] Re-training INFO: --> epoch:   6/240  avg CE: 3.2977  lr: 0.5119182108626198  
[04/15 15:47:39] Re-training INFO: iter:  7625/300480  CE: 3.0968  
[04/15 15:48:43] Re-training INFO: iter:  7750/300480  CE: 2.9337  
[04/15 15:49:47] Re-training INFO: iter:  7875/300480  CE: 3.3665  
[04/15 15:50:51] Re-training INFO: iter:  8000/300480  CE: 2.9754  
[04/15 15:51:54] Re-training INFO: iter:  8125/300480  CE: 3.1842  
[04/15 15:52:59] Re-training INFO: iter:  8250/300480  CE: 3.0830  
[04/15 15:54:03] Re-training INFO: iter:  8375/300480  CE: 3.1706  
[04/15 15:55:07] Re-training INFO: iter:  8500/300480  CE: 3.0422  
[04/15 15:56:11] Re-training INFO: iter:  8625/300480  CE: 3.1532  
[04/15 15:57:15] Re-training INFO: iter:  8750/300480  CE: 3.2148  
[04/15 15:57:20] Re-training INFO: --> epoch:   7/240  avg CE: 3.1638  lr: 0.5119182108626198  
[04/15 15:58:36] Re-training INFO: iter:  8875/300480  CE: 2.9311  
[04/15 15:59:40] Re-training INFO: iter:  9000/300480  CE: 3.0354  
[04/15 16:00:44] Re-training INFO: iter:  9125/300480  CE: 2.9542  
[04/15 16:01:49] Re-training INFO: iter:  9250/300480  CE: 2.9580  
[04/15 16:02:52] Re-training INFO: iter:  9375/300480  CE: 2.8429  
[04/15 16:03:57] Re-training INFO: iter:  9500/300480  CE: 3.1010  
[04/15 16:05:00] Re-training INFO: iter:  9625/300480  CE: 3.0132  
[04/15 16:06:04] Re-training INFO: iter:  9750/300480  CE: 2.9877  
[04/15 16:07:08] Re-training INFO: iter:  9875/300480  CE: 3.0204  
[04/15 16:08:12] Re-training INFO: iter: 10000/300480  CE: 2.9516  
[04/15 16:08:18] Re-training INFO: --> epoch:   8/240  avg CE: 3.0575  lr: 0.49791062054266094  
[04/15 16:09:32] Re-training INFO: iter: 10125/300480  CE: 2.6742  
[04/15 16:10:37] Re-training INFO: iter: 10250/300480  CE: 2.8191  
[04/15 16:11:41] Re-training INFO: iter: 10375/300480  CE: 2.7700  
[04/15 16:12:46] Re-training INFO: iter: 10500/300480  CE: 2.8094  
[04/15 16:13:51] Re-training INFO: iter: 10625/300480  CE: 2.8906  
[04/15 16:14:55] Re-training INFO: iter: 10750/300480  CE: 3.0197  
[04/15 16:16:00] Re-training INFO: iter: 10875/300480  CE: 2.9645  
[04/15 16:17:04] Re-training INFO: iter: 11000/300480  CE: 3.0335  
[04/15 16:18:09] Re-training INFO: iter: 11125/300480  CE: 2.7858  
[04/15 16:19:13] Re-training INFO: iter: 11250/300480  CE: 2.7129  
[04/15 16:19:20] Re-training INFO: --> epoch:   9/240  avg CE: 2.9977  lr: 0.49791062054266094  
[04/15 16:20:34] Re-training INFO: iter: 11375/300480  CE: 2.7240  
[04/15 16:21:38] Re-training INFO: iter: 11500/300480  CE: 2.7348  
[04/15 16:22:42] Re-training INFO: iter: 11625/300480  CE: 3.0673  
[04/15 16:23:47] Re-training INFO: iter: 11750/300480  CE: 2.8924  
[04/15 16:24:51] Re-training INFO: iter: 11875/300480  CE: 3.0730  
[04/15 16:25:55] Re-training INFO: iter: 12000/300480  CE: 3.1816  
[04/15 16:27:00] Re-training INFO: iter: 12125/300480  CE: 3.0653  
[04/15 16:28:03] Re-training INFO: iter: 12250/300480  CE: 2.7761  
[04/15 16:29:08] Re-training INFO: iter: 12375/300480  CE: 2.8287  
[04/15 16:30:13] Re-training INFO: iter: 12500/300480  CE: 3.2531  
[04/15 16:30:22] Re-training INFO: --> epoch:  10/240  avg CE: 2.9399  lr: 0.48297330192638105  
[04/15 16:31:34] Re-training INFO: iter: 12625/300480  CE: 2.5848  
[04/15 16:32:38] Re-training INFO: iter: 12750/300480  CE: 3.0319  
[04/15 16:33:42] Re-training INFO: iter: 12875/300480  CE: 2.7494  
[04/15 16:34:48] Re-training INFO: iter: 13000/300480  CE: 2.8992  
[04/15 16:35:52] Re-training INFO: iter: 13125/300480  CE: 3.1603  
[04/15 16:36:56] Re-training INFO: iter: 13250/300480  CE: 2.8294  
[04/15 16:38:01] Re-training INFO: iter: 13375/300480  CE: 2.9411  
[04/15 16:39:06] Re-training INFO: iter: 13500/300480  CE: 2.9471  
[04/15 16:40:11] Re-training INFO: iter: 13625/300480  CE: 2.9639  
[04/15 16:41:15] Re-training INFO: iter: 13750/300480  CE: 2.9513  
[04/15 16:41:24] Re-training INFO: --> epoch:  11/240  avg CE: 2.8904  lr: 0.48297330192638105  
[04/15 16:42:36] Re-training INFO: iter: 13875/300480  CE: 2.7989  
[04/15 16:43:40] Re-training INFO: iter: 14000/300480  CE: 3.1447  
[04/15 16:44:44] Re-training INFO: iter: 14125/300480  CE: 2.8033  
[04/15 16:45:50] Re-training INFO: iter: 14250/300480  CE: 2.8108  
[04/15 16:46:54] Re-training INFO: iter: 14375/300480  CE: 3.1729  
[04/15 16:48:00] Re-training INFO: iter: 14500/300480  CE: 2.6547  
[04/15 16:49:05] Re-training INFO: iter: 14625/300480  CE: 2.9504  
[04/15 16:50:09] Re-training INFO: iter: 14750/300480  CE: 2.9504  
[04/15 16:51:14] Re-training INFO: iter: 14875/300480  CE: 2.7623  
[04/15 16:52:19] Re-training INFO: iter: 15000/300480  CE: 2.9000  
[04/15 16:52:30] Re-training INFO: --> epoch:  12/240  avg CE: 2.8454  lr: 0.4684841028685896  
[04/15 16:53:40] Re-training INFO: iter: 15125/300480  CE: 2.9028  
[04/15 16:54:44] Re-training INFO: iter: 15250/300480  CE: 3.0535  
[04/15 16:55:49] Re-training INFO: iter: 15375/300480  CE: 2.5411  
[04/15 16:56:54] Re-training INFO: iter: 15500/300480  CE: 2.9408  
[04/15 16:57:59] Re-training INFO: iter: 15625/300480  CE: 2.7657  
[04/15 16:59:04] Re-training INFO: iter: 15750/300480  CE: 2.8782  
[04/15 17:00:09] Re-training INFO: iter: 15875/300480  CE: 2.6626  
[04/15 17:01:12] Re-training INFO: iter: 16000/300480  CE: 2.8278  
[04/15 17:02:16] Re-training INFO: iter: 16125/300480  CE: 2.8580  
[04/15 17:03:20] Re-training INFO: iter: 16250/300480  CE: 2.8656  
[04/15 17:03:32] Re-training INFO: --> epoch:  13/240  avg CE: 2.8167  lr: 0.4684841028685896  
[04/15 17:04:41] Re-training INFO: iter: 16375/300480  CE: 2.9469  
[04/15 17:05:46] Re-training INFO: iter: 16500/300480  CE: 2.7788  
[04/15 17:06:50] Re-training INFO: iter: 16625/300480  CE: 2.8479  
[04/15 17:07:54] Re-training INFO: iter: 16750/300480  CE: 2.7029  
[04/15 17:08:58] Re-training INFO: iter: 16875/300480  CE: 2.9472  
[04/15 17:10:03] Re-training INFO: iter: 17000/300480  CE: 2.6995  
[04/15 17:11:07] Re-training INFO: iter: 17125/300480  CE: 3.0886  
[04/15 17:12:12] Re-training INFO: iter: 17250/300480  CE: 2.7188  
[04/15 17:13:16] Re-training INFO: iter: 17375/300480  CE: 2.7048  
[04/15 17:14:21] Re-training INFO: iter: 17500/300480  CE: 2.7350  
[04/15 17:14:34] Re-training INFO: --> epoch:  14/240  avg CE: 2.7824  lr: 0.4684841028685896  
[04/15 17:15:42] Re-training INFO: iter: 17625/300480  CE: 2.5429  
[04/15 17:16:47] Re-training INFO: iter: 17750/300480  CE: 2.9120  
[04/15 17:17:52] Re-training INFO: iter: 17875/300480  CE: 2.8681  
[04/15 17:18:58] Re-training INFO: iter: 18000/300480  CE: 2.9604  
[04/15 17:20:02] Re-training INFO: iter: 18125/300480  CE: 2.6442  
[04/15 17:21:07] Re-training INFO: iter: 18250/300480  CE: 2.7503  
[04/15 17:22:11] Re-training INFO: iter: 18375/300480  CE: 2.6801  
[04/15 17:23:15] Re-training INFO: iter: 18500/300480  CE: 3.0084  
[04/15 17:24:20] Re-training INFO: iter: 18625/300480  CE: 2.8126  
[04/15 17:25:24] Re-training INFO: iter: 18750/300480  CE: 2.9104  
[04/15 17:25:39] Re-training INFO: --> epoch:  15/240  avg CE: 2.7654  lr: 0.45442957978253196  
[04/15 17:26:46] Re-training INFO: iter: 18875/300480  CE: 2.5883  
[04/15 17:27:51] Re-training INFO: iter: 19000/300480  CE: 2.6575  
[04/15 17:28:55] Re-training INFO: iter: 19125/300480  CE: 2.6993  
[04/15 17:29:59] Re-training INFO: iter: 19250/300480  CE: 2.6459  
[04/15 17:31:04] Re-training INFO: iter: 19375/300480  CE: 2.6957  
[04/15 17:32:09] Re-training INFO: iter: 19500/300480  CE: 2.6126  
[04/15 17:33:14] Re-training INFO: iter: 19625/300480  CE: 2.6335  
[04/15 17:34:19] Re-training INFO: iter: 19750/300480  CE: 2.7116  
[04/15 17:35:24] Re-training INFO: iter: 19875/300480  CE: 2.7345  
[04/15 17:36:28] Re-training INFO: iter: 20000/300480  CE: 2.5739  
[04/15 17:36:43] Re-training INFO: --> epoch:  16/240  avg CE: 2.7355  lr: 0.45442957978253196  
[04/15 17:37:50] Re-training INFO: iter: 20125/300480  CE: 2.8291  
[04/15 17:38:53] Re-training INFO: iter: 20250/300480  CE: 2.8390  
[04/15 17:39:57] Re-training INFO: iter: 20375/300480  CE: 2.5357  
[04/15 17:41:01] Re-training INFO: iter: 20500/300480  CE: 2.6480  
[04/15 17:42:05] Re-training INFO: iter: 20625/300480  CE: 2.7125  
[04/15 17:43:09] Re-training INFO: iter: 20750/300480  CE: 2.6111  
[04/15 17:44:12] Re-training INFO: iter: 20875/300480  CE: 2.6078  
[04/15 17:45:17] Re-training INFO: iter: 21000/300480  CE: 2.7881  
[04/15 17:46:21] Re-training INFO: iter: 21125/300480  CE: 2.8807  
[04/15 17:47:25] Re-training INFO: iter: 21250/300480  CE: 2.5958  
[04/15 17:47:41] Re-training INFO: --> epoch:  17/240  avg CE: 2.7170  lr: 0.440796692389056  
[04/15 17:48:46] Re-training INFO: iter: 21375/300480  CE: 2.6002  
[04/15 17:49:51] Re-training INFO: iter: 21500/300480  CE: 2.7279  
[04/15 17:50:56] Re-training INFO: iter: 21625/300480  CE: 2.7701  
[04/15 17:52:00] Re-training INFO: iter: 21750/300480  CE: 2.5948  
[04/15 17:53:05] Re-training INFO: iter: 21875/300480  CE: 2.7678  
[04/15 17:54:10] Re-training INFO: iter: 22000/300480  CE: 2.6879  
[04/15 17:55:15] Re-training INFO: iter: 22125/300480  CE: 2.5973  
[04/15 17:56:19] Re-training INFO: iter: 22250/300480  CE: 2.4356  
[04/15 17:57:23] Re-training INFO: iter: 22375/300480  CE: 2.6783  
[04/15 17:58:28] Re-training INFO: iter: 22500/300480  CE: 2.6837  
[04/15 17:58:44] Re-training INFO: --> epoch:  18/240  avg CE: 2.6906  lr: 0.440796692389056  
[04/15 17:59:49] Re-training INFO: iter: 22625/300480  CE: 2.7448  
[04/15 18:00:54] Re-training INFO: iter: 22750/300480  CE: 2.7020  
[04/15 18:01:57] Re-training INFO: iter: 22875/300480  CE: 2.7573  
[04/15 18:03:02] Re-training INFO: iter: 23000/300480  CE: 2.5852  
[04/15 18:04:05] Re-training INFO: iter: 23125/300480  CE: 2.8046  
[04/15 18:05:11] Re-training INFO: iter: 23250/300480  CE: 2.8139  
[04/15 18:06:14] Re-training INFO: iter: 23375/300480  CE: 2.9376  
[04/15 18:07:17] Re-training INFO: iter: 23500/300480  CE: 2.9380  
[04/15 18:08:22] Re-training INFO: iter: 23625/300480  CE: 2.8093  
[04/15 18:09:27] Re-training INFO: iter: 23750/300480  CE: 2.8447  
[04/15 18:09:45] Re-training INFO: --> epoch:  19/240  avg CE: 2.6742  lr: 0.440796692389056  
[04/15 18:10:48] Re-training INFO: iter: 23875/300480  CE: 2.5808  
[04/15 18:11:53] Re-training INFO: iter: 24000/300480  CE: 2.4821  
[04/15 18:12:58] Re-training INFO: iter: 24125/300480  CE: 2.5692  
[04/15 18:14:03] Re-training INFO: iter: 24250/300480  CE: 2.4976  
[04/15 18:15:07] Re-training INFO: iter: 24375/300480  CE: 2.6618  
[04/15 18:16:11] Re-training INFO: iter: 24500/300480  CE: 2.7305  
[04/15 18:17:16] Re-training INFO: iter: 24625/300480  CE: 2.6354  
[04/15 18:18:22] Re-training INFO: iter: 24750/300480  CE: 2.7685  
[04/15 18:19:27] Re-training INFO: iter: 24875/300480  CE: 2.4487  
[04/15 18:20:32] Re-training INFO: iter: 25000/300480  CE: 2.4940  
[04/15 18:20:51] Re-training INFO: --> epoch:  20/240  avg CE: 2.6566  lr: 0.42757279161738426  
[04/15 18:21:26] Re-training INFO: # of Test Samples: 50000.0
[04/15 18:21:26] Re-training INFO: Top-1/-5 acc: 61.89 / 84.29
[04/15 18:21:26] Re-training INFO: Top-1/-5 acc: 38.11 / 15.71
[04/15 18:21:26] Re-training INFO: 

[04/15 18:22:27] Re-training INFO: iter: 25125/300480  CE: 2.5939  
[04/15 18:23:32] Re-training INFO: iter: 25250/300480  CE: 2.5079  
[04/15 18:24:35] Re-training INFO: iter: 25375/300480  CE: 2.2552  
[04/15 18:25:38] Re-training INFO: iter: 25500/300480  CE: 2.5332  
[04/15 18:26:42] Re-training INFO: iter: 25625/300480  CE: 2.5268  
[04/15 18:27:45] Re-training INFO: iter: 25750/300480  CE: 2.6463  
[04/15 18:28:50] Re-training INFO: iter: 25875/300480  CE: 2.7431  
[04/15 18:29:54] Re-training INFO: iter: 26000/300480  CE: 2.8313  
[04/15 18:30:58] Re-training INFO: iter: 26125/300480  CE: 2.5969  
[04/15 18:32:01] Re-training INFO: iter: 26250/300480  CE: 2.5119  
[04/15 18:32:21] Re-training INFO: --> epoch:  21/240  avg CE: 2.6506  lr: 0.42757279161738426  
[04/15 18:33:21] Re-training INFO: iter: 26375/300480  CE: 2.5515  
[04/15 18:34:26] Re-training INFO: iter: 26500/300480  CE: 2.5945  
[04/15 18:35:31] Re-training INFO: iter: 26625/300480  CE: 2.7702  
[04/15 18:36:36] Re-training INFO: iter: 26750/300480  CE: 2.7572  
[04/15 18:37:40] Re-training INFO: iter: 26875/300480  CE: 2.8783  
[04/15 18:38:45] Re-training INFO: iter: 27000/300480  CE: 2.6118  
[04/15 18:39:48] Re-training INFO: iter: 27125/300480  CE: 2.6573  
[04/15 18:40:52] Re-training INFO: iter: 27250/300480  CE: 2.5188  
[04/15 18:41:56] Re-training INFO: iter: 27375/300480  CE: 2.5872  
[04/15 18:43:00] Re-training INFO: iter: 27500/300480  CE: 2.7310  
[04/15 18:43:21] Re-training INFO: --> epoch:  22/240  avg CE: 2.6335  lr: 0.4147456078688628  
[04/15 18:44:20] Re-training INFO: iter: 27625/300480  CE: 2.5417  
[04/15 18:45:25] Re-training INFO: iter: 27750/300480  CE: 2.3435  
[04/15 18:46:29] Re-training INFO: iter: 27875/300480  CE: 2.8396  
[04/15 18:47:32] Re-training INFO: iter: 28000/300480  CE: 2.4097  
[04/15 18:48:36] Re-training INFO: iter: 28125/300480  CE: 2.6576  
[04/15 18:49:38] Re-training INFO: iter: 28250/300480  CE: 2.5261  
[04/15 18:50:42] Re-training INFO: iter: 28375/300480  CE: 2.6394  
[04/15 18:51:45] Re-training INFO: iter: 28500/300480  CE: 2.5513  
[04/15 18:52:48] Re-training INFO: iter: 28625/300480  CE: 2.6283  
[04/15 18:53:50] Re-training INFO: iter: 28750/300480  CE: 2.5985  
[04/15 18:54:12] Re-training INFO: --> epoch:  23/240  avg CE: 2.6186  lr: 0.4147456078688628  
[04/15 18:55:10] Re-training INFO: iter: 28875/300480  CE: 2.8687  
[04/15 18:56:13] Re-training INFO: iter: 29000/300480  CE: 2.7617  
[04/15 18:57:18] Re-training INFO: iter: 29125/300480  CE: 2.4239  
[04/15 18:58:21] Re-training INFO: iter: 29250/300480  CE: 2.4018  
[04/15 18:59:23] Re-training INFO: iter: 29375/300480  CE: 2.6141  
[04/15 19:00:25] Re-training INFO: iter: 29500/300480  CE: 2.7133  
[04/15 19:01:26] Re-training INFO: iter: 29625/300480  CE: 2.5535  
[04/15 19:02:27] Re-training INFO: iter: 29750/300480  CE: 2.6929  
[04/15 19:03:30] Re-training INFO: iter: 29875/300480  CE: 2.3918  
[04/15 19:04:30] Re-training INFO: iter: 30000/300480  CE: 2.6689  
[04/15 19:04:53] Re-training INFO: --> epoch:  24/240  avg CE: 2.6027  lr: 0.40230323963279685  
[04/15 19:05:50] Re-training INFO: iter: 30125/300480  CE: 2.6093  
[04/15 19:06:54] Re-training INFO: iter: 30250/300480  CE: 2.5712  
[04/15 19:07:57] Re-training INFO: iter: 30375/300480  CE: 2.4520  
[04/15 19:09:00] Re-training INFO: iter: 30500/300480  CE: 2.8221  
[04/15 19:10:02] Re-training INFO: iter: 30625/300480  CE: 2.4685  
[04/15 19:11:06] Re-training INFO: iter: 30750/300480  CE: 2.6634  
[04/15 19:12:09] Re-training INFO: iter: 30875/300480  CE: 2.4823  
[04/15 19:13:13] Re-training INFO: iter: 31000/300480  CE: 2.8215  
[04/15 19:14:17] Re-training INFO: iter: 31125/300480  CE: 2.4703  
[04/15 19:15:20] Re-training INFO: iter: 31250/300480  CE: 2.7422  
[04/15 19:15:44] Re-training INFO: --> epoch:  25/240  avg CE: 2.5926  lr: 0.40230323963279685  
[04/15 19:16:40] Re-training INFO: iter: 31375/300480  CE: 2.5530  
[04/15 19:17:45] Re-training INFO: iter: 31500/300480  CE: 2.5846  
[04/15 19:18:49] Re-training INFO: iter: 31625/300480  CE: 2.4071  
[04/15 19:19:52] Re-training INFO: iter: 31750/300480  CE: 2.7004  
[04/15 19:20:56] Re-training INFO: iter: 31875/300480  CE: 2.5917  
[04/15 19:21:59] Re-training INFO: iter: 32000/300480  CE: 2.5800  
[04/15 19:23:03] Re-training INFO: iter: 32125/300480  CE: 2.9460  
[04/15 19:24:06] Re-training INFO: iter: 32250/300480  CE: 2.5896  
[04/15 19:25:09] Re-training INFO: iter: 32375/300480  CE: 2.2828  
[04/15 19:26:12] Re-training INFO: iter: 32500/300480  CE: 2.2750  
[04/15 19:26:36] Re-training INFO: --> epoch:  26/240  avg CE: 2.5866  lr: 0.40230323963279685  
[04/15 19:27:31] Re-training INFO: iter: 32625/300480  CE: 2.5746  
[04/15 19:28:33] Re-training INFO: iter: 32750/300480  CE: 2.6125  
[04/15 19:29:35] Re-training INFO: iter: 32875/300480  CE: 2.5688  
[04/15 19:30:38] Re-training INFO: iter: 33000/300480  CE: 2.4468  
[04/15 19:31:40] Re-training INFO: iter: 33125/300480  CE: 2.5390  
[04/15 19:32:45] Re-training INFO: iter: 33250/300480  CE: 2.7844  
[04/15 19:33:47] Re-training INFO: iter: 33375/300480  CE: 2.6034  
[04/15 19:34:50] Re-training INFO: iter: 33500/300480  CE: 2.4945  
[04/15 19:35:52] Re-training INFO: iter: 33625/300480  CE: 2.5359  
[04/15 19:36:54] Re-training INFO: iter: 33750/300480  CE: 2.4708  
[04/15 19:37:20] Re-training INFO: --> epoch:  27/240  avg CE: 2.5655  lr: 0.39023414244381294  
[04/15 19:38:13] Re-training INFO: iter: 33875/300480  CE: 2.6443  
[04/15 19:39:16] Re-training INFO: iter: 34000/300480  CE: 2.8578  
[04/15 19:40:19] Re-training INFO: iter: 34125/300480  CE: 2.6833  
[04/15 19:41:22] Re-training INFO: iter: 34250/300480  CE: 2.4442  
[04/15 19:42:25] Re-training INFO: iter: 34375/300480  CE: 2.4248  
[04/15 19:43:27] Re-training INFO: iter: 34500/300480  CE: 2.5093  
[04/15 19:44:28] Re-training INFO: iter: 34625/300480  CE: 2.4436  
[04/15 19:45:31] Re-training INFO: iter: 34750/300480  CE: 2.8152  
[04/15 19:46:32] Re-training INFO: iter: 34875/300480  CE: 2.6432  
[04/15 19:47:34] Re-training INFO: iter: 35000/300480  CE: 2.4468  
[04/15 19:48:00] Re-training INFO: --> epoch:  28/240  avg CE: 2.5584  lr: 0.39023414244381294  
[04/15 19:48:53] Re-training INFO: iter: 35125/300480  CE: 2.6668  
[04/15 19:49:55] Re-training INFO: iter: 35250/300480  CE: 2.7582  
[04/15 19:50:56] Re-training INFO: iter: 35375/300480  CE: 2.4202  
[04/15 19:51:59] Re-training INFO: iter: 35500/300480  CE: 2.5317  
[04/15 19:53:01] Re-training INFO: iter: 35625/300480  CE: 2.5551  
[04/15 19:54:03] Re-training INFO: iter: 35750/300480  CE: 2.5254  
[04/15 19:55:05] Re-training INFO: iter: 35875/300480  CE: 2.2286  
[04/15 19:56:07] Re-training INFO: iter: 36000/300480  CE: 2.5386  
[04/15 19:57:09] Re-training INFO: iter: 36125/300480  CE: 2.4857  
[04/15 19:58:09] Re-training INFO: iter: 36250/300480  CE: 2.4822  
[04/15 19:58:36] Re-training INFO: --> epoch:  29/240  avg CE: 2.5507  lr: 0.3785271181704985  
[04/15 19:59:28] Re-training INFO: iter: 36375/300480  CE: 2.3466  
[04/15 20:00:31] Re-training INFO: iter: 36500/300480  CE: 2.6568  
[04/15 20:01:34] Re-training INFO: iter: 36625/300480  CE: 2.3338  
[04/15 20:02:37] Re-training INFO: iter: 36750/300480  CE: 2.6623  
[04/15 20:03:40] Re-training INFO: iter: 36875/300480  CE: 2.5354  
[04/15 20:04:43] Re-training INFO: iter: 37000/300480  CE: 2.3213  
[04/15 20:05:46] Re-training INFO: iter: 37125/300480  CE: 2.4109  
[04/15 20:06:47] Re-training INFO: iter: 37250/300480  CE: 2.6968  
[04/15 20:07:48] Re-training INFO: iter: 37375/300480  CE: 2.6180  
[04/15 20:08:49] Re-training INFO: iter: 37500/300480  CE: 2.4498  
[04/15 20:09:16] Re-training INFO: --> epoch:  30/240  avg CE: 2.5383  lr: 0.3785271181704985  
[04/15 20:10:07] Re-training INFO: iter: 37625/300480  CE: 2.4013  
[04/15 20:11:09] Re-training INFO: iter: 37750/300480  CE: 2.7374  
[04/15 20:12:12] Re-training INFO: iter: 37875/300480  CE: 2.5808  
[04/15 20:13:14] Re-training INFO: iter: 38000/300480  CE: 2.4361  
[04/15 20:14:16] Re-training INFO: iter: 38125/300480  CE: 2.5083  
[04/15 20:15:18] Re-training INFO: iter: 38250/300480  CE: 2.5295  
[04/15 20:16:21] Re-training INFO: iter: 38375/300480  CE: 2.7449  
[04/15 20:17:23] Re-training INFO: iter: 38500/300480  CE: 2.4487  
[04/15 20:18:25] Re-training INFO: iter: 38625/300480  CE: 2.6731  
[04/15 20:19:27] Re-training INFO: iter: 38750/300480  CE: 2.5507  
[04/15 20:19:56] Re-training INFO: --> epoch:  31/240  avg CE: 2.5262  lr: 0.3785271181704985  
[04/15 20:20:46] Re-training INFO: iter: 38875/300480  CE: 2.4899  
[04/15 20:21:49] Re-training INFO: iter: 39000/300480  CE: 2.6218  
[04/15 20:22:51] Re-training INFO: iter: 39125/300480  CE: 2.4895  
[04/15 20:23:53] Re-training INFO: iter: 39250/300480  CE: 2.3694  
[04/15 20:24:57] Re-training INFO: iter: 39375/300480  CE: 2.2440  
[04/15 20:25:59] Re-training INFO: iter: 39500/300480  CE: 2.6790  
[04/15 20:27:01] Re-training INFO: iter: 39625/300480  CE: 2.5246  
[04/15 20:28:04] Re-training INFO: iter: 39750/300480  CE: 2.7169  
[04/15 20:29:07] Re-training INFO: iter: 39875/300480  CE: 2.5296  
[04/15 20:30:09] Re-training INFO: iter: 40000/300480  CE: 2.4962  
[04/15 20:30:38] Re-training INFO: --> epoch:  32/240  avg CE: 2.5335  lr: 0.3671713046253836  
[04/15 20:31:27] Re-training INFO: iter: 40125/300480  CE: 2.4694  
[04/15 20:32:30] Re-training INFO: iter: 40250/300480  CE: 2.5512  
[04/15 20:33:33] Re-training INFO: iter: 40375/300480  CE: 2.7197  
[04/15 20:34:36] Re-training INFO: iter: 40500/300480  CE: 2.5448  
[04/15 20:35:39] Re-training INFO: iter: 40625/300480  CE: 2.7191  
[04/15 20:36:42] Re-training INFO: iter: 40750/300480  CE: 2.3960  
[04/15 20:37:45] Re-training INFO: iter: 40875/300480  CE: 2.5242  
[04/15 20:38:48] Re-training INFO: iter: 41000/300480  CE: 2.5751  
[04/15 20:39:51] Re-training INFO: iter: 41125/300480  CE: 2.3996  
[04/15 20:40:53] Re-training INFO: iter: 41250/300480  CE: 3.0005  
[04/15 20:41:25] Re-training INFO: --> epoch:  33/240  avg CE: 2.5155  lr: 0.3671713046253836  
[04/15 20:42:12] Re-training INFO: iter: 41375/300480  CE: 2.6164  
[04/15 20:43:15] Re-training INFO: iter: 41500/300480  CE: 2.4095  
[04/15 20:44:18] Re-training INFO: iter: 41625/300480  CE: 2.5218  
[04/15 20:45:21] Re-training INFO: iter: 41750/300480  CE: 2.5093  
[04/15 20:46:23] Re-training INFO: iter: 41875/300480  CE: 2.6914  
[04/15 20:47:27] Re-training INFO: iter: 42000/300480  CE: 2.5507  
[04/15 20:48:28] Re-training INFO: iter: 42125/300480  CE: 2.5206  
[04/15 20:49:31] Re-training INFO: iter: 42250/300480  CE: 2.3830  
[04/15 20:50:33] Re-training INFO: iter: 42375/300480  CE: 2.6697  
[04/15 20:51:34] Re-training INFO: iter: 42500/300480  CE: 2.5396  
[04/15 20:52:06] Re-training INFO: --> epoch:  34/240  avg CE: 2.5050  lr: 0.35615616548662204  
[04/15 20:52:53] Re-training INFO: iter: 42625/300480  CE: 2.3974  
[04/15 20:53:55] Re-training INFO: iter: 42750/300480  CE: 2.4062  
[04/15 20:54:58] Re-training INFO: iter: 42875/300480  CE: 2.6647  
[04/15 20:56:01] Re-training INFO: iter: 43000/300480  CE: 2.6320  
[04/15 20:57:03] Re-training INFO: iter: 43125/300480  CE: 2.4399  
[04/15 20:58:06] Re-training INFO: iter: 43250/300480  CE: 2.4225  
[04/15 20:59:08] Re-training INFO: iter: 43375/300480  CE: 2.3345  
[04/15 21:00:11] Re-training INFO: iter: 43500/300480  CE: 2.4086  
[04/15 21:01:13] Re-training INFO: iter: 43625/300480  CE: 2.4315  
[04/15 21:02:14] Re-training INFO: iter: 43750/300480  CE: 2.5131  
[04/15 21:02:47] Re-training INFO: --> epoch:  35/240  avg CE: 2.4921  lr: 0.35615616548662204  
[04/15 21:03:33] Re-training INFO: iter: 43875/300480  CE: 2.5172  
[04/15 21:04:36] Re-training INFO: iter: 44000/300480  CE: 2.6346  
[04/15 21:05:39] Re-training INFO: iter: 44125/300480  CE: 2.5670  
[04/15 21:06:41] Re-training INFO: iter: 44250/300480  CE: 2.2236  
[04/15 21:07:44] Re-training INFO: iter: 44375/300480  CE: 2.6534  
[04/15 21:08:46] Re-training INFO: iter: 44500/300480  CE: 2.5438  
[04/15 21:09:48] Re-training INFO: iter: 44625/300480  CE: 2.5142  
[04/15 21:10:50] Re-training INFO: iter: 44750/300480  CE: 2.6828  
[04/15 21:11:52] Re-training INFO: iter: 44875/300480  CE: 2.4293  
[04/15 21:12:53] Re-training INFO: iter: 45000/300480  CE: 2.5797  
[04/15 21:13:28] Re-training INFO: --> epoch:  36/240  avg CE: 2.4885  lr: 0.3454714805220234  
[04/15 21:14:12] Re-training INFO: iter: 45125/300480  CE: 2.7856  
[04/15 21:15:15] Re-training INFO: iter: 45250/300480  CE: 2.6627  
[04/15 21:16:18] Re-training INFO: iter: 45375/300480  CE: 2.6194  
[04/15 21:17:21] Re-training INFO: iter: 45500/300480  CE: 2.4710  
[04/15 21:18:24] Re-training INFO: iter: 45625/300480  CE: 2.8271  
[04/15 21:19:27] Re-training INFO: iter: 45750/300480  CE: 2.4977  
[04/15 21:20:30] Re-training INFO: iter: 45875/300480  CE: 2.2604  
[04/15 21:21:31] Re-training INFO: iter: 46000/300480  CE: 2.6223  
[04/15 21:22:33] Re-training INFO: iter: 46125/300480  CE: 2.3342  
[04/15 21:23:33] Re-training INFO: iter: 46250/300480  CE: 2.3165  
[04/15 21:24:09] Re-training INFO: --> epoch:  37/240  avg CE: 2.4814  lr: 0.3454714805220234  
[04/15 21:24:53] Re-training INFO: iter: 46375/300480  CE: 2.3996  
[04/15 21:25:56] Re-training INFO: iter: 46500/300480  CE: 2.4458  
[04/15 21:26:58] Re-training INFO: iter: 46625/300480  CE: 2.4047  
[04/15 21:28:01] Re-training INFO: iter: 46750/300480  CE: 2.5994  
[04/15 21:29:04] Re-training INFO: iter: 46875/300480  CE: 2.5341  
[04/15 21:30:06] Re-training INFO: iter: 47000/300480  CE: 2.5380  
[04/15 21:31:08] Re-training INFO: iter: 47125/300480  CE: 2.5031  
[04/15 21:32:09] Re-training INFO: iter: 47250/300480  CE: 2.7860  
[04/15 21:33:12] Re-training INFO: iter: 47375/300480  CE: 2.2348  
[04/15 21:34:15] Re-training INFO: iter: 47500/300480  CE: 2.5147  
[04/15 21:34:52] Re-training INFO: --> epoch:  38/240  avg CE: 2.4816  lr: 0.3454714805220234  
[04/15 21:35:35] Re-training INFO: iter: 47625/300480  CE: 2.3713  
[04/15 21:36:37] Re-training INFO: iter: 47750/300480  CE: 2.5550  
[04/15 21:37:40] Re-training INFO: iter: 47875/300480  CE: 2.4404  
[04/15 21:38:42] Re-training INFO: iter: 48000/300480  CE: 2.4767  
[04/15 21:39:44] Re-training INFO: iter: 48125/300480  CE: 2.5294  
[04/15 21:40:47] Re-training INFO: iter: 48250/300480  CE: 2.4133  
[04/15 21:41:48] Re-training INFO: iter: 48375/300480  CE: 2.7038  
[04/15 21:42:50] Re-training INFO: iter: 48500/300480  CE: 2.4650  
[04/15 21:43:53] Re-training INFO: iter: 48625/300480  CE: 2.4707  
[04/15 21:44:54] Re-training INFO: iter: 48750/300480  CE: 2.4086  
[04/15 21:45:32] Re-training INFO: --> epoch:  39/240  avg CE: 2.4687  lr: 0.3351073361063627  
[04/15 21:46:14] Re-training INFO: iter: 48875/300480  CE: 2.5239  
[04/15 21:47:17] Re-training INFO: iter: 49000/300480  CE: 2.3907  
[04/15 21:48:19] Re-training INFO: iter: 49125/300480  CE: 2.5121  
[04/15 21:49:22] Re-training INFO: iter: 49250/300480  CE: 2.2957  
[04/15 21:50:26] Re-training INFO: iter: 49375/300480  CE: 2.5180  
[04/15 21:51:29] Re-training INFO: iter: 49500/300480  CE: 2.3304  
[04/15 21:52:32] Re-training INFO: iter: 49625/300480  CE: 2.1730  
[04/15 21:53:35] Re-training INFO: iter: 49750/300480  CE: 2.2314  
[04/15 21:54:38] Re-training INFO: iter: 49875/300480  CE: 2.1934  
[04/15 21:55:41] Re-training INFO: iter: 50000/300480  CE: 2.3648  
[04/15 21:56:19] Re-training INFO: --> epoch:  40/240  avg CE: 2.4663  lr: 0.3351073361063627  
[04/15 21:56:54] Re-training INFO: # of Test Samples: 50000.0
[04/15 21:56:54] Re-training INFO: Top-1/-5 acc: 65.91 / 87.27
[04/15 21:56:54] Re-training INFO: Top-1/-5 acc: 34.09 / 12.73
[04/15 21:56:54] Re-training INFO: 

[04/15 21:57:35] Re-training INFO: iter: 50125/300480  CE: 2.5834  
[04/15 21:58:38] Re-training INFO: iter: 50250/300480  CE: 2.4294  
[04/15 21:59:41] Re-training INFO: iter: 50375/300480  CE: 2.4873  
[04/15 22:00:43] Re-training INFO: iter: 50500/300480  CE: 2.3304  
[04/15 22:01:46] Re-training INFO: iter: 50625/300480  CE: 2.3014  
[04/15 22:02:48] Re-training INFO: iter: 50750/300480  CE: 2.4353  
[04/15 22:03:50] Re-training INFO: iter: 50875/300480  CE: 2.3851  
[04/15 22:04:51] Re-training INFO: iter: 51000/300480  CE: 2.5196  
[04/15 22:05:53] Re-training INFO: iter: 51125/300480  CE: 2.3216  
[04/15 22:06:54] Re-training INFO: iter: 51250/300480  CE: 2.5502  
[04/15 22:07:33] Re-training INFO: --> epoch:  41/240  avg CE: 2.4568  lr: 0.3250541160231718  
[04/15 22:08:12] Re-training INFO: iter: 51375/300480  CE: 2.5728  
[04/15 22:09:15] Re-training INFO: iter: 51500/300480  CE: 2.4065  
[04/15 22:10:17] Re-training INFO: iter: 51625/300480  CE: 2.3143  
[04/15 22:11:21] Re-training INFO: iter: 51750/300480  CE: 2.5589  
[04/15 22:12:23] Re-training INFO: iter: 51875/300480  CE: 2.5381  
[04/15 22:13:25] Re-training INFO: iter: 52000/300480  CE: 2.2935  
[04/15 22:14:28] Re-training INFO: iter: 52125/300480  CE: 2.3743  
[04/15 22:15:30] Re-training INFO: iter: 52250/300480  CE: 2.8260  
[04/15 22:16:32] Re-training INFO: iter: 52375/300480  CE: 2.4840  
[04/15 22:17:34] Re-training INFO: iter: 52500/300480  CE: 2.2288  
[04/15 22:18:15] Re-training INFO: --> epoch:  42/240  avg CE: 2.4451  lr: 0.3250541160231718  
[04/15 22:18:53] Re-training INFO: iter: 52625/300480  CE: 2.5116  
[04/15 22:19:57] Re-training INFO: iter: 52750/300480  CE: 2.2467  
[04/15 22:20:59] Re-training INFO: iter: 52875/300480  CE: 2.3143  
[04/15 22:22:02] Re-training INFO: iter: 53000/300480  CE: 2.5863  
[04/15 22:23:04] Re-training INFO: iter: 53125/300480  CE: 2.3383  
[04/15 22:24:07] Re-training INFO: iter: 53250/300480  CE: 2.3355  
[04/15 22:25:09] Re-training INFO: iter: 53375/300480  CE: 2.2673  
[04/15 22:26:11] Re-training INFO: iter: 53500/300480  CE: 2.5481  
[04/15 22:27:12] Re-training INFO: iter: 53625/300480  CE: 2.4919  
[04/15 22:28:13] Re-training INFO: iter: 53750/300480  CE: 2.6185  
[04/15 22:28:53] Re-training INFO: --> epoch:  43/240  avg CE: 2.4381  lr: 0.3250541160231718  
[04/15 22:29:30] Re-training INFO: iter: 53875/300480  CE: 2.3085  
[04/15 22:30:33] Re-training INFO: iter: 54000/300480  CE: 2.3269  
[04/15 22:31:37] Re-training INFO: iter: 54125/300480  CE: 2.4136  
[04/15 22:32:39] Re-training INFO: iter: 54250/300480  CE: 2.4785  
[04/15 22:33:43] Re-training INFO: iter: 54375/300480  CE: 2.3421  
[04/15 22:34:44] Re-training INFO: iter: 54500/300480  CE: 2.6249  
[04/15 22:35:46] Re-training INFO: iter: 54625/300480  CE: 2.6532  
[04/15 22:36:48] Re-training INFO: iter: 54750/300480  CE: 2.8797  
[04/15 22:37:49] Re-training INFO: iter: 54875/300480  CE: 2.4129  
[04/15 22:38:52] Re-training INFO: iter: 55000/300480  CE: 2.5339  
[04/15 22:39:33] Re-training INFO: --> epoch:  44/240  avg CE: 2.4302  lr: 0.3153024925424766  
[04/15 22:40:10] Re-training INFO: iter: 55125/300480  CE: 2.5941  
[04/15 22:41:12] Re-training INFO: iter: 55250/300480  CE: 2.5423  
[04/15 22:42:15] Re-training INFO: iter: 55375/300480  CE: 2.4335  
[04/15 22:43:18] Re-training INFO: iter: 55500/300480  CE: 2.3936  
[04/15 22:44:21] Re-training INFO: iter: 55625/300480  CE: 2.5054  
[04/15 22:45:23] Re-training INFO: iter: 55750/300480  CE: 2.3052  
[04/15 22:46:25] Re-training INFO: iter: 55875/300480  CE: 2.6077  
[04/15 22:47:28] Re-training INFO: iter: 56000/300480  CE: 2.4814  
[04/15 22:48:30] Re-training INFO: iter: 56125/300480  CE: 2.7740  
[04/15 22:49:31] Re-training INFO: iter: 56250/300480  CE: 2.2724  
[04/15 22:50:14] Re-training INFO: --> epoch:  45/240  avg CE: 2.4334  lr: 0.3153024925424766  
[04/15 22:50:50] Re-training INFO: iter: 56375/300480  CE: 2.3213  
[04/15 22:51:53] Re-training INFO: iter: 56500/300480  CE: 2.2252  
[04/15 22:52:55] Re-training INFO: iter: 56625/300480  CE: 2.5292  
[04/15 22:54:00] Re-training INFO: iter: 56750/300480  CE: 2.5643  
[04/15 22:55:02] Re-training INFO: iter: 56875/300480  CE: 2.5770  
[04/15 22:56:05] Re-training INFO: iter: 57000/300480  CE: 2.5498  
[04/15 22:57:08] Re-training INFO: iter: 57125/300480  CE: 2.5588  
[04/15 22:58:09] Re-training INFO: iter: 57250/300480  CE: 2.7344  
[04/15 22:59:10] Re-training INFO: iter: 57375/300480  CE: 2.5037  
[04/15 23:00:11] Re-training INFO: iter: 57500/300480  CE: 2.4307  
[04/15 23:00:54] Re-training INFO: --> epoch:  46/240  avg CE: 2.4246  lr: 0.30584341776620233  
[04/15 23:01:29] Re-training INFO: iter: 57625/300480  CE: 2.3163  
[04/15 23:02:33] Re-training INFO: iter: 57750/300480  CE: 2.5123  
[04/15 23:03:36] Re-training INFO: iter: 57875/300480  CE: 2.3549  
[04/15 23:04:40] Re-training INFO: iter: 58000/300480  CE: 2.6702  
[04/15 23:05:44] Re-training INFO: iter: 58125/300480  CE: 2.3717  
[04/15 23:06:47] Re-training INFO: iter: 58250/300480  CE: 2.2785  
[04/15 23:07:49] Re-training INFO: iter: 58375/300480  CE: 2.6175  
[04/15 23:08:52] Re-training INFO: iter: 58500/300480  CE: 2.5409  
[04/15 23:09:55] Re-training INFO: iter: 58625/300480  CE: 2.4553  
[04/15 23:10:56] Re-training INFO: iter: 58750/300480  CE: 2.3468  
[04/15 23:11:41] Re-training INFO: --> epoch:  47/240  avg CE: 2.4147  lr: 0.30584341776620233  
[04/15 23:12:15] Re-training INFO: iter: 58875/300480  CE: 2.2950  
[04/15 23:13:18] Re-training INFO: iter: 59000/300480  CE: 2.2983  
[04/15 23:14:21] Re-training INFO: iter: 59125/300480  CE: 2.4996  
[04/15 23:15:23] Re-training INFO: iter: 59250/300480  CE: 2.2770  
[04/15 23:16:26] Re-training INFO: iter: 59375/300480  CE: 2.4823  
[04/15 23:17:30] Re-training INFO: iter: 59500/300480  CE: 2.3813  
[04/15 23:18:31] Re-training INFO: iter: 59625/300480  CE: 2.2105  
[04/15 23:19:34] Re-training INFO: iter: 59750/300480  CE: 2.3245  
[04/15 23:20:36] Re-training INFO: iter: 59875/300480  CE: 2.4485  
[04/15 23:21:38] Re-training INFO: iter: 60000/300480  CE: 2.3859  
[04/15 23:22:23] Re-training INFO: --> epoch:  48/240  avg CE: 2.4140  lr: 0.29666811523321623  
[04/15 23:22:55] Re-training INFO: iter: 60125/300480  CE: 2.5663  
[04/15 23:23:59] Re-training INFO: iter: 60250/300480  CE: 2.4327  
[04/15 23:25:02] Re-training INFO: iter: 60375/300480  CE: 2.4438  
[04/15 23:26:04] Re-training INFO: iter: 60500/300480  CE: 2.6148  
[04/15 23:27:08] Re-training INFO: iter: 60625/300480  CE: 2.3910  
[04/15 23:28:10] Re-training INFO: iter: 60750/300480  CE: 2.3905  
[04/15 23:29:14] Re-training INFO: iter: 60875/300480  CE: 2.4125  
[04/15 23:30:17] Re-training INFO: iter: 61000/300480  CE: 2.2167  
[04/15 23:31:19] Re-training INFO: iter: 61125/300480  CE: 2.2154  
[04/15 23:32:21] Re-training INFO: iter: 61250/300480  CE: 2.4020  
[04/15 23:33:08] Re-training INFO: --> epoch:  49/240  avg CE: 2.4016  lr: 0.29666811523321623  
[04/15 23:33:40] Re-training INFO: iter: 61375/300480  CE: 2.3449  
[04/15 23:34:42] Re-training INFO: iter: 61500/300480  CE: 2.3552  
[04/15 23:35:46] Re-training INFO: iter: 61625/300480  CE: 2.6383  
[04/15 23:36:49] Re-training INFO: iter: 61750/300480  CE: 2.2761  
[04/15 23:37:52] Re-training INFO: iter: 61875/300480  CE: 2.3509  
[04/15 23:38:55] Re-training INFO: iter: 62000/300480  CE: 2.4430  
[04/15 23:39:56] Re-training INFO: iter: 62125/300480  CE: 2.4769  
[04/15 23:40:59] Re-training INFO: iter: 62250/300480  CE: 2.4202  
[04/15 23:42:00] Re-training INFO: iter: 62375/300480  CE: 2.2402  
[04/15 23:43:01] Re-training INFO: iter: 62500/300480  CE: 2.4923  
[04/15 23:43:48] Re-training INFO: --> epoch:  50/240  avg CE: 2.4042  lr: 0.29666811523321623  
[04/15 23:44:18] Re-training INFO: iter: 62625/300480  CE: 2.3890  
[04/15 23:45:21] Re-training INFO: iter: 62750/300480  CE: 2.3672  
[04/15 23:46:24] Re-training INFO: iter: 62875/300480  CE: 2.1560  
[04/15 23:47:28] Re-training INFO: iter: 63000/300480  CE: 2.1253  
[04/15 23:48:30] Re-training INFO: iter: 63125/300480  CE: 2.3034  
[04/15 23:49:32] Re-training INFO: iter: 63250/300480  CE: 2.3438  
[04/15 23:50:34] Re-training INFO: iter: 63375/300480  CE: 2.2622  
[04/15 23:51:38] Re-training INFO: iter: 63500/300480  CE: 2.7499  
[04/15 23:52:39] Re-training INFO: iter: 63625/300480  CE: 2.3940  
[04/15 23:53:41] Re-training INFO: iter: 63750/300480  CE: 2.4766  
[04/15 23:54:29] Re-training INFO: --> epoch:  51/240  avg CE: 2.4011  lr: 0.28776807177621977  
[04/15 23:54:58] Re-training INFO: iter: 63875/300480  CE: 2.4223  
[04/15 23:56:02] Re-training INFO: iter: 64000/300480  CE: 2.3550  
[04/15 23:57:06] Re-training INFO: iter: 64125/300480  CE: 2.5313  
[04/15 23:58:08] Re-training INFO: iter: 64250/300480  CE: 2.3052  
[04/15 23:59:11] Re-training INFO: iter: 64375/300480  CE: 2.5272  
[04/16 00:00:14] Re-training INFO: iter: 64500/300480  CE: 2.6318  
[04/16 00:01:16] Re-training INFO: iter: 64625/300480  CE: 2.2054  
[04/16 00:02:18] Re-training INFO: iter: 64750/300480  CE: 2.4800  
[04/16 00:03:22] Re-training INFO: iter: 64875/300480  CE: 2.4510  
[04/16 00:04:24] Re-training INFO: iter: 65000/300480  CE: 2.5227  
[04/16 00:05:14] Re-training INFO: --> epoch:  52/240  avg CE: 2.3888  lr: 0.28776807177621977  
[04/16 00:05:42] Re-training INFO: iter: 65125/300480  CE: 2.4217  
[04/16 00:06:45] Re-training INFO: iter: 65250/300480  CE: 2.7099  
[04/16 00:07:49] Re-training INFO: iter: 65375/300480  CE: 2.1455  
[04/16 00:08:52] Re-training INFO: iter: 65500/300480  CE: 2.5849  
[04/16 00:09:54] Re-training INFO: iter: 65625/300480  CE: 2.3490  
[04/16 00:10:57] Re-training INFO: iter: 65750/300480  CE: 2.5861  
[04/16 00:11:59] Re-training INFO: iter: 65875/300480  CE: 2.3110  
[04/16 00:13:01] Re-training INFO: iter: 66000/300480  CE: 2.5688  
[04/16 00:14:02] Re-training INFO: iter: 66125/300480  CE: 2.3232  
[04/16 00:15:05] Re-training INFO: iter: 66250/300480  CE: 2.3708  
[04/16 00:15:56] Re-training INFO: --> epoch:  53/240  avg CE: 2.3924  lr: 0.27913502962293313  
[04/16 00:16:23] Re-training INFO: iter: 66375/300480  CE: 2.0434  
[04/16 00:17:26] Re-training INFO: iter: 66500/300480  CE: 2.4979  
[04/16 00:18:29] Re-training INFO: iter: 66625/300480  CE: 2.4240  
[04/16 00:19:31] Re-training INFO: iter: 66750/300480  CE: 2.2846  
[04/16 00:20:32] Re-training INFO: iter: 66875/300480  CE: 2.3308  
[04/16 00:21:35] Re-training INFO: iter: 67000/300480  CE: 2.4685  
[04/16 00:22:36] Re-training INFO: iter: 67125/300480  CE: 2.6038  
[04/16 00:23:39] Re-training INFO: iter: 67250/300480  CE: 2.2676  
[04/16 00:24:40] Re-training INFO: iter: 67375/300480  CE: 2.0264  
[04/16 00:25:42] Re-training INFO: iter: 67500/300480  CE: 2.3423  
[04/16 00:26:33] Re-training INFO: --> epoch:  54/240  avg CE: 2.3728  lr: 0.27913502962293313  
[04/16 00:27:00] Re-training INFO: iter: 67625/300480  CE: 2.2176  
[04/16 00:28:04] Re-training INFO: iter: 67750/300480  CE: 2.3382  
[04/16 00:29:07] Re-training INFO: iter: 67875/300480  CE: 2.2578  
[04/16 00:30:10] Re-training INFO: iter: 68000/300480  CE: 2.3262  
[04/16 00:31:11] Re-training INFO: iter: 68125/300480  CE: 2.2545  
[04/16 00:32:13] Re-training INFO: iter: 68250/300480  CE: 2.2669  
[04/16 00:33:15] Re-training INFO: iter: 68375/300480  CE: 2.2965  
[04/16 00:34:17] Re-training INFO: iter: 68500/300480  CE: 2.4062  
[04/16 00:35:20] Re-training INFO: iter: 68625/300480  CE: 2.2776  
[04/16 00:36:22] Re-training INFO: iter: 68750/300480  CE: 2.4105  
[04/16 00:37:15] Re-training INFO: --> epoch:  55/240  avg CE: 2.3806  lr: 0.27913502962293313  
[04/16 00:37:40] Re-training INFO: iter: 68875/300480  CE: 2.2385  
[04/16 00:38:45] Re-training INFO: iter: 69000/300480  CE: 2.3317  
[04/16 00:39:49] Re-training INFO: iter: 69125/300480  CE: 2.3441  
[04/16 00:40:52] Re-training INFO: iter: 69250/300480  CE: 2.4844  
[04/16 00:41:55] Re-training INFO: iter: 69375/300480  CE: 2.1260  
[04/16 00:42:57] Re-training INFO: iter: 69500/300480  CE: 2.2421  
[04/16 00:44:01] Re-training INFO: iter: 69625/300480  CE: 2.4571  
[04/16 00:45:02] Re-training INFO: iter: 69750/300480  CE: 2.3525  
[04/16 00:46:02] Re-training INFO: iter: 69875/300480  CE: 2.4757  
[04/16 00:47:02] Re-training INFO: iter: 70000/300480  CE: 2.4807  
[04/16 00:47:55] Re-training INFO: --> epoch:  56/240  avg CE: 2.3647  lr: 0.27076097873424515  
[04/16 00:48:19] Re-training INFO: iter: 70125/300480  CE: 2.4116  
[04/16 00:49:22] Re-training INFO: iter: 70250/300480  CE: 2.1801  
[04/16 00:50:26] Re-training INFO: iter: 70375/300480  CE: 2.5457  
[04/16 00:51:28] Re-training INFO: iter: 70500/300480  CE: 2.4200  
[04/16 00:52:32] Re-training INFO: iter: 70625/300480  CE: 2.4364  
[04/16 00:53:34] Re-training INFO: iter: 70750/300480  CE: 2.4796  
[04/16 00:54:36] Re-training INFO: iter: 70875/300480  CE: 2.2539  
[04/16 00:55:38] Re-training INFO: iter: 71000/300480  CE: 2.5331  
[04/16 00:56:41] Re-training INFO: iter: 71125/300480  CE: 2.3345  
[04/16 00:57:43] Re-training INFO: iter: 71250/300480  CE: 2.4756  
[04/16 00:58:38] Re-training INFO: --> epoch:  57/240  avg CE: 2.3725  lr: 0.27076097873424515  
[04/16 00:59:02] Re-training INFO: iter: 71375/300480  CE: 2.4854  
[04/16 01:00:05] Re-training INFO: iter: 71500/300480  CE: 2.3516  
[04/16 01:01:08] Re-training INFO: iter: 71625/300480  CE: 2.2229  
[04/16 01:02:11] Re-training INFO: iter: 71750/300480  CE: 2.2335  
[04/16 01:03:14] Re-training INFO: iter: 71875/300480  CE: 2.6200  
[04/16 01:04:15] Re-training INFO: iter: 72000/300480  CE: 2.2296  
[04/16 01:05:19] Re-training INFO: iter: 72125/300480  CE: 2.2305  
[04/16 01:06:22] Re-training INFO: iter: 72250/300480  CE: 2.4113  
[04/16 01:07:25] Re-training INFO: iter: 72375/300480  CE: 2.4657  
[04/16 01:08:28] Re-training INFO: iter: 72500/300480  CE: 2.4805  
[04/16 01:09:25] Re-training INFO: --> epoch:  58/240  avg CE: 2.3613  lr: 0.26263814937221774  
[04/16 01:09:47] Re-training INFO: iter: 72625/300480  CE: 2.0559  
[04/16 01:10:50] Re-training INFO: iter: 72750/300480  CE: 2.2510  
[04/16 01:11:54] Re-training INFO: iter: 72875/300480  CE: 2.2923  
[04/16 01:12:58] Re-training INFO: iter: 73000/300480  CE: 2.5244  
[04/16 01:14:01] Re-training INFO: iter: 73125/300480  CE: 2.4522  
[04/16 01:15:04] Re-training INFO: iter: 73250/300480  CE: 2.5527  
[04/16 01:16:06] Re-training INFO: iter: 73375/300480  CE: 2.1742  
[04/16 01:17:08] Re-training INFO: iter: 73500/300480  CE: 2.6002  
[04/16 01:18:13] Re-training INFO: iter: 73625/300480  CE: 2.3916  
[04/16 01:19:14] Re-training INFO: iter: 73750/300480  CE: 2.4898  
[04/16 01:20:12] Re-training INFO: --> epoch:  59/240  avg CE: 2.3552  lr: 0.26263814937221774  
[04/16 01:20:33] Re-training INFO: iter: 73875/300480  CE: 2.4641  
[04/16 01:21:36] Re-training INFO: iter: 74000/300480  CE: 2.4316  
[04/16 01:22:38] Re-training INFO: iter: 74125/300480  CE: 2.5411  
[04/16 01:23:41] Re-training INFO: iter: 74250/300480  CE: 2.3364  
[04/16 01:24:45] Re-training INFO: iter: 74375/300480  CE: 2.1543  
[04/16 01:25:48] Re-training INFO: iter: 74500/300480  CE: 2.5025  
[04/16 01:26:52] Re-training INFO: iter: 74625/300480  CE: 2.3857  
[04/16 01:27:55] Re-training INFO: iter: 74750/300480  CE: 2.4151  
[04/16 01:28:57] Re-training INFO: iter: 74875/300480  CE: 2.3206  
[04/16 01:30:00] Re-training INFO: iter: 75000/300480  CE: 2.5163  
[04/16 01:30:59] Re-training INFO: --> epoch:  60/240  avg CE: 2.3528  lr: 0.25475900489105124  
[04/16 01:31:32] Re-training INFO: # of Test Samples: 50000.0
[04/16 01:31:32] Re-training INFO: Top-1/-5 acc: 67.44 / 87.93
[04/16 01:31:32] Re-training INFO: Top-1/-5 acc: 32.56 / 12.07
[04/16 01:31:32] Re-training INFO: 

[04/16 01:31:52] Re-training INFO: iter: 75125/300480  CE: 2.1895  
[04/16 01:32:56] Re-training INFO: iter: 75250/300480  CE: 2.4943  
[04/16 01:33:59] Re-training INFO: iter: 75375/300480  CE: 2.4451  
[04/16 01:35:03] Re-training INFO: iter: 75500/300480  CE: 2.2188  
[04/16 01:36:06] Re-training INFO: iter: 75625/300480  CE: 2.0082  
[04/16 01:37:10] Re-training INFO: iter: 75750/300480  CE: 2.0650  
[04/16 01:38:14] Re-training INFO: iter: 75875/300480  CE: 2.1573  
[04/16 01:39:17] Re-training INFO: iter: 76000/300480  CE: 2.2326  
[04/16 01:40:20] Re-training INFO: iter: 76125/300480  CE: 2.4662  
[04/16 01:41:23] Re-training INFO: iter: 76250/300480  CE: 2.1303  
[04/16 01:42:22] Re-training INFO: --> epoch:  61/240  avg CE: 2.3523  lr: 0.25475900489105124  
[04/16 01:42:42] Re-training INFO: iter: 76375/300480  CE: 2.2816  
[04/16 01:43:44] Re-training INFO: iter: 76500/300480  CE: 2.3014  
[04/16 01:44:48] Re-training INFO: iter: 76625/300480  CE: 2.2064  
[04/16 01:45:51] Re-training INFO: iter: 76750/300480  CE: 2.2349  
[04/16 01:46:52] Re-training INFO: iter: 76875/300480  CE: 2.4335  
[04/16 01:47:55] Re-training INFO: iter: 77000/300480  CE: 2.1236  
[04/16 01:48:56] Re-training INFO: iter: 77125/300480  CE: 2.2560  
[04/16 01:49:58] Re-training INFO: iter: 77250/300480  CE: 2.2945  
[04/16 01:50:59] Re-training INFO: iter: 77375/300480  CE: 2.3143  
[04/16 01:51:59] Re-training INFO: iter: 77500/300480  CE: 2.6050  
[04/16 01:52:58] Re-training INFO: --> epoch:  62/240  avg CE: 2.3427  lr: 0.25475900489105124  
[04/16 01:53:16] Re-training INFO: iter: 77625/300480  CE: 2.5677  
[04/16 01:54:19] Re-training INFO: iter: 77750/300480  CE: 2.2816  
[04/16 01:55:21] Re-training INFO: iter: 77875/300480  CE: 2.4209  
[04/16 01:56:23] Re-training INFO: iter: 78000/300480  CE: 2.0814  
[04/16 01:57:27] Re-training INFO: iter: 78125/300480  CE: 2.3602  
[04/16 01:58:29] Re-training INFO: iter: 78250/300480  CE: 2.3522  
[04/16 01:59:31] Re-training INFO: iter: 78375/300480  CE: 2.4116  
[04/16 02:00:33] Re-training INFO: iter: 78500/300480  CE: 2.2807  
[04/16 02:01:34] Re-training INFO: iter: 78625/300480  CE: 2.3742  
[04/16 02:02:35] Re-training INFO: iter: 78750/300480  CE: 2.5225  
[04/16 02:03:35] Re-training INFO: iter: 78875/300480  CE: 2.3870  
[04/16 02:03:35] Re-training INFO: --> epoch:  63/240  avg CE: 2.3421  lr: 0.24711623474431968  
[04/16 02:04:55] Re-training INFO: iter: 79000/300480  CE: 2.3274  
[04/16 02:05:59] Re-training INFO: iter: 79125/300480  CE: 2.3673  
[04/16 02:07:01] Re-training INFO: iter: 79250/300480  CE: 2.2333  
[04/16 02:08:03] Re-training INFO: iter: 79375/300480  CE: 2.2081  
[04/16 02:09:06] Re-training INFO: iter: 79500/300480  CE: 2.5820  
[04/16 02:10:09] Re-training INFO: iter: 79625/300480  CE: 2.4210  
[04/16 02:11:11] Re-training INFO: iter: 79750/300480  CE: 2.5872  
[04/16 02:12:14] Re-training INFO: iter: 79875/300480  CE: 2.5030  
[04/16 02:13:15] Re-training INFO: iter: 80000/300480  CE: 2.2552  
[04/16 02:14:18] Re-training INFO: iter: 80125/300480  CE: 2.4876  
[04/16 02:14:18] Re-training INFO: --> epoch:  64/240  avg CE: 2.3296  lr: 0.24711623474431968  
[04/16 02:15:37] Re-training INFO: iter: 80250/300480  CE: 2.1137  
[04/16 02:16:40] Re-training INFO: iter: 80375/300480  CE: 2.3795  
[04/16 02:17:42] Re-training INFO: iter: 80500/300480  CE: 2.3511  
[04/16 02:18:44] Re-training INFO: iter: 80625/300480  CE: 2.3954  
[04/16 02:19:47] Re-training INFO: iter: 80750/300480  CE: 2.2114  
[04/16 02:20:49] Re-training INFO: iter: 80875/300480  CE: 2.2707  
[04/16 02:21:51] Re-training INFO: iter: 81000/300480  CE: 2.5255  
[04/16 02:22:53] Re-training INFO: iter: 81125/300480  CE: 2.2828  
[04/16 02:23:54] Re-training INFO: iter: 81250/300480  CE: 2.1805  
[04/16 02:24:55] Re-training INFO: iter: 81375/300480  CE: 2.6304  
[04/16 02:24:56] Re-training INFO: --> epoch:  65/240  avg CE: 2.3290  lr: 0.2397027477019901  
[04/16 02:26:14] Re-training INFO: iter: 81500/300480  CE: 2.2929  
[04/16 02:27:17] Re-training INFO: iter: 81625/300480  CE: 2.2751  
[04/16 02:28:21] Re-training INFO: iter: 81750/300480  CE: 2.3773  
[04/16 02:29:24] Re-training INFO: iter: 81875/300480  CE: 2.2608  
[04/16 02:30:27] Re-training INFO: iter: 82000/300480  CE: 2.6078  
[04/16 02:31:30] Re-training INFO: iter: 82125/300480  CE: 2.3132  
[04/16 02:32:32] Re-training INFO: iter: 82250/300480  CE: 2.4688  
[04/16 02:33:34] Re-training INFO: iter: 82375/300480  CE: 2.4458  
[04/16 02:34:36] Re-training INFO: iter: 82500/300480  CE: 2.4462  
[04/16 02:35:36] Re-training INFO: iter: 82625/300480  CE: 2.2463  
[04/16 02:35:39] Re-training INFO: --> epoch:  66/240  avg CE: 2.3262  lr: 0.2397027477019901  
[04/16 02:36:55] Re-training INFO: iter: 82750/300480  CE: 2.4534  
[04/16 02:37:58] Re-training INFO: iter: 82875/300480  CE: 2.1351  
[04/16 02:39:00] Re-training INFO: iter: 83000/300480  CE: 2.2200  
[04/16 02:40:03] Re-training INFO: iter: 83125/300480  CE: 1.9349  
[04/16 02:41:05] Re-training INFO: iter: 83250/300480  CE: 2.5222  
[04/16 02:42:07] Re-training INFO: iter: 83375/300480  CE: 2.4239  
[04/16 02:43:09] Re-training INFO: iter: 83500/300480  CE: 2.2147  
[04/16 02:44:11] Re-training INFO: iter: 83625/300480  CE: 2.1971  
[04/16 02:45:13] Re-training INFO: iter: 83750/300480  CE: 2.3440  
[04/16 02:46:15] Re-training INFO: iter: 83875/300480  CE: 2.3265  
[04/16 02:46:17] Re-training INFO: --> epoch:  67/240  avg CE: 2.3202  lr: 0.2397027477019901  
[04/16 02:47:35] Re-training INFO: iter: 84000/300480  CE: 2.4131  
[04/16 02:48:39] Re-training INFO: iter: 84125/300480  CE: 2.4152  
[04/16 02:49:43] Re-training INFO: iter: 84250/300480  CE: 2.1701  
[04/16 02:50:46] Re-training INFO: iter: 84375/300480  CE: 2.2812  
[04/16 02:51:50] Re-training INFO: iter: 84500/300480  CE: 2.2345  
[04/16 02:52:53] Re-training INFO: iter: 84625/300480  CE: 2.2421  
[04/16 02:53:55] Re-training INFO: iter: 84750/300480  CE: 2.2418  
[04/16 02:54:56] Re-training INFO: iter: 84875/300480  CE: 2.2969  
[04/16 02:55:57] Re-training INFO: iter: 85000/300480  CE: 2.3007  
[04/16 02:56:59] Re-training INFO: iter: 85125/300480  CE: 2.3372  
[04/16 02:57:02] Re-training INFO: --> epoch:  68/240  avg CE: 2.3175  lr: 0.2325116652709304  
[04/16 02:58:18] Re-training INFO: iter: 85250/300480  CE: 2.2400  
[04/16 02:59:22] Re-training INFO: iter: 85375/300480  CE: 2.4353  
[04/16 03:00:26] Re-training INFO: iter: 85500/300480  CE: 2.5381  
[04/16 03:01:28] Re-training INFO: iter: 85625/300480  CE: 2.3771  
[04/16 03:02:31] Re-training INFO: iter: 85750/300480  CE: 2.3018  
[04/16 03:03:35] Re-training INFO: iter: 85875/300480  CE: 2.3593  
[04/16 03:04:37] Re-training INFO: iter: 86000/300480  CE: 2.4306  
[04/16 03:05:40] Re-training INFO: iter: 86125/300480  CE: 2.0581  
[04/16 03:06:42] Re-training INFO: iter: 86250/300480  CE: 2.3214  
[04/16 03:07:45] Re-training INFO: iter: 86375/300480  CE: 2.2840  
[04/16 03:07:49] Re-training INFO: --> epoch:  69/240  avg CE: 2.3047  lr: 0.2325116652709304  
[04/16 03:09:04] Re-training INFO: iter: 86500/300480  CE: 2.0892  
[04/16 03:10:08] Re-training INFO: iter: 86625/300480  CE: 2.1841  
[04/16 03:11:12] Re-training INFO: iter: 86750/300480  CE: 2.2868  
[04/16 03:12:16] Re-training INFO: iter: 86875/300480  CE: 2.3963  
[04/16 03:13:19] Re-training INFO: iter: 87000/300480  CE: 2.0703  
[04/16 03:14:22] Re-training INFO: iter: 87125/300480  CE: 2.4971  
[04/16 03:15:26] Re-training INFO: iter: 87250/300480  CE: 1.9994  
[04/16 03:16:28] Re-training INFO: iter: 87375/300480  CE: 2.0252  
[04/16 03:17:32] Re-training INFO: iter: 87500/300480  CE: 2.1925  
[04/16 03:18:35] Re-training INFO: iter: 87625/300480  CE: 2.4570  
[04/16 03:18:41] Re-training INFO: --> epoch:  70/240  avg CE: 2.3135  lr: 0.22553631531280247  
[04/16 03:19:55] Re-training INFO: iter: 87750/300480  CE: 2.1137  
[04/16 03:20:59] Re-training INFO: iter: 87875/300480  CE: 2.3899  
[04/16 03:22:02] Re-training INFO: iter: 88000/300480  CE: 2.4218  
[04/16 03:23:05] Re-training INFO: iter: 88125/300480  CE: 2.3210  
[04/16 03:24:09] Re-training INFO: iter: 88250/300480  CE: 2.3808  
[04/16 03:25:14] Re-training INFO: iter: 88375/300480  CE: 2.3804  
[04/16 03:26:17] Re-training INFO: iter: 88500/300480  CE: 2.4096  
[04/16 03:27:19] Re-training INFO: iter: 88625/300480  CE: 2.4204  
[04/16 03:28:21] Re-training INFO: iter: 88750/300480  CE: 2.2754  
[04/16 03:29:22] Re-training INFO: iter: 88875/300480  CE: 2.4344  
[04/16 03:29:30] Re-training INFO: --> epoch:  71/240  avg CE: 2.2943  lr: 0.22553631531280247  
[04/16 03:30:42] Re-training INFO: iter: 89000/300480  CE: 2.5829  
[04/16 03:31:46] Re-training INFO: iter: 89125/300480  CE: 2.1514  
[04/16 03:32:49] Re-training INFO: iter: 89250/300480  CE: 2.5937  
[04/16 03:33:52] Re-training INFO: iter: 89375/300480  CE: 2.0729  
[04/16 03:34:55] Re-training INFO: iter: 89500/300480  CE: 2.5145  
[04/16 03:35:57] Re-training INFO: iter: 89625/300480  CE: 2.2165  
[04/16 03:37:02] Re-training INFO: iter: 89750/300480  CE: 2.2755  
[04/16 03:38:06] Re-training INFO: iter: 89875/300480  CE: 2.2812  
[04/16 03:39:08] Re-training INFO: iter: 90000/300480  CE: 2.3026  
[04/16 03:40:12] Re-training INFO: iter: 90125/300480  CE: 2.4842  
[04/16 03:40:20] Re-training INFO: --> epoch:  72/240  avg CE: 2.2961  lr: 0.2187702258534184  
[04/16 03:41:32] Re-training INFO: iter: 90250/300480  CE: 2.4346  
[04/16 03:42:36] Re-training INFO: iter: 90375/300480  CE: 2.3921  
[04/16 03:43:41] Re-training INFO: iter: 90500/300480  CE: 2.5193  
[04/16 03:44:44] Re-training INFO: iter: 90625/300480  CE: 2.4023  
[04/16 03:45:46] Re-training INFO: iter: 90750/300480  CE: 2.4696  
[04/16 03:46:50] Re-training INFO: iter: 90875/300480  CE: 2.3513  
[04/16 03:47:53] Re-training INFO: iter: 91000/300480  CE: 2.2767  
[04/16 03:48:56] Re-training INFO: iter: 91125/300480  CE: 2.2028  
[04/16 03:50:00] Re-training INFO: iter: 91250/300480  CE: 2.1012  
[04/16 03:51:02] Re-training INFO: iter: 91375/300480  CE: 2.4126  
[04/16 03:51:11] Re-training INFO: --> epoch:  73/240  avg CE: 2.2911  lr: 0.2187702258534184  
[04/16 03:52:21] Re-training INFO: iter: 91500/300480  CE: 2.2966  
[04/16 03:53:25] Re-training INFO: iter: 91625/300480  CE: 2.2801  
[04/16 03:54:28] Re-training INFO: iter: 91750/300480  CE: 2.3395  
[04/16 03:55:32] Re-training INFO: iter: 91875/300480  CE: 2.1565  
[04/16 03:56:36] Re-training INFO: iter: 92000/300480  CE: 2.1394  
[04/16 03:57:38] Re-training INFO: iter: 92125/300480  CE: 2.2708  
[04/16 03:58:41] Re-training INFO: iter: 92250/300480  CE: 2.3092  
[04/16 03:59:43] Re-training INFO: iter: 92375/300480  CE: 2.1442  
[04/16 04:00:46] Re-training INFO: iter: 92500/300480  CE: 2.3522  
[04/16 04:01:48] Re-training INFO: iter: 92625/300480  CE: 2.2645  
[04/16 04:01:57] Re-training INFO: --> epoch:  74/240  avg CE: 2.2880  lr: 0.2187702258534184  
[04/16 04:03:07] Re-training INFO: iter: 92750/300480  CE: 2.0977  
[04/16 04:04:10] Re-training INFO: iter: 92875/300480  CE: 2.3681  
[04/16 04:05:14] Re-training INFO: iter: 93000/300480  CE: 2.2513  
[04/16 04:06:16] Re-training INFO: iter: 93125/300480  CE: 2.2246  
[04/16 04:07:18] Re-training INFO: iter: 93250/300480  CE: 2.2308  
[04/16 04:08:21] Re-training INFO: iter: 93375/300480  CE: 2.4425  
[04/16 04:09:24] Re-training INFO: iter: 93500/300480  CE: 2.4150  
[04/16 04:10:26] Re-training INFO: iter: 93625/300480  CE: 2.3240  
[04/16 04:11:29] Re-training INFO: iter: 93750/300480  CE: 2.1348  
[04/16 04:12:32] Re-training INFO: iter: 93875/300480  CE: 2.2079  
[04/16 04:12:43] Re-training INFO: --> epoch:  75/240  avg CE: 2.2921  lr: 0.21220711907781584  
[04/16 04:13:52] Re-training INFO: iter: 94000/300480  CE: 2.2690  
[04/16 04:14:57] Re-training INFO: iter: 94125/300480  CE: 2.2021  
[04/16 04:16:00] Re-training INFO: iter: 94250/300480  CE: 2.1913  
[04/16 04:17:04] Re-training INFO: iter: 94375/300480  CE: 2.2289  
[04/16 04:18:07] Re-training INFO: iter: 94500/300480  CE: 2.0411  
[04/16 04:19:09] Re-training INFO: iter: 94625/300480  CE: 2.2996  
[04/16 04:20:13] Re-training INFO: iter: 94750/300480  CE: 2.3659  
[04/16 04:21:16] Re-training INFO: iter: 94875/300480  CE: 2.1594  
[04/16 04:22:19] Re-training INFO: iter: 95000/300480  CE: 2.2606  
[04/16 04:23:21] Re-training INFO: iter: 95125/300480  CE: 2.2327  
[04/16 04:23:33] Re-training INFO: --> epoch:  76/240  avg CE: 2.2796  lr: 0.21220711907781584  
[04/16 04:24:41] Re-training INFO: iter: 95250/300480  CE: 2.1828  
[04/16 04:25:44] Re-training INFO: iter: 95375/300480  CE: 2.4212  
[04/16 04:26:47] Re-training INFO: iter: 95500/300480  CE: 2.1503  
[04/16 04:27:49] Re-training INFO: iter: 95625/300480  CE: 2.4359  
[04/16 04:28:51] Re-training INFO: iter: 95750/300480  CE: 2.2807  
[04/16 04:29:55] Re-training INFO: iter: 95875/300480  CE: 2.2977  
[04/16 04:30:58] Re-training INFO: iter: 96000/300480  CE: 2.4748  
[04/16 04:32:01] Re-training INFO: iter: 96125/300480  CE: 2.4998  
[04/16 04:33:04] Re-training INFO: iter: 96250/300480  CE: 2.5777  
[04/16 04:34:06] Re-training INFO: iter: 96375/300480  CE: 2.4614  
[04/16 04:34:19] Re-training INFO: --> epoch:  77/240  avg CE: 2.2884  lr: 0.20584090550548134  
[04/16 04:35:27] Re-training INFO: iter: 96500/300480  CE: 2.2979  
[04/16 04:36:30] Re-training INFO: iter: 96625/300480  CE: 2.3847  
[04/16 04:37:33] Re-training INFO: iter: 96750/300480  CE: 2.2315  
[04/16 04:38:37] Re-training INFO: iter: 96875/300480  CE: 2.0870  
[04/16 04:39:39] Re-training INFO: iter: 97000/300480  CE: 2.1102  
[04/16 04:40:42] Re-training INFO: iter: 97125/300480  CE: 2.4723  
[04/16 04:41:45] Re-training INFO: iter: 97250/300480  CE: 2.3401  
[04/16 04:42:47] Re-training INFO: iter: 97375/300480  CE: 2.6097  
[04/16 04:43:50] Re-training INFO: iter: 97500/300480  CE: 2.4767  
[04/16 04:44:52] Re-training INFO: iter: 97625/300480  CE: 2.2388  
[04/16 04:45:06] Re-training INFO: --> epoch:  78/240  avg CE: 2.2723  lr: 0.20584090550548134  
[04/16 04:46:11] Re-training INFO: iter: 97750/300480  CE: 2.3686  
[04/16 04:47:15] Re-training INFO: iter: 97875/300480  CE: 2.1553  
[04/16 04:48:18] Re-training INFO: iter: 98000/300480  CE: 2.2853  
[04/16 04:49:22] Re-training INFO: iter: 98125/300480  CE: 2.3524  
[04/16 04:50:25] Re-training INFO: iter: 98250/300480  CE: 2.1955  
[04/16 04:51:27] Re-training INFO: iter: 98375/300480  CE: 2.4817  
[04/16 04:52:30] Re-training INFO: iter: 98500/300480  CE: 2.3793  
[04/16 04:53:32] Re-training INFO: iter: 98625/300480  CE: 2.3117  
[04/16 04:54:34] Re-training INFO: iter: 98750/300480  CE: 2.2212  
[04/16 04:55:36] Re-training INFO: iter: 98875/300480  CE: 2.4247  
[04/16 04:55:50] Re-training INFO: --> epoch:  79/240  avg CE: 2.2745  lr: 0.20584090550548134  
[04/16 04:56:55] Re-training INFO: iter: 99000/300480  CE: 2.2729  
[04/16 04:57:58] Re-training INFO: iter: 99125/300480  CE: 2.5748  
[04/16 04:59:03] Re-training INFO: iter: 99250/300480  CE: 2.2644  
[04/16 05:00:06] Re-training INFO: iter: 99375/300480  CE: 2.3790  
[04/16 05:01:08] Re-training INFO: iter: 99500/300480  CE: 2.1955  
[04/16 05:02:11] Re-training INFO: iter: 99625/300480  CE: 2.1208  
[04/16 05:03:14] Re-training INFO: iter: 99750/300480  CE: 2.1277  
[04/16 05:04:16] Re-training INFO: iter: 99875/300480  CE: 2.2975  
[04/16 05:05:19] Re-training INFO: iter: 100000/300480  CE: 2.3392  
[04/16 05:06:21] Re-training INFO: iter: 100125/300480  CE: 2.1400  
[04/16 05:06:37] Re-training INFO: --> epoch:  80/240  avg CE: 2.2677  lr: 0.1996656783403169  
[04/16 05:07:11] Re-training INFO: # of Test Samples: 50000.0
[04/16 05:07:11] Re-training INFO: Top-1/-5 acc: 69.27 / 89.16
[04/16 05:07:11] Re-training INFO: Top-1/-5 acc: 30.73 / 10.84
[04/16 05:07:11] Re-training INFO: 

[04/16 05:08:15] Re-training INFO: iter: 100250/300480  CE: 2.1080  
[04/16 05:09:19] Re-training INFO: iter: 100375/300480  CE: 2.3805  
[04/16 05:10:23] Re-training INFO: iter: 100500/300480  CE: 2.2841  
[04/16 05:11:25] Re-training INFO: iter: 100625/300480  CE: 2.1995  
[04/16 05:12:28] Re-training INFO: iter: 100750/300480  CE: 2.1077  
[04/16 05:13:31] Re-training INFO: iter: 100875/300480  CE: 2.1382  
[04/16 05:14:33] Re-training INFO: iter: 101000/300480  CE: 2.5301  
[04/16 05:15:36] Re-training INFO: iter: 101125/300480  CE: 2.3942  
[04/16 05:16:38] Re-training INFO: iter: 101250/300480  CE: 2.3647  
[04/16 05:17:41] Re-training INFO: iter: 101375/300480  CE: 2.2312  
[04/16 05:17:58] Re-training INFO: --> epoch:  81/240  avg CE: 2.2565  lr: 0.1996656783403169  
[04/16 05:19:00] Re-training INFO: iter: 101500/300480  CE: 2.3799  
[04/16 05:20:03] Re-training INFO: iter: 101625/300480  CE: 2.1421  
[04/16 05:21:06] Re-training INFO: iter: 101750/300480  CE: 2.4428  
[04/16 05:22:09] Re-training INFO: iter: 101875/300480  CE: 2.2463  
[04/16 05:23:12] Re-training INFO: iter: 102000/300480  CE: 2.5152  
[04/16 05:24:15] Re-training INFO: iter: 102125/300480  CE: 2.5491  
[04/16 05:25:18] Re-training INFO: iter: 102250/300480  CE: 2.0921  
[04/16 05:26:20] Re-training INFO: iter: 102375/300480  CE: 2.0509  
[04/16 05:27:23] Re-training INFO: iter: 102500/300480  CE: 2.2208  
[04/16 05:28:25] Re-training INFO: iter: 102625/300480  CE: 2.4674  
[04/16 05:28:43] Re-training INFO: --> epoch:  82/240  avg CE: 2.2679  lr: 0.1936757079901074  
[04/16 05:29:44] Re-training INFO: iter: 102750/300480  CE: 2.1237  
[04/16 05:30:49] Re-training INFO: iter: 102875/300480  CE: 2.1041  
[04/16 05:31:51] Re-training INFO: iter: 103000/300480  CE: 2.1268  
[04/16 05:32:54] Re-training INFO: iter: 103125/300480  CE: 2.2025  
[04/16 05:33:56] Re-training INFO: iter: 103250/300480  CE: 2.1686  
[04/16 05:34:58] Re-training INFO: iter: 103375/300480  CE: 2.0893  
[04/16 05:36:01] Re-training INFO: iter: 103500/300480  CE: 2.1975  
[04/16 05:37:04] Re-training INFO: iter: 103625/300480  CE: 2.2609  
[04/16 05:38:07] Re-training INFO: iter: 103750/300480  CE: 2.2102  
[04/16 05:39:09] Re-training INFO: iter: 103875/300480  CE: 2.3396  
[04/16 05:39:28] Re-training INFO: --> epoch:  83/240  avg CE: 2.2572  lr: 0.1936757079901074  
[04/16 05:40:28] Re-training INFO: iter: 104000/300480  CE: 2.0819  
[04/16 05:41:32] Re-training INFO: iter: 104125/300480  CE: 2.2816  
[04/16 05:42:35] Re-training INFO: iter: 104250/300480  CE: 2.3539  
[04/16 05:43:39] Re-training INFO: iter: 104375/300480  CE: 2.4200  
[04/16 05:44:42] Re-training INFO: iter: 104500/300480  CE: 2.3399  
[04/16 05:45:44] Re-training INFO: iter: 104625/300480  CE: 2.2035  
[04/16 05:46:46] Re-training INFO: iter: 104750/300480  CE: 2.2598  
[04/16 05:47:49] Re-training INFO: iter: 104875/300480  CE: 2.3454  
[04/16 05:48:51] Re-training INFO: iter: 105000/300480  CE: 2.6005  
[04/16 05:49:52] Re-training INFO: iter: 105125/300480  CE: 2.2928  
[04/16 05:50:12] Re-training INFO: --> epoch:  84/240  avg CE: 2.2587  lr: 0.18786543675040418  
[04/16 05:51:12] Re-training INFO: iter: 105250/300480  CE: 2.1604  
[04/16 05:52:14] Re-training INFO: iter: 105375/300480  CE: 2.3472  
[04/16 05:53:17] Re-training INFO: iter: 105500/300480  CE: 2.1619  
[04/16 05:54:19] Re-training INFO: iter: 105625/300480  CE: 2.1494  
[04/16 05:55:22] Re-training INFO: iter: 105750/300480  CE: 2.1292  
[04/16 05:56:24] Re-training INFO: iter: 105875/300480  CE: 2.0631  
[04/16 05:57:25] Re-training INFO: iter: 106000/300480  CE: 2.3221  
[04/16 05:58:28] Re-training INFO: iter: 106125/300480  CE: 2.1476  
[04/16 05:59:31] Re-training INFO: iter: 106250/300480  CE: 2.1652  
[04/16 06:00:32] Re-training INFO: iter: 106375/300480  CE: 2.1424  
[04/16 06:00:53] Re-training INFO: --> epoch:  85/240  avg CE: 2.2413  lr: 0.18786543675040418  
[04/16 06:01:51] Re-training INFO: iter: 106500/300480  CE: 2.0519  
[04/16 06:02:54] Re-training INFO: iter: 106625/300480  CE: 2.1286  
[04/16 06:03:57] Re-training INFO: iter: 106750/300480  CE: 2.1053  
[04/16 06:04:59] Re-training INFO: iter: 106875/300480  CE: 2.2893  
[04/16 06:06:02] Re-training INFO: iter: 107000/300480  CE: 2.0433  
[04/16 06:07:05] Re-training INFO: iter: 107125/300480  CE: 2.1638  
[04/16 06:08:07] Re-training INFO: iter: 107250/300480  CE: 2.3264  
[04/16 06:09:08] Re-training INFO: iter: 107375/300480  CE: 2.3264  
[04/16 06:10:10] Re-training INFO: iter: 107500/300480  CE: 2.5301  
[04/16 06:11:11] Re-training INFO: iter: 107625/300480  CE: 2.2611  
[04/16 06:11:33] Re-training INFO: --> epoch:  86/240  avg CE: 2.2449  lr: 0.18786543675040418  
[04/16 06:12:31] Re-training INFO: iter: 107750/300480  CE: 2.3347  
[04/16 06:13:34] Re-training INFO: iter: 107875/300480  CE: 2.2066  
[04/16 06:14:37] Re-training INFO: iter: 108000/300480  CE: 2.1542  
[04/16 06:15:38] Re-training INFO: iter: 108125/300480  CE: 2.1487  
[04/16 06:16:42] Re-training INFO: iter: 108250/300480  CE: 2.2653  
[04/16 06:17:44] Re-training INFO: iter: 108375/300480  CE: 2.3537  
[04/16 06:18:48] Re-training INFO: iter: 108500/300480  CE: 2.2399  
[04/16 06:19:50] Re-training INFO: iter: 108625/300480  CE: 2.3030  
[04/16 06:20:53] Re-training INFO: iter: 108750/300480  CE: 2.1910  
[04/16 06:21:55] Re-training INFO: iter: 108875/300480  CE: 2.1871  
[04/16 06:22:18] Re-training INFO: --> epoch:  87/240  avg CE: 2.2498  lr: 0.18222947364789205  
[04/16 06:23:14] Re-training INFO: iter: 109000/300480  CE: 2.2930  
[04/16 06:24:18] Re-training INFO: iter: 109125/300480  CE: 2.2048  
[04/16 06:25:21] Re-training INFO: iter: 109250/300480  CE: 2.2806  
[04/16 06:26:24] Re-training INFO: iter: 109375/300480  CE: 2.1964  
[04/16 06:27:27] Re-training INFO: iter: 109500/300480  CE: 2.2591  
[04/16 06:28:29] Re-training INFO: iter: 109625/300480  CE: 2.0025  
[04/16 06:29:31] Re-training INFO: iter: 109750/300480  CE: 2.0898  
[04/16 06:30:33] Re-training INFO: iter: 109875/300480  CE: 2.4044  
[04/16 06:31:36] Re-training INFO: iter: 110000/300480  CE: 2.0143  
[04/16 06:32:39] Re-training INFO: iter: 110125/300480  CE: 2.0728  
[04/16 06:33:02] Re-training INFO: --> epoch:  88/240  avg CE: 2.2372  lr: 0.18222947364789205  
[04/16 06:33:58] Re-training INFO: iter: 110250/300480  CE: 2.2639  
[04/16 06:35:01] Re-training INFO: iter: 110375/300480  CE: 2.3864  
[04/16 06:36:04] Re-training INFO: iter: 110500/300480  CE: 2.3226  
[04/16 06:37:08] Re-training INFO: iter: 110625/300480  CE: 2.0760  
[04/16 06:38:10] Re-training INFO: iter: 110750/300480  CE: 2.2011  
[04/16 06:39:13] Re-training INFO: iter: 110875/300480  CE: 2.4471  
[04/16 06:40:17] Re-training INFO: iter: 111000/300480  CE: 2.1008  
[04/16 06:41:20] Re-training INFO: iter: 111125/300480  CE: 2.1945  
[04/16 06:42:22] Re-training INFO: iter: 111250/300480  CE: 2.0714  
[04/16 06:43:25] Re-training INFO: iter: 111375/300480  CE: 2.1665  
[04/16 06:43:51] Re-training INFO: --> epoch:  89/240  avg CE: 2.2367  lr: 0.17676258943845527  
[04/16 06:44:46] Re-training INFO: iter: 111500/300480  CE: 2.2684  
[04/16 06:45:50] Re-training INFO: iter: 111625/300480  CE: 2.0206  
[04/16 06:46:54] Re-training INFO: iter: 111750/300480  CE: 2.2567  
[04/16 06:47:56] Re-training INFO: iter: 111875/300480  CE: 2.1949  
[04/16 06:48:59] Re-training INFO: iter: 112000/300480  CE: 2.5520  
[04/16 06:50:02] Re-training INFO: iter: 112125/300480  CE: 2.1757  
[04/16 06:51:04] Re-training INFO: iter: 112250/300480  CE: 2.0281  
[04/16 06:52:06] Re-training INFO: iter: 112375/300480  CE: 2.0335  
[04/16 06:53:09] Re-training INFO: iter: 112500/300480  CE: 2.3130  
[04/16 06:54:10] Re-training INFO: iter: 112625/300480  CE: 2.1425  
[04/16 06:54:36] Re-training INFO: --> epoch:  90/240  avg CE: 2.2357  lr: 0.17676258943845527  
[04/16 06:55:30] Re-training INFO: iter: 112750/300480  CE: 2.5428  
[04/16 06:56:33] Re-training INFO: iter: 112875/300480  CE: 2.1610  
[04/16 06:57:37] Re-training INFO: iter: 113000/300480  CE: 2.6440  
[04/16 06:58:41] Re-training INFO: iter: 113125/300480  CE: 2.1311  
[04/16 06:59:43] Re-training INFO: iter: 113250/300480  CE: 2.1840  
[04/16 07:00:46] Re-training INFO: iter: 113375/300480  CE: 2.2061  
[04/16 07:01:48] Re-training INFO: iter: 113500/300480  CE: 2.2114  
[04/16 07:02:51] Re-training INFO: iter: 113625/300480  CE: 2.1398  
[04/16 07:03:54] Re-training INFO: iter: 113750/300480  CE: 2.4847  
[04/16 07:04:57] Re-training INFO: iter: 113875/300480  CE: 2.3734  
[04/16 07:05:24] Re-training INFO: --> epoch:  91/240  avg CE: 2.2340  lr: 0.17676258943845527  
[04/16 07:06:16] Re-training INFO: iter: 114000/300480  CE: 2.0751  
[04/16 07:07:19] Re-training INFO: iter: 114125/300480  CE: 2.0826  
[04/16 07:08:22] Re-training INFO: iter: 114250/300480  CE: 2.2065  
[04/16 07:09:26] Re-training INFO: iter: 114375/300480  CE: 2.0881  
[04/16 07:10:29] Re-training INFO: iter: 114500/300480  CE: 2.3039  
[04/16 07:11:31] Re-training INFO: iter: 114625/300480  CE: 2.3220  
[04/16 07:12:33] Re-training INFO: iter: 114750/300480  CE: 2.4593  
[04/16 07:13:36] Re-training INFO: iter: 114875/300480  CE: 1.9811  
[04/16 07:14:39] Re-training INFO: iter: 115000/300480  CE: 2.1468  
[04/16 07:15:42] Re-training INFO: iter: 115125/300480  CE: 2.2173  
[04/16 07:16:10] Re-training INFO: --> epoch:  92/240  avg CE: 2.2249  lr: 0.1714597117553016  
[04/16 07:17:02] Re-training INFO: iter: 115250/300480  CE: 2.2207  
[04/16 07:18:05] Re-training INFO: iter: 115375/300480  CE: 2.2024  
[04/16 07:19:07] Re-training INFO: iter: 115500/300480  CE: 2.4236  
[04/16 07:20:10] Re-training INFO: iter: 115625/300480  CE: 2.2005  
[04/16 07:21:13] Re-training INFO: iter: 115750/300480  CE: 2.2556  
[04/16 07:22:16] Re-training INFO: iter: 115875/300480  CE: 2.1258  
[04/16 07:23:18] Re-training INFO: iter: 116000/300480  CE: 2.4478  
[04/16 07:24:21] Re-training INFO: iter: 116125/300480  CE: 2.2756  
[04/16 07:25:23] Re-training INFO: iter: 116250/300480  CE: 2.1545  
[04/16 07:26:25] Re-training INFO: iter: 116375/300480  CE: 2.2239  
[04/16 07:26:54] Re-training INFO: --> epoch:  93/240  avg CE: 2.2251  lr: 0.1714597117553016  
[04/16 07:27:44] Re-training INFO: iter: 116500/300480  CE: 2.2993  
[04/16 07:28:47] Re-training INFO: iter: 116625/300480  CE: 2.2378  
[04/16 07:29:49] Re-training INFO: iter: 116750/300480  CE: 1.9016  
[04/16 07:30:51] Re-training INFO: iter: 116875/300480  CE: 2.2911  
[04/16 07:31:55] Re-training INFO: iter: 117000/300480  CE: 2.2305  
[04/16 07:33:00] Re-training INFO: iter: 117125/300480  CE: 2.2183  
[04/16 07:34:01] Re-training INFO: iter: 117250/300480  CE: 2.3864  
[04/16 07:35:03] Re-training INFO: iter: 117375/300480  CE: 2.1110  
[04/16 07:36:06] Re-training INFO: iter: 117500/300480  CE: 2.4130  
[04/16 07:37:09] Re-training INFO: iter: 117625/300480  CE: 2.1361  
[04/16 07:37:38] Re-training INFO: --> epoch:  94/240  avg CE: 2.2281  lr: 0.16631592040264256  
[04/16 07:38:27] Re-training INFO: iter: 117750/300480  CE: 1.9855  
[04/16 07:39:31] Re-training INFO: iter: 117875/300480  CE: 2.0785  
[04/16 07:40:34] Re-training INFO: iter: 118000/300480  CE: 2.2940  
[04/16 07:41:38] Re-training INFO: iter: 118125/300480  CE: 2.1765  
[04/16 07:42:40] Re-training INFO: iter: 118250/300480  CE: 2.2534  
[04/16 07:43:44] Re-training INFO: iter: 118375/300480  CE: 1.9944  
[04/16 07:44:47] Re-training INFO: iter: 118500/300480  CE: 2.2487  
[04/16 07:45:50] Re-training INFO: iter: 118625/300480  CE: 2.3492  
[04/16 07:46:52] Re-training INFO: iter: 118750/300480  CE: 2.0990  
[04/16 07:47:54] Re-training INFO: iter: 118875/300480  CE: 2.0435  
[04/16 07:48:25] Re-training INFO: --> epoch:  95/240  avg CE: 2.2164  lr: 0.16631592040264256  
[04/16 07:49:13] Re-training INFO: iter: 119000/300480  CE: 1.9955  
[04/16 07:50:17] Re-training INFO: iter: 119125/300480  CE: 2.2017  
[04/16 07:51:20] Re-training INFO: iter: 119250/300480  CE: 2.3106  
[04/16 07:52:22] Re-training INFO: iter: 119375/300480  CE: 2.3998  
[04/16 07:53:25] Re-training INFO: iter: 119500/300480  CE: 2.1844  
[04/16 07:54:28] Re-training INFO: iter: 119625/300480  CE: 2.1739  
[04/16 07:55:31] Re-training INFO: iter: 119750/300480  CE: 2.2392  
[04/16 07:56:34] Re-training INFO: iter: 119875/300480  CE: 2.2353  
[04/16 07:57:37] Re-training INFO: iter: 120000/300480  CE: 2.3993  
[04/16 07:58:40] Re-training INFO: iter: 120125/300480  CE: 2.3810  
[04/16 07:59:13] Re-training INFO: --> epoch:  96/240  avg CE: 2.2191  lr: 0.16132644279056327  
[04/16 08:00:00] Re-training INFO: iter: 120250/300480  CE: 2.3217  
[04/16 08:01:04] Re-training INFO: iter: 120375/300480  CE: 2.2938  
[04/16 08:02:07] Re-training INFO: iter: 120500/300480  CE: 2.5355  
[04/16 08:03:10] Re-training INFO: iter: 120625/300480  CE: 2.2561  
[04/16 08:04:13] Re-training INFO: iter: 120750/300480  CE: 2.0797  
[04/16 08:05:16] Re-training INFO: iter: 120875/300480  CE: 2.2832  
[04/16 08:06:19] Re-training INFO: iter: 121000/300480  CE: 1.9996  
[04/16 08:07:22] Re-training INFO: iter: 121125/300480  CE: 2.1778  
[04/16 08:08:25] Re-training INFO: iter: 121250/300480  CE: 2.1909  
[04/16 08:09:27] Re-training INFO: iter: 121375/300480  CE: 2.2824  
[04/16 08:10:00] Re-training INFO: --> epoch:  97/240  avg CE: 2.2066  lr: 0.16132644279056327  
[04/16 08:10:47] Re-training INFO: iter: 121500/300480  CE: 2.0135  
[04/16 08:11:50] Re-training INFO: iter: 121625/300480  CE: 2.3736  
[04/16 08:12:53] Re-training INFO: iter: 121750/300480  CE: 2.1986  
[04/16 08:13:56] Re-training INFO: iter: 121875/300480  CE: 2.1407  
[04/16 08:15:00] Re-training INFO: iter: 122000/300480  CE: 2.3077  
[04/16 08:16:03] Re-training INFO: iter: 122125/300480  CE: 2.3349  
[04/16 08:17:06] Re-training INFO: iter: 122250/300480  CE: 2.2801  
[04/16 08:18:09] Re-training INFO: iter: 122375/300480  CE: 2.4501  
[04/16 08:19:11] Re-training INFO: iter: 122500/300480  CE: 1.9843  
[04/16 08:20:13] Re-training INFO: iter: 122625/300480  CE: 2.2054  
[04/16 08:20:47] Re-training INFO: --> epoch:  98/240  avg CE: 2.2095  lr: 0.16132644279056327  
[04/16 08:21:32] Re-training INFO: iter: 122750/300480  CE: 2.2328  
[04/16 08:22:35] Re-training INFO: iter: 122875/300480  CE: 2.1428  
[04/16 08:23:38] Re-training INFO: iter: 123000/300480  CE: 2.1351  
[04/16 08:24:42] Re-training INFO: iter: 123125/300480  CE: 2.2026  
[04/16 08:25:45] Re-training INFO: iter: 123250/300480  CE: 2.1902  
[04/16 08:26:48] Re-training INFO: iter: 123375/300480  CE: 2.2010  
[04/16 08:27:51] Re-training INFO: iter: 123500/300480  CE: 2.3199  
[04/16 08:28:55] Re-training INFO: iter: 123625/300480  CE: 2.2688  
[04/16 08:29:58] Re-training INFO: iter: 123750/300480  CE: 2.3581  
[04/16 08:31:01] Re-training INFO: iter: 123875/300480  CE: 2.1626  
[04/16 08:31:37] Re-training INFO: --> epoch:  99/240  avg CE: 2.2045  lr: 0.15648664950684638  
[04/16 08:32:21] Re-training INFO: iter: 124000/300480  CE: 2.2716  
[04/16 08:33:24] Re-training INFO: iter: 124125/300480  CE: 2.1516  
[04/16 08:34:27] Re-training INFO: iter: 124250/300480  CE: 2.0493  
[04/16 08:35:31] Re-training INFO: iter: 124375/300480  CE: 2.1819  
[04/16 08:36:34] Re-training INFO: iter: 124500/300480  CE: 2.1975  
[04/16 08:37:37] Re-training INFO: iter: 124625/300480  CE: 2.1599  
[04/16 08:38:39] Re-training INFO: iter: 124750/300480  CE: 2.5418  
[04/16 08:39:43] Re-training INFO: iter: 124875/300480  CE: 2.3211  
[04/16 08:40:46] Re-training INFO: iter: 125000/300480  CE: 2.2211  
[04/16 08:41:48] Re-training INFO: iter: 125125/300480  CE: 2.1298  
[04/16 08:42:25] Re-training INFO: --> epoch: 100/240  avg CE: 2.2003  lr: 0.15648664950684638  
[04/16 08:42:59] Re-training INFO: # of Test Samples: 50000.0
[04/16 08:42:59] Re-training INFO: Top-1/-5 acc: 69.98 / 89.46
[04/16 08:42:59] Re-training INFO: Top-1/-5 acc: 30.02 / 10.54
[04/16 08:42:59] Re-training INFO: 

[04/16 08:43:42] Re-training INFO: iter: 125250/300480  CE: 2.2416  
[04/16 08:44:45] Re-training INFO: iter: 125375/300480  CE: 2.2102  
[04/16 08:45:49] Re-training INFO: iter: 125500/300480  CE: 2.2128  
[04/16 08:46:51] Re-training INFO: iter: 125625/300480  CE: 2.2087  
[04/16 08:47:54] Re-training INFO: iter: 125750/300480  CE: 2.0215  
[04/16 08:48:57] Re-training INFO: iter: 125875/300480  CE: 2.2228  
[04/16 08:50:01] Re-training INFO: iter: 126000/300480  CE: 2.3078  
[04/16 08:51:03] Re-training INFO: iter: 126125/300480  CE: 2.2396  
[04/16 08:52:06] Re-training INFO: iter: 126250/300480  CE: 1.9544  
[04/16 08:53:10] Re-training INFO: iter: 126375/300480  CE: 2.2559  
[04/16 08:53:47] Re-training INFO: --> epoch: 101/240  avg CE: 2.2024  lr: 0.15179205002164095  
[04/16 08:54:29] Re-training INFO: iter: 126500/300480  CE: 2.1151  
[04/16 08:55:31] Re-training INFO: iter: 126625/300480  CE: 2.2475  
[04/16 08:56:34] Re-training INFO: iter: 126750/300480  CE: 2.1444  
[04/16 08:57:35] Re-training INFO: iter: 126875/300480  CE: 2.2499  
[04/16 08:58:39] Re-training INFO: iter: 127000/300480  CE: 2.1745  
[04/16 08:59:41] Re-training INFO: iter: 127125/300480  CE: 2.2132  
[04/16 09:00:43] Re-training INFO: iter: 127250/300480  CE: 2.3405  
[04/16 09:01:46] Re-training INFO: iter: 127375/300480  CE: 2.0572  
[04/16 09:02:49] Re-training INFO: iter: 127500/300480  CE: 2.2157  
[04/16 09:03:51] Re-training INFO: iter: 127625/300480  CE: 2.2036  
[04/16 09:04:29] Re-training INFO: --> epoch: 102/240  avg CE: 2.1959  lr: 0.15179205002164095  
[04/16 09:05:10] Re-training INFO: iter: 127750/300480  CE: 2.2285  
[04/16 09:06:13] Re-training INFO: iter: 127875/300480  CE: 2.0521  
[04/16 09:07:15] Re-training INFO: iter: 128000/300480  CE: 2.3411  
[04/16 09:08:17] Re-training INFO: iter: 128125/300480  CE: 2.1937  
[04/16 09:09:19] Re-training INFO: iter: 128250/300480  CE: 2.2921  
[04/16 09:10:22] Re-training INFO: iter: 128375/300480  CE: 2.1344  
[04/16 09:11:24] Re-training INFO: iter: 128500/300480  CE: 2.4137  
[04/16 09:12:27] Re-training INFO: iter: 128625/300480  CE: 2.3067  
[04/16 09:13:29] Re-training INFO: iter: 128750/300480  CE: 2.2763  
[04/16 09:14:31] Re-training INFO: iter: 128875/300480  CE: 2.3633  
[04/16 09:15:09] Re-training INFO: --> epoch: 103/240  avg CE: 2.1901  lr: 0.15179205002164095  
[04/16 09:15:50] Re-training INFO: iter: 129000/300480  CE: 2.2369  
[04/16 09:16:53] Re-training INFO: iter: 129125/300480  CE: 2.2126  
[04/16 09:17:56] Re-training INFO: iter: 129250/300480  CE: 1.9969  
[04/16 09:18:59] Re-training INFO: iter: 129375/300480  CE: 2.3715  
[04/16 09:20:01] Re-training INFO: iter: 129500/300480  CE: 2.4211  
[04/16 09:21:04] Re-training INFO: iter: 129625/300480  CE: 2.1454  
[04/16 09:22:08] Re-training INFO: iter: 129750/300480  CE: 1.9885  
[04/16 09:23:11] Re-training INFO: iter: 129875/300480  CE: 2.0548  
[04/16 09:24:13] Re-training INFO: iter: 130000/300480  CE: 2.2137  
[04/16 09:25:16] Re-training INFO: iter: 130125/300480  CE: 2.4326  
[04/16 09:25:56] Re-training INFO: --> epoch: 104/240  avg CE: 2.1894  lr: 0.14723828852099174  
[04/16 09:26:34] Re-training INFO: iter: 130250/300480  CE: 2.0302  
[04/16 09:27:37] Re-training INFO: iter: 130375/300480  CE: 2.3201  
[04/16 09:28:40] Re-training INFO: iter: 130500/300480  CE: 2.1813  
[04/16 09:29:43] Re-training INFO: iter: 130625/300480  CE: 2.1073  
[04/16 09:30:45] Re-training INFO: iter: 130750/300480  CE: 2.0760  
[04/16 09:31:47] Re-training INFO: iter: 130875/300480  CE: 2.1265  
[04/16 09:32:49] Re-training INFO: iter: 131000/300480  CE: 2.2784  
[04/16 09:33:52] Re-training INFO: iter: 131125/300480  CE: 2.1941  
[04/16 09:34:54] Re-training INFO: iter: 131250/300480  CE: 2.2309  
[04/16 09:35:57] Re-training INFO: iter: 131375/300480  CE: 2.3242  
[04/16 09:36:38] Re-training INFO: --> epoch: 105/240  avg CE: 2.1880  lr: 0.14723828852099174  
[04/16 09:37:16] Re-training INFO: iter: 131500/300480  CE: 2.1719  
[04/16 09:38:20] Re-training INFO: iter: 131625/300480  CE: 2.2695  
[04/16 09:39:23] Re-training INFO: iter: 131750/300480  CE: 2.1467  
[04/16 09:40:27] Re-training INFO: iter: 131875/300480  CE: 2.1618  
[04/16 09:41:32] Re-training INFO: iter: 132000/300480  CE: 2.1898  
[04/16 09:42:35] Re-training INFO: iter: 132125/300480  CE: 1.8902  
[04/16 09:43:39] Re-training INFO: iter: 132250/300480  CE: 2.1712  
[04/16 09:44:43] Re-training INFO: iter: 132375/300480  CE: 2.3860  
[04/16 09:45:47] Re-training INFO: iter: 132500/300480  CE: 2.1471  
[04/16 09:46:51] Re-training INFO: iter: 132625/300480  CE: 1.9037  
[04/16 09:47:33] Re-training INFO: --> epoch: 106/240  avg CE: 2.1814  lr: 0.14282113986536196  
[04/16 09:48:10] Re-training INFO: iter: 132750/300480  CE: 2.2801  
[04/16 09:49:13] Re-training INFO: iter: 132875/300480  CE: 2.0340  
[04/16 09:50:16] Re-training INFO: iter: 133000/300480  CE: 2.0545  
[04/16 09:51:20] Re-training INFO: iter: 133125/300480  CE: 2.2118  
[04/16 09:52:23] Re-training INFO: iter: 133250/300480  CE: 2.0918  
[04/16 09:53:25] Re-training INFO: iter: 133375/300480  CE: 2.2423  
[04/16 09:54:28] Re-training INFO: iter: 133500/300480  CE: 2.2972  
[04/16 09:55:30] Re-training INFO: iter: 133625/300480  CE: 2.0774  
[04/16 09:56:33] Re-training INFO: iter: 133750/300480  CE: 2.1785  
[04/16 09:57:36] Re-training INFO: iter: 133875/300480  CE: 2.3952  
[04/16 09:58:19] Re-training INFO: --> epoch: 107/240  avg CE: 2.1819  lr: 0.14282113986536196  
[04/16 09:58:55] Re-training INFO: iter: 134000/300480  CE: 2.2433  
[04/16 09:59:57] Re-training INFO: iter: 134125/300480  CE: 2.1962  
[04/16 10:01:01] Re-training INFO: iter: 134250/300480  CE: 2.1813  
[04/16 10:02:04] Re-training INFO: iter: 134375/300480  CE: 2.0395  
[04/16 10:03:06] Re-training INFO: iter: 134500/300480  CE: 1.9704  
[04/16 10:04:09] Re-training INFO: iter: 134625/300480  CE: 2.0673  
[04/16 10:05:12] Re-training INFO: iter: 134750/300480  CE: 1.9490  
[04/16 10:06:14] Re-training INFO: iter: 134875/300480  CE: 2.2740  
[04/16 10:07:16] Re-training INFO: iter: 135000/300480  CE: 2.4402  
[04/16 10:08:18] Re-training INFO: iter: 135125/300480  CE: 2.3844  
[04/16 10:09:01] Re-training INFO: --> epoch: 108/240  avg CE: 2.1812  lr: 0.13853650566940112  
[04/16 10:09:36] Re-training INFO: iter: 135250/300480  CE: 2.0769  
[04/16 10:10:39] Re-training INFO: iter: 135375/300480  CE: 2.1034  
[04/16 10:11:41] Re-training INFO: iter: 135500/300480  CE: 2.3335  
[04/16 10:12:44] Re-training INFO: iter: 135625/300480  CE: 2.2388  
[04/16 10:13:48] Re-training INFO: iter: 135750/300480  CE: 2.1901  
[04/16 10:14:49] Re-training INFO: iter: 135875/300480  CE: 2.1622  
[04/16 10:15:53] Re-training INFO: iter: 136000/300480  CE: 2.0857  
[04/16 10:16:55] Re-training INFO: iter: 136125/300480  CE: 2.1194  
[04/16 10:17:58] Re-training INFO: iter: 136250/300480  CE: 2.4373  
[04/16 10:19:01] Re-training INFO: iter: 136375/300480  CE: 2.2296  
[04/16 10:19:47] Re-training INFO: --> epoch: 109/240  avg CE: 2.1796  lr: 0.13853650566940112  
[04/16 10:20:21] Re-training INFO: iter: 136500/300480  CE: 2.0998  
[04/16 10:21:24] Re-training INFO: iter: 136625/300480  CE: 1.9595  
[04/16 10:22:27] Re-training INFO: iter: 136750/300480  CE: 1.9605  
[04/16 10:23:30] Re-training INFO: iter: 136875/300480  CE: 2.2318  
[04/16 10:24:32] Re-training INFO: iter: 137000/300480  CE: 2.2205  
[04/16 10:25:35] Re-training INFO: iter: 137125/300480  CE: 2.0148  
[04/16 10:26:38] Re-training INFO: iter: 137250/300480  CE: 2.3653  
[04/16 10:27:41] Re-training INFO: iter: 137375/300480  CE: 2.0633  
[04/16 10:28:43] Re-training INFO: iter: 137500/300480  CE: 2.0937  
[04/16 10:29:45] Re-training INFO: iter: 137625/300480  CE: 2.2704  
[04/16 10:30:31] Re-training INFO: --> epoch: 110/240  avg CE: 2.1744  lr: 0.13853650566940112  
[04/16 10:31:04] Re-training INFO: iter: 137750/300480  CE: 2.2854  
[04/16 10:32:09] Re-training INFO: iter: 137875/300480  CE: 2.0365  
[04/16 10:33:12] Re-training INFO: iter: 138000/300480  CE: 2.3520  
[04/16 10:34:15] Re-training INFO: iter: 138125/300480  CE: 2.3626  
[04/16 10:35:17] Re-training INFO: iter: 138250/300480  CE: 2.1005  
[04/16 10:36:21] Re-training INFO: iter: 138375/300480  CE: 1.9650  
[04/16 10:37:25] Re-training INFO: iter: 138500/300480  CE: 2.2175  
[04/16 10:38:28] Re-training INFO: iter: 138625/300480  CE: 2.2460  
[04/16 10:39:32] Re-training INFO: iter: 138750/300480  CE: 2.0821  
[04/16 10:40:35] Re-training INFO: iter: 138875/300480  CE: 2.1792  
[04/16 10:41:23] Re-training INFO: --> epoch: 111/240  avg CE: 2.1739  lr: 0.1343804104993191  
[04/16 10:41:55] Re-training INFO: iter: 139000/300480  CE: 2.0787  
[04/16 10:42:58] Re-training INFO: iter: 139125/300480  CE: 2.1217  
[04/16 10:44:01] Re-training INFO: iter: 139250/300480  CE: 2.1423  
[04/16 10:45:04] Re-training INFO: iter: 139375/300480  CE: 2.1676  
[04/16 10:46:08] Re-training INFO: iter: 139500/300480  CE: 2.4199  
[04/16 10:47:10] Re-training INFO: iter: 139625/300480  CE: 2.2674  
[04/16 10:48:13] Re-training INFO: iter: 139750/300480  CE: 2.0125  
[04/16 10:49:15] Re-training INFO: iter: 139875/300480  CE: 2.3032  
[04/16 10:50:17] Re-training INFO: iter: 140000/300480  CE: 2.0764  
[04/16 10:51:20] Re-training INFO: iter: 140125/300480  CE: 2.1933  
[04/16 10:52:07] Re-training INFO: --> epoch: 112/240  avg CE: 2.1635  lr: 0.1343804104993191  
[04/16 10:52:37] Re-training INFO: iter: 140250/300480  CE: 2.1299  
[04/16 10:53:42] Re-training INFO: iter: 140375/300480  CE: 2.1402  
[04/16 10:54:45] Re-training INFO: iter: 140500/300480  CE: 2.0202  
[04/16 10:55:49] Re-training INFO: iter: 140625/300480  CE: 2.1843  
[04/16 10:56:52] Re-training INFO: iter: 140750/300480  CE: 2.2305  
[04/16 10:57:54] Re-training INFO: iter: 140875/300480  CE: 1.9009  
[04/16 10:58:57] Re-training INFO: iter: 141000/300480  CE: 2.0624  
[04/16 10:59:59] Re-training INFO: iter: 141125/300480  CE: 2.0119  
[04/16 11:01:02] Re-training INFO: iter: 141250/300480  CE: 2.3429  
[04/16 11:02:04] Re-training INFO: iter: 141375/300480  CE: 2.1751  
[04/16 11:02:53] Re-training INFO: --> epoch: 113/240  avg CE: 2.1674  lr: 0.1303489981843395  
[04/16 11:03:23] Re-training INFO: iter: 141500/300480  CE: 2.2840  
[04/16 11:04:27] Re-training INFO: iter: 141625/300480  CE: 2.2006  
[04/16 11:05:31] Re-training INFO: iter: 141750/300480  CE: 2.2173  
[04/16 11:06:34] Re-training INFO: iter: 141875/300480  CE: 2.1731  
[04/16 11:07:37] Re-training INFO: iter: 142000/300480  CE: 2.1376  
[04/16 11:08:40] Re-training INFO: iter: 142125/300480  CE: 2.1667  
[04/16 11:09:43] Re-training INFO: iter: 142250/300480  CE: 1.8981  
[04/16 11:10:46] Re-training INFO: iter: 142375/300480  CE: 2.0815  
[04/16 11:11:50] Re-training INFO: iter: 142500/300480  CE: 2.2220  
[04/16 11:12:53] Re-training INFO: iter: 142625/300480  CE: 2.1625  
[04/16 11:13:43] Re-training INFO: --> epoch: 114/240  avg CE: 2.1602  lr: 0.1303489981843395  
[04/16 11:14:13] Re-training INFO: iter: 142750/300480  CE: 1.9760  
[04/16 11:15:17] Re-training INFO: iter: 142875/300480  CE: 2.2309  
[04/16 11:16:21] Re-training INFO: iter: 143000/300480  CE: 2.2026  
[04/16 11:17:25] Re-training INFO: iter: 143125/300480  CE: 2.2580  
[04/16 11:18:28] Re-training INFO: iter: 143250/300480  CE: 2.3107  
[04/16 11:19:31] Re-training INFO: iter: 143375/300480  CE: 2.1057  
[04/16 11:20:35] Re-training INFO: iter: 143500/300480  CE: 2.0785  
[04/16 11:21:38] Re-training INFO: iter: 143625/300480  CE: 2.1284  
[04/16 11:22:41] Re-training INFO: iter: 143750/300480  CE: 2.0192  
[04/16 11:23:44] Re-training INFO: iter: 143875/300480  CE: 2.1030  
[04/16 11:24:34] Re-training INFO: --> epoch: 115/240  avg CE: 2.1588  lr: 0.1303489981843395  
[04/16 11:25:03] Re-training INFO: iter: 144000/300480  CE: 2.1877  
[04/16 11:26:06] Re-training INFO: iter: 144125/300480  CE: 2.1633  
[04/16 11:27:11] Re-training INFO: iter: 144250/300480  CE: 2.0808  
[04/16 11:28:14] Re-training INFO: iter: 144375/300480  CE: 2.0600  
[04/16 11:29:17] Re-training INFO: iter: 144500/300480  CE: 2.0221  
[04/16 11:30:20] Re-training INFO: iter: 144625/300480  CE: 2.1975  
[04/16 11:31:23] Re-training INFO: iter: 144750/300480  CE: 2.2217  
[04/16 11:32:26] Re-training INFO: iter: 144875/300480  CE: 2.0115  
[04/16 11:33:29] Re-training INFO: iter: 145000/300480  CE: 2.1957  
[04/16 11:34:32] Re-training INFO: iter: 145125/300480  CE: 2.0394  
[04/16 11:35:25] Re-training INFO: --> epoch: 116/240  avg CE: 2.1534  lr: 0.12643852823880933  
[04/16 11:35:52] Re-training INFO: iter: 145250/300480  CE: 2.1159  
[04/16 11:36:56] Re-training INFO: iter: 145375/300480  CE: 2.1419  
[04/16 11:38:00] Re-training INFO: iter: 145500/300480  CE: 2.0964  
[04/16 11:39:03] Re-training INFO: iter: 145625/300480  CE: 2.2954  
[04/16 11:40:06] Re-training INFO: iter: 145750/300480  CE: 2.1803  
[04/16 11:41:11] Re-training INFO: iter: 145875/300480  CE: 2.1525  
[04/16 11:42:13] Re-training INFO: iter: 146000/300480  CE: 2.0931  
[04/16 11:43:16] Re-training INFO: iter: 146125/300480  CE: 2.2429  
[04/16 11:44:20] Re-training INFO: iter: 146250/300480  CE: 1.8664  
[04/16 11:45:24] Re-training INFO: iter: 146375/300480  CE: 1.9923  
[04/16 11:46:18] Re-training INFO: --> epoch: 117/240  avg CE: 2.1497  lr: 0.12643852823880933  
[04/16 11:46:44] Re-training INFO: iter: 146500/300480  CE: 2.3611  
[04/16 11:47:49] Re-training INFO: iter: 146625/300480  CE: 2.2192  
[04/16 11:48:52] Re-training INFO: iter: 146750/300480  CE: 2.6427  
[04/16 11:49:55] Re-training INFO: iter: 146875/300480  CE: 2.2422  
[04/16 11:50:59] Re-training INFO: iter: 147000/300480  CE: 2.2908  
[04/16 11:52:02] Re-training INFO: iter: 147125/300480  CE: 2.2699  
[04/16 11:53:06] Re-training INFO: iter: 147250/300480  CE: 1.9569  
[04/16 11:54:10] Re-training INFO: iter: 147375/300480  CE: 2.0030  
[04/16 11:55:13] Re-training INFO: iter: 147500/300480  CE: 2.0999  
[04/16 11:56:16] Re-training INFO: iter: 147625/300480  CE: 2.0475  
[04/16 11:57:10] Re-training INFO: --> epoch: 118/240  avg CE: 2.1505  lr: 0.12264537239164504  
[04/16 11:57:35] Re-training INFO: iter: 147750/300480  CE: 2.1302  
[04/16 11:58:39] Re-training INFO: iter: 147875/300480  CE: 2.2779  
[04/16 11:59:42] Re-training INFO: iter: 148000/300480  CE: 2.1008  
[04/16 12:00:47] Re-training INFO: iter: 148125/300480  CE: 2.1020  
[04/16 12:01:51] Re-training INFO: iter: 148250/300480  CE: 2.1478  
[04/16 12:02:54] Re-training INFO: iter: 148375/300480  CE: 2.0535  
[04/16 12:03:58] Re-training INFO: iter: 148500/300480  CE: 2.1756  
[04/16 12:05:02] Re-training INFO: iter: 148625/300480  CE: 2.3195  
[04/16 12:06:06] Re-training INFO: iter: 148750/300480  CE: 1.9293  
[04/16 12:07:10] Re-training INFO: iter: 148875/300480  CE: 2.0091  
[04/16 12:08:06] Re-training INFO: --> epoch: 119/240  avg CE: 2.1481  lr: 0.12264537239164504  
[04/16 12:08:30] Re-training INFO: iter: 149000/300480  CE: 2.1095  
[04/16 12:09:35] Re-training INFO: iter: 149125/300480  CE: 2.1262  
[04/16 12:10:39] Re-training INFO: iter: 149250/300480  CE: 1.9329  
[04/16 12:11:43] Re-training INFO: iter: 149375/300480  CE: 2.0901  
[04/16 12:12:47] Re-training INFO: iter: 149500/300480  CE: 2.0953  
[04/16 12:13:51] Re-training INFO: iter: 149625/300480  CE: 2.3116  
[04/16 12:14:54] Re-training INFO: iter: 149750/300480  CE: 2.1004  
[04/16 12:15:58] Re-training INFO: iter: 149875/300480  CE: 2.2842  
[04/16 12:17:02] Re-training INFO: iter: 150000/300480  CE: 2.1801  
[04/16 12:18:06] Re-training INFO: iter: 150125/300480  CE: 1.9315  
[04/16 12:19:03] Re-training INFO: --> epoch: 120/240  avg CE: 2.1454  lr: 0.11896601121989568  
[04/16 12:19:37] Re-training INFO: # of Test Samples: 50000.0
[04/16 12:19:37] Re-training INFO: Top-1/-5 acc: 70.84 / 89.98
[04/16 12:19:37] Re-training INFO: Top-1/-5 acc: 29.16 / 10.02
[04/16 12:19:37] Re-training INFO: 

[04/16 12:20:00] Re-training INFO: iter: 150250/300480  CE: 2.1403  
[04/16 12:21:03] Re-training INFO: iter: 150375/300480  CE: 2.2482  
[04/16 12:22:07] Re-training INFO: iter: 150500/300480  CE: 2.1363  
[04/16 12:23:12] Re-training INFO: iter: 150625/300480  CE: 2.0743  
[04/16 12:24:15] Re-training INFO: iter: 150750/300480  CE: 2.1966  
[04/16 12:25:20] Re-training INFO: iter: 150875/300480  CE: 2.1356  
[04/16 12:26:23] Re-training INFO: iter: 151000/300480  CE: 2.2325  
[04/16 12:27:28] Re-training INFO: iter: 151125/300480  CE: 2.2074  
[04/16 12:28:30] Re-training INFO: iter: 151250/300480  CE: 2.0881  
[04/16 12:29:34] Re-training INFO: iter: 151375/300480  CE: 2.2353  
[04/16 12:30:32] Re-training INFO: --> epoch: 121/240  avg CE: 2.1367  lr: 0.11896601121989568  
[04/16 12:30:54] Re-training INFO: iter: 151500/300480  CE: 2.2392  
[04/16 12:31:59] Re-training INFO: iter: 151625/300480  CE: 1.9875  
[04/16 12:33:03] Re-training INFO: iter: 151750/300480  CE: 2.1407  
[04/16 12:34:08] Re-training INFO: iter: 151875/300480  CE: 1.8953  
[04/16 12:35:11] Re-training INFO: iter: 152000/300480  CE: 2.1398  
[04/16 12:36:15] Re-training INFO: iter: 152125/300480  CE: 2.3247  
[04/16 12:37:18] Re-training INFO: iter: 152250/300480  CE: 2.1656  
[04/16 12:38:21] Re-training INFO: iter: 152375/300480  CE: 2.1479  
[04/16 12:39:24] Re-training INFO: iter: 152500/300480  CE: 2.0598  
[04/16 12:40:27] Re-training INFO: iter: 152625/300480  CE: 1.7142  
[04/16 12:41:25] Re-training INFO: --> epoch: 122/240  avg CE: 2.1336  lr: 0.11896601121989568  
[04/16 12:41:45] Re-training INFO: iter: 152750/300480  CE: 2.1413  
[04/16 12:42:50] Re-training INFO: iter: 152875/300480  CE: 2.0046  
[04/16 12:43:54] Re-training INFO: iter: 153000/300480  CE: 2.2205  
[04/16 12:44:58] Re-training INFO: iter: 153125/300480  CE: 2.0950  
[04/16 12:46:01] Re-training INFO: iter: 153250/300480  CE: 2.1650  
[04/16 12:47:04] Re-training INFO: iter: 153375/300480  CE: 2.0514  
[04/16 12:48:06] Re-training INFO: iter: 153500/300480  CE: 1.9845  
[04/16 12:49:10] Re-training INFO: iter: 153625/300480  CE: 2.1421  
[04/16 12:50:13] Re-training INFO: iter: 153750/300480  CE: 2.2803  
[04/16 12:51:15] Re-training INFO: iter: 153875/300480  CE: 2.3675  
[04/16 12:52:15] Re-training INFO: --> epoch: 123/240  avg CE: 2.1329  lr: 0.1153970308832988  
[04/16 12:52:36] Re-training INFO: iter: 154000/300480  CE: 2.1454  
[04/16 12:53:40] Re-training INFO: iter: 154125/300480  CE: 1.9891  
[04/16 12:54:43] Re-training INFO: iter: 154250/300480  CE: 2.0727  
[04/16 12:55:47] Re-training INFO: iter: 154375/300480  CE: 2.3097  
[04/16 12:56:51] Re-training INFO: iter: 154500/300480  CE: 2.1102  
[04/16 12:57:55] Re-training INFO: iter: 154625/300480  CE: 2.2088  
[04/16 12:58:58] Re-training INFO: iter: 154750/300480  CE: 2.1503  
[04/16 13:00:01] Re-training INFO: iter: 154875/300480  CE: 2.1395  
[04/16 13:01:03] Re-training INFO: iter: 155000/300480  CE: 2.4930  
[04/16 13:02:08] Re-training INFO: iter: 155125/300480  CE: 2.2329  
[04/16 13:03:08] Re-training INFO: --> epoch: 124/240  avg CE: 2.1391  lr: 0.1153970308832988  
[04/16 13:03:27] Re-training INFO: iter: 155250/300480  CE: 2.2645  
[04/16 13:04:32] Re-training INFO: iter: 155375/300480  CE: 2.0956  
[04/16 13:05:35] Re-training INFO: iter: 155500/300480  CE: 1.9735  
[04/16 13:06:38] Re-training INFO: iter: 155625/300480  CE: 2.3264  
[04/16 13:07:43] Re-training INFO: iter: 155750/300480  CE: 2.0794  
[04/16 13:08:47] Re-training INFO: iter: 155875/300480  CE: 2.3756  
[04/16 13:09:51] Re-training INFO: iter: 156000/300480  CE: 2.0695  
[04/16 13:10:55] Re-training INFO: iter: 156125/300480  CE: 1.9707  
[04/16 13:11:59] Re-training INFO: iter: 156250/300480  CE: 2.2596  
[04/16 13:13:02] Re-training INFO: iter: 156375/300480  CE: 2.2665  
[04/16 13:14:04] Re-training INFO: iter: 156500/300480  CE: 2.2616  
[04/16 13:14:04] Re-training INFO: --> epoch: 125/240  avg CE: 2.1356  lr: 0.11193511995679983  
[04/16 13:15:26] Re-training INFO: iter: 156625/300480  CE: 2.0850  
[04/16 13:16:30] Re-training INFO: iter: 156750/300480  CE: 1.9183  
[04/16 13:17:34] Re-training INFO: iter: 156875/300480  CE: 2.1696  
[04/16 13:18:38] Re-training INFO: iter: 157000/300480  CE: 2.0878  
[04/16 13:19:44] Re-training INFO: iter: 157125/300480  CE: 2.3910  
[04/16 13:20:48] Re-training INFO: iter: 157250/300480  CE: 2.3493  
[04/16 13:21:53] Re-training INFO: iter: 157375/300480  CE: 2.2386  
[04/16 13:22:57] Re-training INFO: iter: 157500/300480  CE: 2.5525  
[04/16 13:24:00] Re-training INFO: iter: 157625/300480  CE: 2.1708  
[04/16 13:25:02] Re-training INFO: iter: 157750/300480  CE: 2.0125  
[04/16 13:25:03] Re-training INFO: --> epoch: 126/240  avg CE: 2.1314  lr: 0.11193511995679983  
[04/16 13:26:23] Re-training INFO: iter: 157875/300480  CE: 1.9098  
[04/16 13:27:28] Re-training INFO: iter: 158000/300480  CE: 2.0713  
[04/16 13:28:33] Re-training INFO: iter: 158125/300480  CE: 2.1164  
[04/16 13:29:37] Re-training INFO: iter: 158250/300480  CE: 2.1200  
[04/16 13:30:41] Re-training INFO: iter: 158375/300480  CE: 2.3349  
[04/16 13:31:45] Re-training INFO: iter: 158500/300480  CE: 2.1657  
[04/16 13:32:48] Re-training INFO: iter: 158625/300480  CE: 2.2590  
[04/16 13:33:51] Re-training INFO: iter: 158750/300480  CE: 2.1229  
[04/16 13:34:54] Re-training INFO: iter: 158875/300480  CE: 1.9841  
[04/16 13:35:56] Re-training INFO: iter: 159000/300480  CE: 2.1538  
[04/16 13:35:57] Re-training INFO: --> epoch: 127/240  avg CE: 2.1288  lr: 0.11193511995679983  
[04/16 13:37:17] Re-training INFO: iter: 159125/300480  CE: 2.2673  
[04/16 13:38:19] Re-training INFO: iter: 159250/300480  CE: 2.3281  
[04/16 13:39:22] Re-training INFO: iter: 159375/300480  CE: 2.0534  
[04/16 13:40:25] Re-training INFO: iter: 159500/300480  CE: 2.5176  
[04/16 13:41:28] Re-training INFO: iter: 159625/300480  CE: 2.1155  
[04/16 13:42:31] Re-training INFO: iter: 159750/300480  CE: 2.2011  
[04/16 13:43:34] Re-training INFO: iter: 159875/300480  CE: 1.9173  
[04/16 13:44:37] Re-training INFO: iter: 160000/300480  CE: 2.2059  
[04/16 13:45:40] Re-training INFO: iter: 160125/300480  CE: 2.2976  
[04/16 13:46:42] Re-training INFO: iter: 160250/300480  CE: 1.9995  
[04/16 13:46:44] Re-training INFO: --> epoch: 128/240  avg CE: 2.1202  lr: 0.10857706635809584  
[04/16 13:48:02] Re-training INFO: iter: 160375/300480  CE: 2.3455  
[04/16 13:49:06] Re-training INFO: iter: 160500/300480  CE: 2.0911  
[04/16 13:50:08] Re-training INFO: iter: 160625/300480  CE: 2.1169  
[04/16 13:51:13] Re-training INFO: iter: 160750/300480  CE: 2.0186  
[04/16 13:52:15] Re-training INFO: iter: 160875/300480  CE: 2.1625  
[04/16 13:53:18] Re-training INFO: iter: 161000/300480  CE: 2.2946  
[04/16 13:54:22] Re-training INFO: iter: 161125/300480  CE: 2.1723  
[04/16 13:55:26] Re-training INFO: iter: 161250/300480  CE: 2.1571  
[04/16 13:56:28] Re-training INFO: iter: 161375/300480  CE: 2.2323  
[04/16 13:57:32] Re-training INFO: iter: 161500/300480  CE: 2.3760  
[04/16 13:57:34] Re-training INFO: --> epoch: 129/240  avg CE: 2.1290  lr: 0.10857706635809584  
[04/16 13:58:52] Re-training INFO: iter: 161625/300480  CE: 2.0521  
[04/16 13:59:57] Re-training INFO: iter: 161750/300480  CE: 2.0822  
[04/16 14:01:00] Re-training INFO: iter: 161875/300480  CE: 2.1290  
[04/16 14:02:03] Re-training INFO: iter: 162000/300480  CE: 2.0045  
[04/16 14:03:06] Re-training INFO: iter: 162125/300480  CE: 1.9441  
[04/16 14:04:10] Re-training INFO: iter: 162250/300480  CE: 2.1408  
[04/16 14:05:14] Re-training INFO: iter: 162375/300480  CE: 1.8356  
[04/16 14:06:17] Re-training INFO: iter: 162500/300480  CE: 2.1921  
[04/16 14:07:20] Re-training INFO: iter: 162625/300480  CE: 2.2956  
[04/16 14:08:23] Re-training INFO: iter: 162750/300480  CE: 2.3025  
[04/16 14:08:26] Re-training INFO: --> epoch: 130/240  avg CE: 2.1231  lr: 0.10531975436735297  
[04/16 14:09:43] Re-training INFO: iter: 162875/300480  CE: 2.0120  
[04/16 14:10:46] Re-training INFO: iter: 163000/300480  CE: 2.1854  
[04/16 14:11:50] Re-training INFO: iter: 163125/300480  CE: 2.2747  
[04/16 14:12:53] Re-training INFO: iter: 163250/300480  CE: 2.1728  
[04/16 14:13:55] Re-training INFO: iter: 163375/300480  CE: 2.0712  
[04/16 14:14:58] Re-training INFO: iter: 163500/300480  CE: 1.8511  
[04/16 14:16:02] Re-training INFO: iter: 163625/300480  CE: 2.2377  
[04/16 14:17:04] Re-training INFO: iter: 163750/300480  CE: 2.1306  
[04/16 14:18:08] Re-training INFO: iter: 163875/300480  CE: 2.3387  
[04/16 14:19:11] Re-training INFO: iter: 164000/300480  CE: 1.9713  
[04/16 14:19:15] Re-training INFO: --> epoch: 131/240  avg CE: 2.1144  lr: 0.10531975436735297  
[04/16 14:20:31] Re-training INFO: iter: 164125/300480  CE: 2.0839  
[04/16 14:21:35] Re-training INFO: iter: 164250/300480  CE: 2.1851  
[04/16 14:22:39] Re-training INFO: iter: 164375/300480  CE: 2.1021  
[04/16 14:23:43] Re-training INFO: iter: 164500/300480  CE: 2.1994  
[04/16 14:24:46] Re-training INFO: iter: 164625/300480  CE: 2.4128  
[04/16 14:25:50] Re-training INFO: iter: 164750/300480  CE: 2.0682  
[04/16 14:26:53] Re-training INFO: iter: 164875/300480  CE: 2.1931  
[04/16 14:27:56] Re-training INFO: iter: 165000/300480  CE: 2.1721  
[04/16 14:28:58] Re-training INFO: iter: 165125/300480  CE: 2.1798  
[04/16 14:30:02] Re-training INFO: iter: 165250/300480  CE: 2.0297  
[04/16 14:30:07] Re-training INFO: --> epoch: 132/240  avg CE: 2.1189  lr: 0.10216016173633237  
[04/16 14:31:22] Re-training INFO: iter: 165375/300480  CE: 1.9337  
[04/16 14:32:26] Re-training INFO: iter: 165500/300480  CE: 2.3137  
[04/16 14:33:31] Re-training INFO: iter: 165625/300480  CE: 2.1689  
[04/16 14:34:34] Re-training INFO: iter: 165750/300480  CE: 1.9864  
[04/16 14:35:39] Re-training INFO: iter: 165875/300480  CE: 2.0774  
[04/16 14:36:42] Re-training INFO: iter: 166000/300480  CE: 2.2669  
[04/16 14:37:45] Re-training INFO: iter: 166125/300480  CE: 2.0529  
[04/16 14:38:48] Re-training INFO: iter: 166250/300480  CE: 2.0994  
[04/16 14:39:52] Re-training INFO: iter: 166375/300480  CE: 1.9719  
[04/16 14:40:55] Re-training INFO: iter: 166500/300480  CE: 2.2122  
[04/16 14:41:01] Re-training INFO: --> epoch: 133/240  avg CE: 2.1132  lr: 0.10216016173633237  
[04/16 14:42:15] Re-training INFO: iter: 166625/300480  CE: 2.1211  
[04/16 14:43:19] Re-training INFO: iter: 166750/300480  CE: 2.3010  
[04/16 14:44:24] Re-training INFO: iter: 166875/300480  CE: 2.1008  
[04/16 14:45:27] Re-training INFO: iter: 167000/300480  CE: 1.8659  
[04/16 14:46:30] Re-training INFO: iter: 167125/300480  CE: 2.2526  
[04/16 14:47:33] Re-training INFO: iter: 167250/300480  CE: 1.8714  
[04/16 14:48:37] Re-training INFO: iter: 167375/300480  CE: 1.9956  
[04/16 14:49:40] Re-training INFO: iter: 167500/300480  CE: 2.2722  
[04/16 14:50:42] Re-training INFO: iter: 167625/300480  CE: 2.0591  
[04/16 14:51:45] Re-training INFO: iter: 167750/300480  CE: 2.1168  
[04/16 14:51:52] Re-training INFO: --> epoch: 134/240  avg CE: 2.1072  lr: 0.10216016173633237  
[04/16 14:53:05] Re-training INFO: iter: 167875/300480  CE: 2.4785  
[04/16 14:54:08] Re-training INFO: iter: 168000/300480  CE: 2.2110  
[04/16 14:55:13] Re-training INFO: iter: 168125/300480  CE: 2.1290  
[04/16 14:56:16] Re-training INFO: iter: 168250/300480  CE: 2.1822  
[04/16 14:57:20] Re-training INFO: iter: 168375/300480  CE: 1.9615  
[04/16 14:58:23] Re-training INFO: iter: 168500/300480  CE: 2.2691  
[04/16 14:59:25] Re-training INFO: iter: 168625/300480  CE: 2.1587  
[04/16 15:00:28] Re-training INFO: iter: 168750/300480  CE: 1.9725  
[04/16 15:01:30] Re-training INFO: iter: 168875/300480  CE: 2.1146  
[04/16 15:02:32] Re-training INFO: iter: 169000/300480  CE: 2.1435  
[04/16 15:02:41] Re-training INFO: --> epoch: 135/240  avg CE: 2.1095  lr: 0.0990953568842424  
[04/16 15:03:52] Re-training INFO: iter: 169125/300480  CE: 2.3727  
[04/16 15:04:55] Re-training INFO: iter: 169250/300480  CE: 1.9660  
[04/16 15:05:58] Re-training INFO: iter: 169375/300480  CE: 1.6929  
[04/16 15:07:02] Re-training INFO: iter: 169500/300480  CE: 1.9575  
[04/16 15:08:04] Re-training INFO: iter: 169625/300480  CE: 2.0548  
[04/16 15:09:07] Re-training INFO: iter: 169750/300480  CE: 2.1129  
[04/16 15:10:10] Re-training INFO: iter: 169875/300480  CE: 2.2058  
[04/16 15:11:11] Re-training INFO: iter: 170000/300480  CE: 2.2029  
[04/16 15:12:15] Re-training INFO: iter: 170125/300480  CE: 2.1327  
[04/16 15:13:17] Re-training INFO: iter: 170250/300480  CE: 2.1660  
[04/16 15:13:27] Re-training INFO: --> epoch: 136/240  avg CE: 2.1055  lr: 0.0990953568842424  
[04/16 15:14:37] Re-training INFO: iter: 170375/300480  CE: 2.1122  
[04/16 15:15:42] Re-training INFO: iter: 170500/300480  CE: 1.9505  
[04/16 15:16:46] Re-training INFO: iter: 170625/300480  CE: 2.0660  
[04/16 15:17:49] Re-training INFO: iter: 170750/300480  CE: 2.0916  
[04/16 15:18:52] Re-training INFO: iter: 170875/300480  CE: 1.9747  
[04/16 15:19:54] Re-training INFO: iter: 171000/300480  CE: 2.1078  
[04/16 15:20:58] Re-training INFO: iter: 171125/300480  CE: 1.9706  
[04/16 15:22:02] Re-training INFO: iter: 171250/300480  CE: 2.2180  
[04/16 15:23:06] Re-training INFO: iter: 171375/300480  CE: 2.1213  
[04/16 15:24:09] Re-training INFO: iter: 171500/300480  CE: 2.0690  
[04/16 15:24:20] Re-training INFO: --> epoch: 137/240  avg CE: 2.0977  lr: 0.09612249617771512  
[04/16 15:25:29] Re-training INFO: iter: 171625/300480  CE: 1.9503  
[04/16 15:26:32] Re-training INFO: iter: 171750/300480  CE: 2.1992  
[04/16 15:27:35] Re-training INFO: iter: 171875/300480  CE: 1.9154  
[04/16 15:28:38] Re-training INFO: iter: 172000/300480  CE: 1.9251  
[04/16 15:29:42] Re-training INFO: iter: 172125/300480  CE: 2.2013  
[04/16 15:30:45] Re-training INFO: iter: 172250/300480  CE: 1.8578  
[04/16 15:31:49] Re-training INFO: iter: 172375/300480  CE: 2.0389  
[04/16 15:32:52] Re-training INFO: iter: 172500/300480  CE: 2.1219  
[04/16 15:33:55] Re-training INFO: iter: 172625/300480  CE: 2.0800  
[04/16 15:34:57] Re-training INFO: iter: 172750/300480  CE: 2.0246  
[04/16 15:35:09] Re-training INFO: --> epoch: 138/240  avg CE: 2.0980  lr: 0.09612249617771512  
[04/16 15:36:17] Re-training INFO: iter: 172875/300480  CE: 2.0279  
[04/16 15:37:20] Re-training INFO: iter: 173000/300480  CE: 2.1217  
[04/16 15:38:23] Re-training INFO: iter: 173125/300480  CE: 2.0454  
[04/16 15:39:27] Re-training INFO: iter: 173250/300480  CE: 2.0748  
[04/16 15:40:29] Re-training INFO: iter: 173375/300480  CE: 2.1066  
[04/16 15:41:32] Re-training INFO: iter: 173500/300480  CE: 2.0979  
[04/16 15:42:35] Re-training INFO: iter: 173625/300480  CE: 2.2545  
[04/16 15:43:39] Re-training INFO: iter: 173750/300480  CE: 2.2054  
[04/16 15:44:43] Re-training INFO: iter: 173875/300480  CE: 2.1668  
[04/16 15:45:46] Re-training INFO: iter: 174000/300480  CE: 2.0606  
[04/16 15:45:59] Re-training INFO: --> epoch: 139/240  avg CE: 2.0996  lr: 0.09612249617771512  
[04/16 15:47:06] Re-training INFO: iter: 174125/300480  CE: 2.3431  
[04/16 15:48:10] Re-training INFO: iter: 174250/300480  CE: 2.1054  
[04/16 15:49:15] Re-training INFO: iter: 174375/300480  CE: 1.8669  
[04/16 15:50:18] Re-training INFO: iter: 174500/300480  CE: 2.0303  
[04/16 15:51:21] Re-training INFO: iter: 174625/300480  CE: 2.3515  
[04/16 15:52:25] Re-training INFO: iter: 174750/300480  CE: 2.1353  
[04/16 15:53:28] Re-training INFO: iter: 174875/300480  CE: 2.1836  
[04/16 15:54:31] Re-training INFO: iter: 175000/300480  CE: 2.2528  
[04/16 15:55:35] Re-training INFO: iter: 175125/300480  CE: 1.9594  
[04/16 15:56:38] Re-training INFO: iter: 175250/300480  CE: 2.3177  
[04/16 15:56:51] Re-training INFO: --> epoch: 140/240  avg CE: 2.0943  lr: 0.09323882129238366  
[04/16 15:57:25] Re-training INFO: # of Test Samples: 50000.0
[04/16 15:57:25] Re-training INFO: Top-1/-5 acc: 71.65 / 90.31
[04/16 15:57:25] Re-training INFO: Top-1/-5 acc: 28.35 /  9.69
[04/16 15:57:25] Re-training INFO: 

[04/16 15:58:32] Re-training INFO: iter: 175375/300480  CE: 2.0803  
[04/16 15:59:35] Re-training INFO: iter: 175500/300480  CE: 2.2164  
[04/16 16:00:38] Re-training INFO: iter: 175625/300480  CE: 1.9615  
[04/16 16:01:40] Re-training INFO: iter: 175750/300480  CE: 2.1419  
[04/16 16:02:42] Re-training INFO: iter: 175875/300480  CE: 2.0069  
[04/16 16:03:46] Re-training INFO: iter: 176000/300480  CE: 2.1145  
[04/16 16:04:49] Re-training INFO: iter: 176125/300480  CE: 2.3363  
[04/16 16:05:52] Re-training INFO: iter: 176250/300480  CE: 2.0023  
[04/16 16:06:55] Re-training INFO: iter: 176375/300480  CE: 2.4199  
[04/16 16:07:58] Re-training INFO: iter: 176500/300480  CE: 2.0455  
[04/16 16:08:12] Re-training INFO: --> epoch: 141/240  avg CE: 2.0981  lr: 0.09323882129238366  
[04/16 16:09:17] Re-training INFO: iter: 176625/300480  CE: 2.0362  
[04/16 16:10:21] Re-training INFO: iter: 176750/300480  CE: 1.9655  
[04/16 16:11:25] Re-training INFO: iter: 176875/300480  CE: 1.8321  
[04/16 16:12:29] Re-training INFO: iter: 177000/300480  CE: 1.9665  
[04/16 16:13:32] Re-training INFO: iter: 177125/300480  CE: 2.2706  
[04/16 16:14:36] Re-training INFO: iter: 177250/300480  CE: 2.1272  
[04/16 16:15:39] Re-training INFO: iter: 177375/300480  CE: 2.3831  
[04/16 16:16:43] Re-training INFO: iter: 177500/300480  CE: 2.1592  
[04/16 16:17:45] Re-training INFO: iter: 177625/300480  CE: 2.4285  
[04/16 16:18:49] Re-training INFO: iter: 177750/300480  CE: 2.0940  
[04/16 16:19:04] Re-training INFO: --> epoch: 142/240  avg CE: 2.0920  lr: 0.09044165665361216  
[04/16 16:20:09] Re-training INFO: iter: 177875/300480  CE: 1.9870  
[04/16 16:21:12] Re-training INFO: iter: 178000/300480  CE: 2.0346  
[04/16 16:22:15] Re-training INFO: iter: 178125/300480  CE: 1.8884  
[04/16 16:23:18] Re-training INFO: iter: 178250/300480  CE: 1.7787  
[04/16 16:24:21] Re-training INFO: iter: 178375/300480  CE: 2.1985  
[04/16 16:25:24] Re-training INFO: iter: 178500/300480  CE: 2.4097  
[04/16 16:26:27] Re-training INFO: iter: 178625/300480  CE: 2.2177  
[04/16 16:27:31] Re-training INFO: iter: 178750/300480  CE: 2.2737  
[04/16 16:28:34] Re-training INFO: iter: 178875/300480  CE: 2.0042  
[04/16 16:29:36] Re-training INFO: iter: 179000/300480  CE: 2.0285  
[04/16 16:29:52] Re-training INFO: --> epoch: 143/240  avg CE: 2.0905  lr: 0.09044165665361216  
[04/16 16:30:56] Re-training INFO: iter: 179125/300480  CE: 1.9467  
[04/16 16:32:00] Re-training INFO: iter: 179250/300480  CE: 1.7874  
[04/16 16:33:04] Re-training INFO: iter: 179375/300480  CE: 2.1191  
[04/16 16:34:08] Re-training INFO: iter: 179500/300480  CE: 2.1086  
[04/16 16:35:10] Re-training INFO: iter: 179625/300480  CE: 2.1251  
[04/16 16:36:14] Re-training INFO: iter: 179750/300480  CE: 1.8823  
[04/16 16:37:18] Re-training INFO: iter: 179875/300480  CE: 2.1796  
[04/16 16:38:22] Re-training INFO: iter: 180000/300480  CE: 2.2532  
[04/16 16:39:25] Re-training INFO: iter: 180125/300480  CE: 1.9697  
[04/16 16:40:28] Re-training INFO: iter: 180250/300480  CE: 2.0650  
[04/16 16:40:46] Re-training INFO: --> epoch: 144/240  avg CE: 2.0916  lr: 0.08772840695400379  
[04/16 16:41:48] Re-training INFO: iter: 180375/300480  CE: 2.2494  
[04/16 16:42:53] Re-training INFO: iter: 180500/300480  CE: 2.0593  
[04/16 16:43:56] Re-training INFO: iter: 180625/300480  CE: 2.2083  
[04/16 16:45:00] Re-training INFO: iter: 180750/300480  CE: 1.9108  
[04/16 16:46:03] Re-training INFO: iter: 180875/300480  CE: 2.0643  
[04/16 16:47:07] Re-training INFO: iter: 181000/300480  CE: 2.2039  
[04/16 16:48:10] Re-training INFO: iter: 181125/300480  CE: 2.1443  
[04/16 16:49:14] Re-training INFO: iter: 181250/300480  CE: 2.0812  
[04/16 16:50:18] Re-training INFO: iter: 181375/300480  CE: 2.1125  
[04/16 16:51:22] Re-training INFO: iter: 181500/300480  CE: 2.0483  
[04/16 16:51:40] Re-training INFO: --> epoch: 145/240  avg CE: 2.0846  lr: 0.08772840695400379  
[04/16 16:52:41] Re-training INFO: iter: 181625/300480  CE: 2.0060  
[04/16 16:53:46] Re-training INFO: iter: 181750/300480  CE: 2.2047  
[04/16 16:54:49] Re-training INFO: iter: 181875/300480  CE: 2.0230  
[04/16 16:55:53] Re-training INFO: iter: 182000/300480  CE: 2.1314  
[04/16 16:56:56] Re-training INFO: iter: 182125/300480  CE: 2.2219  
[04/16 16:58:00] Re-training INFO: iter: 182250/300480  CE: 2.0419  
[04/16 16:59:05] Re-training INFO: iter: 182375/300480  CE: 2.0619  
[04/16 17:00:08] Re-training INFO: iter: 182500/300480  CE: 2.1457  
[04/16 17:01:12] Re-training INFO: iter: 182625/300480  CE: 2.1334  
[04/16 17:02:14] Re-training INFO: iter: 182750/300480  CE: 1.8887  
[04/16 17:02:34] Re-training INFO: --> epoch: 146/240  avg CE: 2.0801  lr: 0.08772840695400379  
[04/16 17:03:35] Re-training INFO: iter: 182875/300480  CE: 2.0848  
[04/16 17:04:39] Re-training INFO: iter: 183000/300480  CE: 1.9383  
[04/16 17:05:42] Re-training INFO: iter: 183125/300480  CE: 2.2248  
[04/16 17:06:46] Re-training INFO: iter: 183250/300480  CE: 1.9707  
[04/16 17:07:50] Re-training INFO: iter: 183375/300480  CE: 2.0821  
[04/16 17:08:54] Re-training INFO: iter: 183500/300480  CE: 2.2986  
[04/16 17:09:57] Re-training INFO: iter: 183625/300480  CE: 1.9134  
[04/16 17:11:02] Re-training INFO: iter: 183750/300480  CE: 2.0569  
[04/16 17:12:05] Re-training INFO: iter: 183875/300480  CE: 2.3548  
[04/16 17:13:07] Re-training INFO: iter: 184000/300480  CE: 2.1094  
[04/16 17:13:28] Re-training INFO: --> epoch: 147/240  avg CE: 2.0797  lr: 0.08509655474538368  
[04/16 17:14:27] Re-training INFO: iter: 184125/300480  CE: 1.9831  
[04/16 17:15:31] Re-training INFO: iter: 184250/300480  CE: 2.0364  
[04/16 17:16:34] Re-training INFO: iter: 184375/300480  CE: 2.1195  
[04/16 17:17:38] Re-training INFO: iter: 184500/300480  CE: 2.1135  
[04/16 17:18:41] Re-training INFO: iter: 184625/300480  CE: 1.9461  
[04/16 17:19:44] Re-training INFO: iter: 184750/300480  CE: 2.2453  
[04/16 17:20:48] Re-training INFO: iter: 184875/300480  CE: 1.9922  
[04/16 17:21:51] Re-training INFO: iter: 185000/300480  CE: 1.9766  
[04/16 17:22:54] Re-training INFO: iter: 185125/300480  CE: 1.9986  
[04/16 17:23:57] Re-training INFO: iter: 185250/300480  CE: 2.1799  
[04/16 17:24:19] Re-training INFO: --> epoch: 148/240  avg CE: 2.0820  lr: 0.08509655474538368  
[04/16 17:25:17] Re-training INFO: iter: 185375/300480  CE: 2.1530  
[04/16 17:26:21] Re-training INFO: iter: 185500/300480  CE: 2.0501  
[04/16 17:27:24] Re-training INFO: iter: 185625/300480  CE: 2.1357  
[04/16 17:28:27] Re-training INFO: iter: 185750/300480  CE: 2.1761  
[04/16 17:29:30] Re-training INFO: iter: 185875/300480  CE: 2.0001  
[04/16 17:30:32] Re-training INFO: iter: 186000/300480  CE: 1.9093  
[04/16 17:31:36] Re-training INFO: iter: 186125/300480  CE: 2.1064  
[04/16 17:32:37] Re-training INFO: iter: 186250/300480  CE: 2.1010  
[04/16 17:33:40] Re-training INFO: iter: 186375/300480  CE: 2.2222  
[04/16 17:34:42] Re-training INFO: iter: 186500/300480  CE: 2.0508  
[04/16 17:35:04] Re-training INFO: --> epoch: 149/240  avg CE: 2.0785  lr: 0.08254365810302215  
[04/16 17:36:01] Re-training INFO: iter: 186625/300480  CE: 2.1600  
[04/16 17:37:04] Re-training INFO: iter: 186750/300480  CE: 2.1059  
[04/16 17:38:07] Re-training INFO: iter: 186875/300480  CE: 2.1662  
[04/16 17:39:11] Re-training INFO: iter: 187000/300480  CE: 2.0613  
[04/16 17:40:15] Re-training INFO: iter: 187125/300480  CE: 2.1313  
[04/16 17:41:19] Re-training INFO: iter: 187250/300480  CE: 2.1008  
[04/16 17:42:22] Re-training INFO: iter: 187375/300480  CE: 2.1723  
[04/16 17:43:26] Re-training INFO: iter: 187500/300480  CE: 1.9160  
[04/16 17:44:30] Re-training INFO: iter: 187625/300480  CE: 2.2704  
[04/16 17:45:33] Re-training INFO: iter: 187750/300480  CE: 1.9302  
[04/16 17:45:57] Re-training INFO: --> epoch: 150/240  avg CE: 2.0676  lr: 0.08254365810302215  
[04/16 17:46:54] Re-training INFO: iter: 187875/300480  CE: 2.0509  
[04/16 17:47:58] Re-training INFO: iter: 188000/300480  CE: 2.0123  
[04/16 17:49:02] Re-training INFO: iter: 188125/300480  CE: 2.0267  
[04/16 17:50:06] Re-training INFO: iter: 188250/300480  CE: 2.0323  
[04/16 17:51:10] Re-training INFO: iter: 188375/300480  CE: 1.9765  
[04/16 17:52:15] Re-training INFO: iter: 188500/300480  CE: 1.9385  
[04/16 17:53:20] Re-training INFO: iter: 188625/300480  CE: 1.9368  
[04/16 17:54:25] Re-training INFO: iter: 188750/300480  CE: 2.0847  
[04/16 17:55:28] Re-training INFO: iter: 188875/300480  CE: 1.9466  
[04/16 17:56:32] Re-training INFO: iter: 189000/300480  CE: 2.2988  
[04/16 17:56:55] Re-training INFO: --> epoch: 151/240  avg CE: 2.0782  lr: 0.08254365810302215  
[04/16 17:57:51] Re-training INFO: iter: 189125/300480  CE: 1.8792  
[04/16 17:58:55] Re-training INFO: iter: 189250/300480  CE: 2.1819  
[04/16 17:59:58] Re-training INFO: iter: 189375/300480  CE: 2.2178  
[04/16 18:01:02] Re-training INFO: iter: 189500/300480  CE: 2.2115  
[04/16 18:02:05] Re-training INFO: iter: 189625/300480  CE: 1.7672  
[04/16 18:03:10] Re-training INFO: iter: 189750/300480  CE: 2.0708  
[04/16 18:04:13] Re-training INFO: iter: 189875/300480  CE: 1.9726  
[04/16 18:05:17] Re-training INFO: iter: 190000/300480  CE: 2.1742  
[04/16 18:06:21] Re-training INFO: iter: 190125/300480  CE: 1.8534  
[04/16 18:07:24] Re-training INFO: iter: 190250/300480  CE: 2.3007  
[04/16 18:07:50] Re-training INFO: --> epoch: 152/240  avg CE: 2.0652  lr: 0.08006734835993148  
[04/16 18:08:44] Re-training INFO: iter: 190375/300480  CE: 1.8677  
[04/16 18:09:48] Re-training INFO: iter: 190500/300480  CE: 2.0417  
[04/16 18:10:52] Re-training INFO: iter: 190625/300480  CE: 2.2398  
[04/16 18:11:56] Re-training INFO: iter: 190750/300480  CE: 2.0648  
[04/16 18:13:02] Re-training INFO: iter: 190875/300480  CE: 2.0779  
[04/16 18:14:07] Re-training INFO: iter: 191000/300480  CE: 1.9576  
[04/16 18:15:12] Re-training INFO: iter: 191125/300480  CE: 1.8465  
[04/16 18:16:17] Re-training INFO: iter: 191250/300480  CE: 2.0579  
[04/16 18:17:21] Re-training INFO: iter: 191375/300480  CE: 2.0859  
[04/16 18:18:25] Re-training INFO: iter: 191500/300480  CE: 2.3778  
[04/16 18:18:51] Re-training INFO: --> epoch: 153/240  avg CE: 2.0676  lr: 0.08006734835993148  
[04/16 18:19:45] Re-training INFO: iter: 191625/300480  CE: 2.2272  
[04/16 18:20:51] Re-training INFO: iter: 191750/300480  CE: 2.2101  
[04/16 18:21:56] Re-training INFO: iter: 191875/300480  CE: 1.8724  
[04/16 18:23:01] Re-training INFO: iter: 192000/300480  CE: 2.0902  
[04/16 18:24:06] Re-training INFO: iter: 192125/300480  CE: 1.9941  
[04/16 18:25:10] Re-training INFO: iter: 192250/300480  CE: 2.2423  
[04/16 18:26:15] Re-training INFO: iter: 192375/300480  CE: 2.1941  
[04/16 18:27:19] Re-training INFO: iter: 192500/300480  CE: 2.2047  
[04/16 18:28:23] Re-training INFO: iter: 192625/300480  CE: 2.1824  
[04/16 18:29:27] Re-training INFO: iter: 192750/300480  CE: 2.1879  
[04/16 18:29:55] Re-training INFO: --> epoch: 154/240  avg CE: 2.0651  lr: 0.07766532790913355  
[04/16 18:30:48] Re-training INFO: iter: 192875/300480  CE: 2.1349  
[04/16 18:31:53] Re-training INFO: iter: 193000/300480  CE: 2.1261  
[04/16 18:32:57] Re-training INFO: iter: 193125/300480  CE: 2.1845  
[04/16 18:34:02] Re-training INFO: iter: 193250/300480  CE: 2.2216  
[04/16 18:35:06] Re-training INFO: iter: 193375/300480  CE: 1.8975  
[04/16 18:36:10] Re-training INFO: iter: 193500/300480  CE: 1.9554  
[04/16 18:37:15] Re-training INFO: iter: 193625/300480  CE: 1.9311  
[04/16 18:38:20] Re-training INFO: iter: 193750/300480  CE: 1.9793  
[04/16 18:39:25] Re-training INFO: iter: 193875/300480  CE: 2.1327  
[04/16 18:40:29] Re-training INFO: iter: 194000/300480  CE: 2.1533  
[04/16 18:40:58] Re-training INFO: --> epoch: 155/240  avg CE: 2.0610  lr: 0.07766532790913355  
[04/16 18:41:50] Re-training INFO: iter: 194125/300480  CE: 2.1756  
[04/16 18:42:55] Re-training INFO: iter: 194250/300480  CE: 2.1693  
[04/16 18:44:00] Re-training INFO: iter: 194375/300480  CE: 2.1494  
[04/16 18:45:05] Re-training INFO: iter: 194500/300480  CE: 2.1648  
[04/16 18:46:10] Re-training INFO: iter: 194625/300480  CE: 2.1590  
[04/16 18:47:15] Re-training INFO: iter: 194750/300480  CE: 2.1970  
[04/16 18:48:19] Re-training INFO: iter: 194875/300480  CE: 2.2256  
[04/16 18:49:23] Re-training INFO: iter: 195000/300480  CE: 2.2060  
[04/16 18:50:26] Re-training INFO: iter: 195125/300480  CE: 1.9166  
[04/16 18:51:31] Re-training INFO: iter: 195250/300480  CE: 2.2460  
[04/16 18:52:01] Re-training INFO: --> epoch: 156/240  avg CE: 2.0640  lr: 0.07533536807185953  
[04/16 18:52:52] Re-training INFO: iter: 195375/300480  CE: 2.1922  
[04/16 18:53:58] Re-training INFO: iter: 195500/300480  CE: 2.0876  
[04/16 18:55:02] Re-training INFO: iter: 195625/300480  CE: 1.9444  
[04/16 18:56:08] Re-training INFO: iter: 195750/300480  CE: 2.0426  
[04/16 18:57:13] Re-training INFO: iter: 195875/300480  CE: 1.8707  
[04/16 18:58:17] Re-training INFO: iter: 196000/300480  CE: 2.2055  
[04/16 18:59:21] Re-training INFO: iter: 196125/300480  CE: 2.2354  
[04/16 19:00:25] Re-training INFO: iter: 196250/300480  CE: 2.1488  
[04/16 19:01:30] Re-training INFO: iter: 196375/300480  CE: 2.0089  
[04/16 19:02:35] Re-training INFO: iter: 196500/300480  CE: 2.1279  
[04/16 19:03:06] Re-training INFO: --> epoch: 157/240  avg CE: 2.0554  lr: 0.07533536807185953  
[04/16 19:03:56] Re-training INFO: iter: 196625/300480  CE: 1.9151  
[04/16 19:05:01] Re-training INFO: iter: 196750/300480  CE: 1.9962  
[04/16 19:06:07] Re-training INFO: iter: 196875/300480  CE: 1.9500  
[04/16 19:07:12] Re-training INFO: iter: 197000/300480  CE: 2.2283  
[04/16 19:08:17] Re-training INFO: iter: 197125/300480  CE: 1.9591  
[04/16 19:09:21] Re-training INFO: iter: 197250/300480  CE: 1.9412  
[04/16 19:10:27] Re-training INFO: iter: 197375/300480  CE: 2.3288  
[04/16 19:11:30] Re-training INFO: iter: 197500/300480  CE: 2.0815  
[04/16 19:12:34] Re-training INFO: iter: 197625/300480  CE: 1.9658  
[04/16 19:13:38] Re-training INFO: iter: 197750/300480  CE: 2.0803  
[04/16 19:14:11] Re-training INFO: --> epoch: 158/240  avg CE: 2.0557  lr: 0.07533536807185953  
[04/16 19:15:00] Re-training INFO: iter: 197875/300480  CE: 1.9514  
[04/16 19:16:06] Re-training INFO: iter: 198000/300480  CE: 2.0659  
[04/16 19:17:11] Re-training INFO: iter: 198125/300480  CE: 1.9982  
[04/16 19:18:16] Re-training INFO: iter: 198250/300480  CE: 2.0673  
[04/16 19:19:22] Re-training INFO: iter: 198375/300480  CE: 2.2508  
[04/16 19:20:28] Re-training INFO: iter: 198500/300480  CE: 1.9953  
[04/16 19:21:32] Re-training INFO: iter: 198625/300480  CE: 2.0759  
[04/16 19:22:37] Re-training INFO: iter: 198750/300480  CE: 2.0249  
[04/16 19:23:43] Re-training INFO: iter: 198875/300480  CE: 2.1203  
[04/16 19:24:48] Re-training INFO: iter: 199000/300480  CE: 2.0194  
[04/16 19:25:20] Re-training INFO: --> epoch: 159/240  avg CE: 2.0532  lr: 0.07307530702970376  
[04/16 19:26:08] Re-training INFO: iter: 199125/300480  CE: 2.3564  
[04/16 19:27:13] Re-training INFO: iter: 199250/300480  CE: 1.9894  
[04/16 19:28:18] Re-training INFO: iter: 199375/300480  CE: 2.3136  
[04/16 19:29:24] Re-training INFO: iter: 199500/300480  CE: 1.9052  
[04/16 19:30:29] Re-training INFO: iter: 199625/300480  CE: 2.1384  
[04/16 19:31:34] Re-training INFO: iter: 199750/300480  CE: 2.0116  
[04/16 19:32:39] Re-training INFO: iter: 199875/300480  CE: 2.0745  
[04/16 19:33:43] Re-training INFO: iter: 200000/300480  CE: 2.1584  
[04/16 19:34:47] Re-training INFO: iter: 200125/300480  CE: 2.1564  
[04/16 19:35:53] Re-training INFO: iter: 200250/300480  CE: 2.1797  
[04/16 19:36:28] Re-training INFO: --> epoch: 160/240  avg CE: 2.0512  lr: 0.07307530702970376  
[04/16 19:37:02] Re-training INFO: # of Test Samples: 50000.0
[04/16 19:37:02] Re-training INFO: Top-1/-5 acc: 71.56 / 90.18
[04/16 19:37:02] Re-training INFO: Top-1/-5 acc: 28.44 /  9.82
[04/16 19:37:02] Re-training INFO: 

[04/16 19:37:50] Re-training INFO: iter: 200375/300480  CE: 1.9083  
[04/16 19:38:54] Re-training INFO: iter: 200500/300480  CE: 2.0334  
[04/16 19:40:01] Re-training INFO: iter: 200625/300480  CE: 2.1127  
[04/16 19:41:07] Re-training INFO: iter: 200750/300480  CE: 2.1684  
[04/16 19:42:12] Re-training INFO: iter: 200875/300480  CE: 1.9050  
[04/16 19:43:16] Re-training INFO: iter: 201000/300480  CE: 2.2086  
[04/16 19:44:20] Re-training INFO: iter: 201125/300480  CE: 2.2141  
[04/16 19:45:26] Re-training INFO: iter: 201250/300480  CE: 2.1665  
[04/16 19:46:31] Re-training INFO: iter: 201375/300480  CE: 1.9813  
[04/16 19:47:36] Re-training INFO: iter: 201500/300480  CE: 1.9850  
[04/16 19:48:12] Re-training INFO: --> epoch: 161/240  avg CE: 2.0465  lr: 0.07088304781881262  
[04/16 19:48:58] Re-training INFO: iter: 201625/300480  CE: 1.9871  
[04/16 19:50:03] Re-training INFO: iter: 201750/300480  CE: 2.0799  
[04/16 19:51:09] Re-training INFO: iter: 201875/300480  CE: 1.8546  
[04/16 19:52:15] Re-training INFO: iter: 202000/300480  CE: 1.9860  
[04/16 19:53:20] Re-training INFO: iter: 202125/300480  CE: 2.2730  
[04/16 19:54:26] Re-training INFO: iter: 202250/300480  CE: 2.2137  
[04/16 19:55:31] Re-training INFO: iter: 202375/300480  CE: 1.9595  
[04/16 19:56:37] Re-training INFO: iter: 202500/300480  CE: 2.0200  
[04/16 19:57:42] Re-training INFO: iter: 202625/300480  CE: 2.1344  
[04/16 19:58:46] Re-training INFO: iter: 202750/300480  CE: 1.8578  
[04/16 19:59:23] Re-training INFO: --> epoch: 162/240  avg CE: 2.0468  lr: 0.07088304781881262  
[04/16 20:00:07] Re-training INFO: iter: 202875/300480  CE: 1.8348  
[04/16 20:01:12] Re-training INFO: iter: 203000/300480  CE: 2.0764  
[04/16 20:02:18] Re-training INFO: iter: 203125/300480  CE: 2.0425  
[04/16 20:03:24] Re-training INFO: iter: 203250/300480  CE: 1.9712  
[04/16 20:04:29] Re-training INFO: iter: 203375/300480  CE: 2.0747  
[04/16 20:05:33] Re-training INFO: iter: 203500/300480  CE: 1.8961  
[04/16 20:06:39] Re-training INFO: iter: 203625/300480  CE: 1.8490  
[04/16 20:07:44] Re-training INFO: iter: 203750/300480  CE: 2.1590  
[04/16 20:08:49] Re-training INFO: iter: 203875/300480  CE: 2.2040  
[04/16 20:09:54] Re-training INFO: iter: 204000/300480  CE: 2.2095  
[04/16 20:10:32] Re-training INFO: --> epoch: 163/240  avg CE: 2.0433  lr: 0.07088304781881262  
[04/16 20:11:16] Re-training INFO: iter: 204125/300480  CE: 2.0417  
[04/16 20:12:21] Re-training INFO: iter: 204250/300480  CE: 2.0036  
[04/16 20:13:26] Re-training INFO: iter: 204375/300480  CE: 1.9117  
[04/16 20:14:32] Re-training INFO: iter: 204500/300480  CE: 2.0103  
[04/16 20:15:36] Re-training INFO: iter: 204625/300480  CE: 2.1342  
[04/16 20:16:42] Re-training INFO: iter: 204750/300480  CE: 1.9910  
[04/16 20:17:48] Re-training INFO: iter: 204875/300480  CE: 1.9389  
[04/16 20:18:54] Re-training INFO: iter: 205000/300480  CE: 2.1781  
[04/16 20:20:00] Re-training INFO: iter: 205125/300480  CE: 1.9892  
[04/16 20:21:06] Re-training INFO: iter: 205250/300480  CE: 1.9616  
[04/16 20:21:44] Re-training INFO: --> epoch: 164/240  avg CE: 2.0433  lr: 0.06875655638424824  
[04/16 20:22:27] Re-training INFO: iter: 205375/300480  CE: 2.1346  
[04/16 20:23:33] Re-training INFO: iter: 205500/300480  CE: 2.0158  
[04/16 20:24:39] Re-training INFO: iter: 205625/300480  CE: 2.0362  
[04/16 20:25:46] Re-training INFO: iter: 205750/300480  CE: 2.1959  
[04/16 20:26:51] Re-training INFO: iter: 205875/300480  CE: 1.9401  
[04/16 20:27:55] Re-training INFO: iter: 206000/300480  CE: 1.9056  
[04/16 20:28:59] Re-training INFO: iter: 206125/300480  CE: 1.9568  
[04/16 20:30:05] Re-training INFO: iter: 206250/300480  CE: 1.9263  
[04/16 20:31:11] Re-training INFO: iter: 206375/300480  CE: 2.1118  
[04/16 20:32:16] Re-training INFO: iter: 206500/300480  CE: 2.0080  
[04/16 20:32:56] Re-training INFO: --> epoch: 165/240  avg CE: 2.0468  lr: 0.06875655638424824  
[04/16 20:33:37] Re-training INFO: iter: 206625/300480  CE: 1.9496  
[04/16 20:34:43] Re-training INFO: iter: 206750/300480  CE: 2.0159  
[04/16 20:35:48] Re-training INFO: iter: 206875/300480  CE: 1.9634  
[04/16 20:36:53] Re-training INFO: iter: 207000/300480  CE: 2.1378  
[04/16 20:37:58] Re-training INFO: iter: 207125/300480  CE: 2.0291  
[04/16 20:39:03] Re-training INFO: iter: 207250/300480  CE: 2.0665  
[04/16 20:40:06] Re-training INFO: iter: 207375/300480  CE: 2.0987  
[04/16 20:41:11] Re-training INFO: iter: 207500/300480  CE: 2.2024  
[04/16 20:42:16] Re-training INFO: iter: 207625/300480  CE: 1.7918  
[04/16 20:43:21] Re-training INFO: iter: 207750/300480  CE: 1.9664  
[04/16 20:44:01] Re-training INFO: --> epoch: 166/240  avg CE: 2.0412  lr: 0.06669385969272079  
[04/16 20:44:41] Re-training INFO: iter: 207875/300480  CE: 1.9641  
[04/16 20:45:47] Re-training INFO: iter: 208000/300480  CE: 1.8587  
[04/16 20:46:53] Re-training INFO: iter: 208125/300480  CE: 1.9251  
[04/16 20:47:58] Re-training INFO: iter: 208250/300480  CE: 2.1148  
[04/16 20:49:03] Re-training INFO: iter: 208375/300480  CE: 2.0386  
[04/16 20:50:10] Re-training INFO: iter: 208500/300480  CE: 1.9808  
[04/16 20:51:15] Re-training INFO: iter: 208625/300480  CE: 2.1461  
[04/16 20:52:20] Re-training INFO: iter: 208750/300480  CE: 1.9885  
[04/16 20:53:27] Re-training INFO: iter: 208875/300480  CE: 2.3079  
[04/16 20:54:33] Re-training INFO: iter: 209000/300480  CE: 1.9073  
[04/16 20:55:16] Re-training INFO: --> epoch: 167/240  avg CE: 2.0323  lr: 0.06669385969272079  
[04/16 20:55:55] Re-training INFO: iter: 209125/300480  CE: 2.0786  
[04/16 20:57:01] Re-training INFO: iter: 209250/300480  CE: 1.9885  
[04/16 20:58:07] Re-training INFO: iter: 209375/300480  CE: 1.9248  
[04/16 20:59:13] Re-training INFO: iter: 209500/300480  CE: 2.0142  
[04/16 21:00:19] Re-training INFO: iter: 209625/300480  CE: 1.8387  
[04/16 21:01:25] Re-training INFO: iter: 209750/300480  CE: 1.9572  
[04/16 21:02:31] Re-training INFO: iter: 209875/300480  CE: 2.1614  
[04/16 21:03:36] Re-training INFO: iter: 210000/300480  CE: 1.9854  
[04/16 21:04:42] Re-training INFO: iter: 210125/300480  CE: 2.2204  
[04/16 21:05:48] Re-training INFO: iter: 210250/300480  CE: 2.4156  
[04/16 21:06:31] Re-training INFO: --> epoch: 168/240  avg CE: 2.0369  lr: 0.06469304390193917  
[04/16 21:07:08] Re-training INFO: iter: 210375/300480  CE: 1.7660  
[04/16 21:08:13] Re-training INFO: iter: 210500/300480  CE: 1.8735  
[04/16 21:09:18] Re-training INFO: iter: 210625/300480  CE: 1.9032  
[04/16 21:10:23] Re-training INFO: iter: 210750/300480  CE: 1.9233  
[04/16 21:11:28] Re-training INFO: iter: 210875/300480  CE: 2.1474  
[04/16 21:12:32] Re-training INFO: iter: 211000/300480  CE: 2.2139  
[04/16 21:13:37] Re-training INFO: iter: 211125/300480  CE: 2.0342  
[04/16 21:14:43] Re-training INFO: iter: 211250/300480  CE: 2.1615  
[04/16 21:15:48] Re-training INFO: iter: 211375/300480  CE: 1.9724  
[04/16 21:16:53] Re-training INFO: iter: 211500/300480  CE: 2.2663  
[04/16 21:17:37] Re-training INFO: --> epoch: 169/240  avg CE: 2.0318  lr: 0.06469304390193917  
[04/16 21:18:15] Re-training INFO: iter: 211625/300480  CE: 1.9835  
[04/16 21:19:18] Re-training INFO: iter: 211750/300480  CE: 1.9943  
[04/16 21:20:24] Re-training INFO: iter: 211875/300480  CE: 2.0762  
[04/16 21:21:28] Re-training INFO: iter: 212000/300480  CE: 1.8718  
[04/16 21:22:33] Re-training INFO: iter: 212125/300480  CE: 2.1159  
[04/16 21:23:37] Re-training INFO: iter: 212250/300480  CE: 2.2405  
[04/16 21:24:41] Re-training INFO: iter: 212375/300480  CE: 2.0383  
[04/16 21:25:45] Re-training INFO: iter: 212500/300480  CE: 2.0568  
[04/16 21:26:50] Re-training INFO: iter: 212625/300480  CE: 1.9566  
[04/16 21:27:54] Re-training INFO: iter: 212750/300480  CE: 2.0960  
[04/16 21:28:38] Re-training INFO: --> epoch: 170/240  avg CE: 2.0331  lr: 0.06469304390193917  
[04/16 21:29:15] Re-training INFO: iter: 212875/300480  CE: 1.9220  
[04/16 21:30:19] Re-training INFO: iter: 213000/300480  CE: 1.8341  
[04/16 21:31:24] Re-training INFO: iter: 213125/300480  CE: 1.9108  
[04/16 21:32:29] Re-training INFO: iter: 213250/300480  CE: 2.0093  
[04/16 21:33:33] Re-training INFO: iter: 213375/300480  CE: 2.0516  
[04/16 21:34:38] Re-training INFO: iter: 213500/300480  CE: 2.1527  
[04/16 21:35:44] Re-training INFO: iter: 213625/300480  CE: 1.9942  
[04/16 21:36:48] Re-training INFO: iter: 213750/300480  CE: 2.1852  
[04/16 21:37:52] Re-training INFO: iter: 213875/300480  CE: 1.9634  
[04/16 21:38:57] Re-training INFO: iter: 214000/300480  CE: 2.1290  
[04/16 21:39:42] Re-training INFO: --> epoch: 171/240  avg CE: 2.0318  lr: 0.06275225258488099  
[04/16 21:40:17] Re-training INFO: iter: 214125/300480  CE: 1.9864  
[04/16 21:41:22] Re-training INFO: iter: 214250/300480  CE: 1.9281  
[04/16 21:42:26] Re-training INFO: iter: 214375/300480  CE: 1.9645  
[04/16 21:43:29] Re-training INFO: iter: 214500/300480  CE: 2.0036  
[04/16 21:44:33] Re-training INFO: iter: 214625/300480  CE: 1.9375  
[04/16 21:45:38] Re-training INFO: iter: 214750/300480  CE: 2.2116  
[04/16 21:46:42] Re-training INFO: iter: 214875/300480  CE: 1.9397  
[04/16 21:47:47] Re-training INFO: iter: 215000/300480  CE: 1.8758  
[04/16 21:48:50] Re-training INFO: iter: 215125/300480  CE: 2.1225  
[04/16 21:49:55] Re-training INFO: iter: 215250/300480  CE: 1.8814  
[04/16 21:50:41] Re-training INFO: --> epoch: 172/240  avg CE: 2.0296  lr: 0.06275225258488099  
[04/16 21:51:15] Re-training INFO: iter: 215375/300480  CE: 1.9316  
[04/16 21:52:19] Re-training INFO: iter: 215500/300480  CE: 2.1827  
[04/16 21:53:24] Re-training INFO: iter: 215625/300480  CE: 2.0741  
[04/16 21:54:30] Re-training INFO: iter: 215750/300480  CE: 1.9049  
[04/16 21:55:33] Re-training INFO: iter: 215875/300480  CE: 1.9654  
[04/16 21:56:37] Re-training INFO: iter: 216000/300480  CE: 2.0141  
[04/16 21:57:42] Re-training INFO: iter: 216125/300480  CE: 2.1140  
[04/16 21:58:45] Re-training INFO: iter: 216250/300480  CE: 2.0337  
[04/16 21:59:50] Re-training INFO: iter: 216375/300480  CE: 2.0381  
[04/16 22:00:54] Re-training INFO: iter: 216500/300480  CE: 2.1912  
[04/16 22:01:41] Re-training INFO: --> epoch: 173/240  avg CE: 2.0260  lr: 0.06086968500733456  
[04/16 22:02:13] Re-training INFO: iter: 216625/300480  CE: 2.0352  
[04/16 22:03:17] Re-training INFO: iter: 216750/300480  CE: 2.0784  
[04/16 22:04:23] Re-training INFO: iter: 216875/300480  CE: 2.0812  
[04/16 22:05:28] Re-training INFO: iter: 217000/300480  CE: 1.9318  
[04/16 22:06:33] Re-training INFO: iter: 217125/300480  CE: 1.9524  
[04/16 22:07:38] Re-training INFO: iter: 217250/300480  CE: 2.1895  
[04/16 22:08:43] Re-training INFO: iter: 217375/300480  CE: 2.3302  
[04/16 22:09:48] Re-training INFO: iter: 217500/300480  CE: 2.1184  
[04/16 22:10:53] Re-training INFO: iter: 217625/300480  CE: 1.9895  
[04/16 22:11:58] Re-training INFO: iter: 217750/300480  CE: 1.9630  
[04/16 22:12:47] Re-training INFO: --> epoch: 174/240  avg CE: 2.0229  lr: 0.06086968500733456  
[04/16 22:13:19] Re-training INFO: iter: 217875/300480  CE: 1.7592  
[04/16 22:14:25] Re-training INFO: iter: 218000/300480  CE: 1.9228  
[04/16 22:15:29] Re-training INFO: iter: 218125/300480  CE: 1.8352  
[04/16 22:16:33] Re-training INFO: iter: 218250/300480  CE: 2.0258  
[04/16 22:17:39] Re-training INFO: iter: 218375/300480  CE: 1.8112  
[04/16 22:18:43] Re-training INFO: iter: 218500/300480  CE: 2.0947  
[04/16 22:19:48] Re-training INFO: iter: 218625/300480  CE: 1.8848  
[04/16 22:20:52] Re-training INFO: iter: 218750/300480  CE: 2.0227  
[04/16 22:21:58] Re-training INFO: iter: 218875/300480  CE: 2.0657  
[04/16 22:23:03] Re-training INFO: iter: 219000/300480  CE: 1.8269  
[04/16 22:23:54] Re-training INFO: --> epoch: 175/240  avg CE: 2.0295  lr: 0.06086968500733456  
[04/16 22:24:25] Re-training INFO: iter: 219125/300480  CE: 1.8748  
[04/16 22:25:31] Re-training INFO: iter: 219250/300480  CE: 2.1153  
[04/16 22:26:36] Re-training INFO: iter: 219375/300480  CE: 2.0694  
[04/16 22:27:42] Re-training INFO: iter: 219500/300480  CE: 2.0882  
[04/16 22:28:46] Re-training INFO: iter: 219625/300480  CE: 1.8985  
[04/16 22:29:52] Re-training INFO: iter: 219750/300480  CE: 2.1601  
[04/16 22:30:56] Re-training INFO: iter: 219875/300480  CE: 1.9312  
[04/16 22:32:02] Re-training INFO: iter: 220000/300480  CE: 2.0350  
[04/16 22:33:07] Re-training INFO: iter: 220125/300480  CE: 1.9603  
[04/16 22:34:12] Re-training INFO: iter: 220250/300480  CE: 1.8983  
[04/16 22:35:04] Re-training INFO: --> epoch: 176/240  avg CE: 2.0204  lr: 0.05904359445711452  
[04/16 22:35:34] Re-training INFO: iter: 220375/300480  CE: 2.0552  
[04/16 22:36:39] Re-training INFO: iter: 220500/300480  CE: 1.9592  
[04/16 22:37:45] Re-training INFO: iter: 220625/300480  CE: 1.8212  
[04/16 22:38:51] Re-training INFO: iter: 220750/300480  CE: 1.9677  
[04/16 22:39:57] Re-training INFO: iter: 220875/300480  CE: 2.1541  
[04/16 22:41:02] Re-training INFO: iter: 221000/300480  CE: 1.8939  
[04/16 22:42:07] Re-training INFO: iter: 221125/300480  CE: 2.2563  
[04/16 22:43:13] Re-training INFO: iter: 221250/300480  CE: 1.8229  
[04/16 22:44:18] Re-training INFO: iter: 221375/300480  CE: 2.0195  
[04/16 22:45:23] Re-training INFO: iter: 221500/300480  CE: 2.0336  
[04/16 22:46:14] Re-training INFO: --> epoch: 177/240  avg CE: 2.0155  lr: 0.05904359445711452  
[04/16 22:46:43] Re-training INFO: iter: 221625/300480  CE: 2.0344  
[04/16 22:47:47] Re-training INFO: iter: 221750/300480  CE: 2.1257  
[04/16 22:48:53] Re-training INFO: iter: 221875/300480  CE: 1.9796  
[04/16 22:49:58] Re-training INFO: iter: 222000/300480  CE: 1.7995  
[04/16 22:51:05] Re-training INFO: iter: 222125/300480  CE: 1.9262  
[04/16 22:52:10] Re-training INFO: iter: 222250/300480  CE: 2.0838  
[04/16 22:53:14] Re-training INFO: iter: 222375/300480  CE: 1.9226  
[04/16 22:54:19] Re-training INFO: iter: 222500/300480  CE: 2.1518  
[04/16 22:55:25] Re-training INFO: iter: 222625/300480  CE: 1.9223  
[04/16 22:56:30] Re-training INFO: iter: 222750/300480  CE: 2.0599  
[04/16 22:57:24] Re-training INFO: --> epoch: 178/240  avg CE: 2.0155  lr: 0.05727228662340108  
[04/16 22:57:53] Re-training INFO: iter: 222875/300480  CE: 1.9899  
[04/16 22:58:58] Re-training INFO: iter: 223000/300480  CE: 1.9580  
[04/16 23:00:04] Re-training INFO: iter: 223125/300480  CE: 1.7754  
[04/16 23:01:10] Re-training INFO: iter: 223250/300480  CE: 2.1300  
[04/16 23:02:15] Re-training INFO: iter: 223375/300480  CE: 1.7554  
[04/16 23:03:20] Re-training INFO: iter: 223500/300480  CE: 1.9299  
[04/16 23:04:25] Re-training INFO: iter: 223625/300480  CE: 1.8160  
[04/16 23:05:29] Re-training INFO: iter: 223750/300480  CE: 1.9035  
[04/16 23:06:34] Re-training INFO: iter: 223875/300480  CE: 1.7763  
[04/16 23:07:39] Re-training INFO: iter: 224000/300480  CE: 2.1004  
[04/16 23:08:33] Re-training INFO: --> epoch: 179/240  avg CE: 2.0122  lr: 0.05727228662340108  
[04/16 23:08:59] Re-training INFO: iter: 224125/300480  CE: 1.8450  
[04/16 23:10:03] Re-training INFO: iter: 224250/300480  CE: 2.0106  
[04/16 23:11:10] Re-training INFO: iter: 224375/300480  CE: 2.0315  
[04/16 23:12:16] Re-training INFO: iter: 224500/300480  CE: 2.2537  
[04/16 23:13:20] Re-training INFO: iter: 224625/300480  CE: 1.9993  
[04/16 23:14:24] Re-training INFO: iter: 224750/300480  CE: 1.9560  
[04/16 23:15:30] Re-training INFO: iter: 224875/300480  CE: 2.0434  
[04/16 23:16:36] Re-training INFO: iter: 225000/300480  CE: 1.9070  
[04/16 23:17:42] Re-training INFO: iter: 225125/300480  CE: 2.1131  
[04/16 23:18:47] Re-training INFO: iter: 225250/300480  CE: 1.9692  
[04/16 23:19:41] Re-training INFO: --> epoch: 180/240  avg CE: 2.0164  lr: 0.055554118024699055  
[04/16 23:20:16] Re-training INFO: # of Test Samples: 50000.0
[04/16 23:20:16] Re-training INFO: Top-1/-5 acc: 72.05 / 90.58
[04/16 23:20:16] Re-training INFO: Top-1/-5 acc: 27.95 /  9.42
[04/16 23:20:16] Re-training INFO: 

[04/16 23:20:41] Re-training INFO: iter: 225375/300480  CE: 2.0912  
[04/16 23:21:46] Re-training INFO: iter: 225500/300480  CE: 1.9937  
[04/16 23:22:50] Re-training INFO: iter: 225625/300480  CE: 1.8329  
[04/16 23:23:54] Re-training INFO: iter: 225750/300480  CE: 2.0681  
[04/16 23:24:59] Re-training INFO: iter: 225875/300480  CE: 2.0836  
[04/16 23:26:05] Re-training INFO: iter: 226000/300480  CE: 2.1840  
[04/16 23:27:09] Re-training INFO: iter: 226125/300480  CE: 1.9886  
[04/16 23:28:14] Re-training INFO: iter: 226250/300480  CE: 1.9604  
[04/16 23:29:19] Re-training INFO: iter: 226375/300480  CE: 2.2031  
[04/16 23:30:24] Re-training INFO: iter: 226500/300480  CE: 2.0885  
[04/16 23:31:21] Re-training INFO: --> epoch: 181/240  avg CE: 2.0120  lr: 0.055554118024699055  
[04/16 23:31:47] Re-training INFO: iter: 226625/300480  CE: 2.0130  
[04/16 23:32:53] Re-training INFO: iter: 226750/300480  CE: 2.0786  
[04/16 23:33:58] Re-training INFO: iter: 226875/300480  CE: 2.1287  
[04/16 23:35:03] Re-training INFO: iter: 227000/300480  CE: 1.9617  
[04/16 23:36:08] Re-training INFO: iter: 227125/300480  CE: 2.0017  
[04/16 23:37:12] Re-training INFO: iter: 227250/300480  CE: 2.0915  
[04/16 23:38:17] Re-training INFO: iter: 227375/300480  CE: 2.1895  
[04/16 23:39:22] Re-training INFO: iter: 227500/300480  CE: 1.9917  
[04/16 23:40:26] Re-training INFO: iter: 227625/300480  CE: 2.0240  
[04/16 23:41:32] Re-training INFO: iter: 227750/300480  CE: 2.0551  
[04/16 23:42:30] Re-training INFO: --> epoch: 182/240  avg CE: 2.0157  lr: 0.055554118024699055  
[04/16 23:42:53] Re-training INFO: iter: 227875/300480  CE: 2.0718  
[04/16 23:43:57] Re-training INFO: iter: 228000/300480  CE: 1.9577  
[04/16 23:45:02] Re-training INFO: iter: 228125/300480  CE: 2.1196  
[04/16 23:46:07] Re-training INFO: iter: 228250/300480  CE: 1.9644  
[04/16 23:47:12] Re-training INFO: iter: 228375/300480  CE: 2.1170  
[04/16 23:48:16] Re-training INFO: iter: 228500/300480  CE: 1.8589  
[04/16 23:49:19] Re-training INFO: iter: 228625/300480  CE: 1.9417  
[04/16 23:50:24] Re-training INFO: iter: 228750/300480  CE: 1.8599  
[04/16 23:51:29] Re-training INFO: iter: 228875/300480  CE: 2.0093  
[04/16 23:52:32] Re-training INFO: iter: 229000/300480  CE: 2.0642  
[04/16 23:53:31] Re-training INFO: --> epoch: 183/240  avg CE: 2.0058  lr: 0.053887494483958076  
[04/16 23:53:53] Re-training INFO: iter: 229125/300480  CE: 1.9925  
[04/16 23:54:59] Re-training INFO: iter: 229250/300480  CE: 2.1121  
[04/16 23:56:03] Re-training INFO: iter: 229375/300480  CE: 2.0791  
[04/16 23:57:09] Re-training INFO: iter: 229500/300480  CE: 2.1624  
[04/16 23:58:14] Re-training INFO: iter: 229625/300480  CE: 1.9837  
[04/16 23:59:20] Re-training INFO: iter: 229750/300480  CE: 1.9853  
[04/17 00:00:26] Re-training INFO: iter: 229875/300480  CE: 2.0627  
[04/17 00:01:32] Re-training INFO: iter: 230000/300480  CE: 2.0066  
[04/17 00:02:36] Re-training INFO: iter: 230125/300480  CE: 1.8797  
[04/17 00:03:42] Re-training INFO: iter: 230250/300480  CE: 2.0935  
[04/17 00:04:40] Re-training INFO: --> epoch: 184/240  avg CE: 2.0051  lr: 0.053887494483958076  
[04/17 00:05:02] Re-training INFO: iter: 230375/300480  CE: 1.9878  
[04/17 00:06:06] Re-training INFO: iter: 230500/300480  CE: 2.1221  
[04/17 00:07:12] Re-training INFO: iter: 230625/300480  CE: 1.9439  
[04/17 00:08:16] Re-training INFO: iter: 230750/300480  CE: 1.9412  
[04/17 00:09:21] Re-training INFO: iter: 230875/300480  CE: 2.0953  
[04/17 00:10:27] Re-training INFO: iter: 231000/300480  CE: 2.0851  
[04/17 00:11:32] Re-training INFO: iter: 231125/300480  CE: 1.9751  
[04/17 00:12:38] Re-training INFO: iter: 231250/300480  CE: 2.0045  
[04/17 00:13:43] Re-training INFO: iter: 231375/300480  CE: 2.2667  
[04/17 00:14:48] Re-training INFO: iter: 231500/300480  CE: 1.9045  
[04/17 00:15:50] Re-training INFO: --> epoch: 185/240  avg CE: 2.0050  lr: 0.05227086964943933  
[04/17 00:16:10] Re-training INFO: iter: 231625/300480  CE: 1.9577  
[04/17 00:17:16] Re-training INFO: iter: 231750/300480  CE: 1.8369  
[04/17 00:18:21] Re-training INFO: iter: 231875/300480  CE: 1.9638  
[04/17 00:19:27] Re-training INFO: iter: 232000/300480  CE: 1.9670  
[04/17 00:20:33] Re-training INFO: iter: 232125/300480  CE: 1.8901  
[04/17 00:21:39] Re-training INFO: iter: 232250/300480  CE: 1.8248  
[04/17 00:22:45] Re-training INFO: iter: 232375/300480  CE: 1.8859  
[04/17 00:23:50] Re-training INFO: iter: 232500/300480  CE: 1.9115  
[04/17 00:24:55] Re-training INFO: iter: 232625/300480  CE: 2.2081  
[04/17 00:26:01] Re-training INFO: iter: 232750/300480  CE: 2.0882  
[04/17 00:27:03] Re-training INFO: --> epoch: 186/240  avg CE: 1.9973  lr: 0.05227086964943933  
[04/17 00:27:22] Re-training INFO: iter: 232875/300480  CE: 1.8012  
[04/17 00:28:29] Re-training INFO: iter: 233000/300480  CE: 1.8573  
[04/17 00:29:34] Re-training INFO: iter: 233125/300480  CE: 2.0299  
[04/17 00:30:40] Re-training INFO: iter: 233250/300480  CE: 2.1033  
[04/17 00:31:46] Re-training INFO: iter: 233375/300480  CE: 2.1215  
[04/17 00:32:50] Re-training INFO: iter: 233500/300480  CE: 2.0221  
[04/17 00:33:56] Re-training INFO: iter: 233625/300480  CE: 1.9266  
[04/17 00:35:01] Re-training INFO: iter: 233750/300480  CE: 2.1375  
[04/17 00:36:07] Re-training INFO: iter: 233875/300480  CE: 1.9249  
[04/17 00:37:13] Re-training INFO: iter: 234000/300480  CE: 1.9878  
[04/17 00:38:16] Re-training INFO: --> epoch: 187/240  avg CE: 2.0041  lr: 0.05227086964943933  
[04/17 00:38:35] Re-training INFO: iter: 234125/300480  CE: 1.9645  
[04/17 00:39:40] Re-training INFO: iter: 234250/300480  CE: 1.9497  
[04/17 00:40:46] Re-training INFO: iter: 234375/300480  CE: 1.9479  
[04/17 00:41:52] Re-training INFO: iter: 234500/300480  CE: 1.9834  
[04/17 00:42:58] Re-training INFO: iter: 234625/300480  CE: 1.9195  
[04/17 00:44:03] Re-training INFO: iter: 234750/300480  CE: 1.9243  
[04/17 00:45:09] Re-training INFO: iter: 234875/300480  CE: 2.1353  
[04/17 00:46:15] Re-training INFO: iter: 235000/300480  CE: 2.1066  
[04/17 00:47:22] Re-training INFO: iter: 235125/300480  CE: 1.9431  
[04/17 00:48:27] Re-training INFO: iter: 235250/300480  CE: 1.8311  
[04/17 00:49:31] Re-training INFO: iter: 235375/300480  CE: 1.9003  
[04/17 00:49:31] Re-training INFO: --> epoch: 188/240  avg CE: 1.9969  lr: 0.050702743559956145  
[04/17 00:50:54] Re-training INFO: iter: 235500/300480  CE: 1.9195  
[04/17 00:51:59] Re-training INFO: iter: 235625/300480  CE: 1.8247  
[04/17 00:53:04] Re-training INFO: iter: 235750/300480  CE: 2.2199  
[04/17 00:54:08] Re-training INFO: iter: 235875/300480  CE: 1.9508  
[04/17 00:55:13] Re-training INFO: iter: 236000/300480  CE: 1.9129  
[04/17 00:56:19] Re-training INFO: iter: 236125/300480  CE: 2.0120  
[04/17 00:57:25] Re-training INFO: iter: 236250/300480  CE: 1.9852  
[04/17 00:58:31] Re-training INFO: iter: 236375/300480  CE: 2.0267  
[04/17 00:59:37] Re-training INFO: iter: 236500/300480  CE: 2.0842  
[04/17 01:00:41] Re-training INFO: iter: 236625/300480  CE: 2.1025  
[04/17 01:00:42] Re-training INFO: --> epoch: 189/240  avg CE: 1.9980  lr: 0.050702743559956145  
[04/17 01:02:05] Re-training INFO: iter: 236750/300480  CE: 1.9479  
[04/17 01:03:10] Re-training INFO: iter: 236875/300480  CE: 1.8731  
[04/17 01:04:16] Re-training INFO: iter: 237000/300480  CE: 2.0161  
[04/17 01:05:23] Re-training INFO: iter: 237125/300480  CE: 1.9742  
[04/17 01:06:28] Re-training INFO: iter: 237250/300480  CE: 2.0375  
[04/17 01:07:34] Re-training INFO: iter: 237375/300480  CE: 1.9988  
[04/17 01:08:39] Re-training INFO: iter: 237500/300480  CE: 2.0952  
[04/17 01:09:44] Re-training INFO: iter: 237625/300480  CE: 1.8901  
[04/17 01:10:50] Re-training INFO: iter: 237750/300480  CE: 2.2774  
[04/17 01:11:54] Re-training INFO: iter: 237875/300480  CE: 1.8368  
[04/17 01:11:56] Re-training INFO: --> epoch: 190/240  avg CE: 1.9922  lr: 0.04918166125315747  
[04/17 01:13:18] Re-training INFO: iter: 238000/300480  CE: 1.9505  
[04/17 01:14:23] Re-training INFO: iter: 238125/300480  CE: 1.8474  
[04/17 01:15:29] Re-training INFO: iter: 238250/300480  CE: 1.6325  
[04/17 01:16:35] Re-training INFO: iter: 238375/300480  CE: 2.0097  
[04/17 01:17:41] Re-training INFO: iter: 238500/300480  CE: 1.9065  
[04/17 01:18:47] Re-training INFO: iter: 238625/300480  CE: 2.0524  
[04/17 01:19:53] Re-training INFO: iter: 238750/300480  CE: 2.0959  
[04/17 01:20:59] Re-training INFO: iter: 238875/300480  CE: 1.9918  
[04/17 01:22:05] Re-training INFO: iter: 239000/300480  CE: 2.0458  
[04/17 01:23:10] Re-training INFO: iter: 239125/300480  CE: 2.2250  
[04/17 01:23:12] Re-training INFO: --> epoch: 191/240  avg CE: 1.9940  lr: 0.04918166125315747  
[04/17 01:24:33] Re-training INFO: iter: 239250/300480  CE: 2.0190  
[04/17 01:25:38] Re-training INFO: iter: 239375/300480  CE: 1.9098  
[04/17 01:26:44] Re-training INFO: iter: 239500/300480  CE: 2.2988  
[04/17 01:27:49] Re-training INFO: iter: 239625/300480  CE: 1.8426  
[04/17 01:28:55] Re-training INFO: iter: 239750/300480  CE: 1.9276  
[04/17 01:30:01] Re-training INFO: iter: 239875/300480  CE: 1.8613  
[04/17 01:31:07] Re-training INFO: iter: 240000/300480  CE: 2.0211  
[04/17 01:32:13] Re-training INFO: iter: 240125/300480  CE: 1.9566  
[04/17 01:33:17] Re-training INFO: iter: 240250/300480  CE: 2.0119  
[04/17 01:34:23] Re-training INFO: iter: 240375/300480  CE: 2.0236  
[04/17 01:34:26] Re-training INFO: --> epoch: 192/240  avg CE: 1.9957  lr: 0.04770621141556274  
[04/17 01:35:44] Re-training INFO: iter: 240500/300480  CE: 1.9027  
[04/17 01:36:50] Re-training INFO: iter: 240625/300480  CE: 2.1027  
[04/17 01:37:56] Re-training INFO: iter: 240750/300480  CE: 1.8688  
[04/17 01:39:02] Re-training INFO: iter: 240875/300480  CE: 2.1413  
[04/17 01:40:08] Re-training INFO: iter: 241000/300480  CE: 1.7672  
[04/17 01:41:13] Re-training INFO: iter: 241125/300480  CE: 2.2947  
[04/17 01:42:20] Re-training INFO: iter: 241250/300480  CE: 1.9743  
[04/17 01:43:26] Re-training INFO: iter: 241375/300480  CE: 2.2030  
[04/17 01:44:31] Re-training INFO: iter: 241500/300480  CE: 1.9850  
[04/17 01:45:37] Re-training INFO: iter: 241625/300480  CE: 1.9184  
[04/17 01:45:40] Re-training INFO: --> epoch: 193/240  avg CE: 1.9883  lr: 0.04770621141556274  
[04/17 01:46:58] Re-training INFO: iter: 241750/300480  CE: 1.9543  
[04/17 01:48:04] Re-training INFO: iter: 241875/300480  CE: 1.7585  
[04/17 01:49:10] Re-training INFO: iter: 242000/300480  CE: 1.9980  
[04/17 01:50:15] Re-training INFO: iter: 242125/300480  CE: 2.1345  
[04/17 01:51:20] Re-training INFO: iter: 242250/300480  CE: 1.9113  
[04/17 01:52:25] Re-training INFO: iter: 242375/300480  CE: 2.0480  
[04/17 01:53:29] Re-training INFO: iter: 242500/300480  CE: 2.0146  
[04/17 01:54:35] Re-training INFO: iter: 242625/300480  CE: 2.1411  
[04/17 01:55:40] Re-training INFO: iter: 242750/300480  CE: 2.1420  
[04/17 01:56:46] Re-training INFO: iter: 242875/300480  CE: 2.0737  
[04/17 01:56:50] Re-training INFO: --> epoch: 194/240  avg CE: 1.9926  lr: 0.04770621141556274  
[04/17 01:58:08] Re-training INFO: iter: 243000/300480  CE: 2.0979  
[04/17 01:59:14] Re-training INFO: iter: 243125/300480  CE: 1.9392  
[04/17 02:00:20] Re-training INFO: iter: 243250/300480  CE: 2.0144  
[04/17 02:01:26] Re-training INFO: iter: 243375/300480  CE: 1.9757  
[04/17 02:02:32] Re-training INFO: iter: 243500/300480  CE: 2.0003  
[04/17 02:03:39] Re-training INFO: iter: 243625/300480  CE: 2.0260  
[04/17 02:04:45] Re-training INFO: iter: 243750/300480  CE: 2.1938  
[04/17 02:05:50] Re-training INFO: iter: 243875/300480  CE: 1.9590  
[04/17 02:06:55] Re-training INFO: iter: 244000/300480  CE: 2.1643  
[04/17 02:08:01] Re-training INFO: iter: 244125/300480  CE: 1.9079  
[04/17 02:08:07] Re-training INFO: --> epoch: 195/240  avg CE: 1.9866  lr: 0.04627502507309585  
[04/17 02:09:22] Re-training INFO: iter: 244250/300480  CE: 1.8384  
[04/17 02:10:29] Re-training INFO: iter: 244375/300480  CE: 2.2145  
[04/17 02:11:35] Re-training INFO: iter: 244500/300480  CE: 2.0567  
[04/17 02:12:40] Re-training INFO: iter: 244625/300480  CE: 1.9587  
[04/17 02:13:47] Re-training INFO: iter: 244750/300480  CE: 2.0246  
[04/17 02:14:53] Re-training INFO: iter: 244875/300480  CE: 2.0285  
[04/17 02:15:59] Re-training INFO: iter: 245000/300480  CE: 1.9641  
[04/17 02:17:04] Re-training INFO: iter: 245125/300480  CE: 1.8518  
[04/17 02:18:09] Re-training INFO: iter: 245250/300480  CE: 2.0264  
[04/17 02:19:15] Re-training INFO: iter: 245375/300480  CE: 2.1123  
[04/17 02:19:22] Re-training INFO: --> epoch: 196/240  avg CE: 1.9782  lr: 0.04627502507309585  
[04/17 02:20:36] Re-training INFO: iter: 245500/300480  CE: 2.2207  
[04/17 02:21:42] Re-training INFO: iter: 245625/300480  CE: 2.0861  
[04/17 02:22:47] Re-training INFO: iter: 245750/300480  CE: 2.1321  
[04/17 02:23:53] Re-training INFO: iter: 245875/300480  CE: 2.1019  
[04/17 02:24:58] Re-training INFO: iter: 246000/300480  CE: 2.0653  
[04/17 02:26:05] Re-training INFO: iter: 246125/300480  CE: 1.9788  
[04/17 02:27:10] Re-training INFO: iter: 246250/300480  CE: 1.9239  
[04/17 02:28:16] Re-training INFO: iter: 246375/300480  CE: 2.1779  
[04/17 02:29:22] Re-training INFO: iter: 246500/300480  CE: 1.8478  
[04/17 02:30:28] Re-training INFO: iter: 246625/300480  CE: 1.9545  
[04/17 02:30:36] Re-training INFO: --> epoch: 197/240  avg CE: 1.9847  lr: 0.04488677432090298  
[04/17 02:31:48] Re-training INFO: iter: 246750/300480  CE: 1.7777  
[04/17 02:32:54] Re-training INFO: iter: 246875/300480  CE: 2.2247  
[04/17 02:34:00] Re-training INFO: iter: 247000/300480  CE: 1.9733  
[04/17 02:35:06] Re-training INFO: iter: 247125/300480  CE: 2.0258  
[04/17 02:36:11] Re-training INFO: iter: 247250/300480  CE: 1.8803  
[04/17 02:37:18] Re-training INFO: iter: 247375/300480  CE: 2.1004  
[04/17 02:38:25] Re-training INFO: iter: 247500/300480  CE: 1.9155  
[04/17 02:39:31] Re-training INFO: iter: 247625/300480  CE: 1.9798  
[04/17 02:40:37] Re-training INFO: iter: 247750/300480  CE: 2.2679  
[04/17 02:41:44] Re-training INFO: iter: 247875/300480  CE: 2.1489  
[04/17 02:41:53] Re-training INFO: --> epoch: 198/240  avg CE: 1.9851  lr: 0.04488677432090298  
[04/17 02:43:04] Re-training INFO: iter: 248000/300480  CE: 1.9557  
[04/17 02:44:10] Re-training INFO: iter: 248125/300480  CE: 1.8937  
[04/17 02:45:16] Re-training INFO: iter: 248250/300480  CE: 2.0776  
[04/17 02:46:24] Re-training INFO: iter: 248375/300480  CE: 1.8157  
[04/17 02:47:29] Re-training INFO: iter: 248500/300480  CE: 1.8702  
[04/17 02:48:35] Re-training INFO: iter: 248625/300480  CE: 2.0124  
[04/17 02:49:40] Re-training INFO: iter: 248750/300480  CE: 2.1566  
[04/17 02:50:46] Re-training INFO: iter: 248875/300480  CE: 1.7651  
[04/17 02:51:52] Re-training INFO: iter: 249000/300480  CE: 1.8626  
[04/17 02:52:57] Re-training INFO: iter: 249125/300480  CE: 2.0927  
[04/17 02:53:06] Re-training INFO: --> epoch: 199/240  avg CE: 1.9852  lr: 0.04488677432090298  
[04/17 02:54:18] Re-training INFO: iter: 249250/300480  CE: 1.9329  
[04/17 02:55:25] Re-training INFO: iter: 249375/300480  CE: 2.0234  
[04/17 02:56:30] Re-training INFO: iter: 249500/300480  CE: 2.1180  
[04/17 02:57:37] Re-training INFO: iter: 249625/300480  CE: 2.1827  
[04/17 02:58:45] Re-training INFO: iter: 249750/300480  CE: 1.9070  
[04/17 02:59:52] Re-training INFO: iter: 249875/300480  CE: 2.0528  
[04/17 03:01:00] Re-training INFO: iter: 250000/300480  CE: 1.8771  
[04/17 03:02:06] Re-training INFO: iter: 250125/300480  CE: 2.0085  
[04/17 03:03:13] Re-training INFO: iter: 250250/300480  CE: 2.0886  
[04/17 03:04:19] Re-training INFO: iter: 250375/300480  CE: 1.9498  
[04/17 03:04:30] Re-training INFO: --> epoch: 200/240  avg CE: 1.9771  lr: 0.043540171091275885  
[04/17 03:05:04] Re-training INFO: # of Test Samples: 50000.0
[04/17 03:05:04] Re-training INFO: Top-1/-5 acc: 72.24 / 90.65
[04/17 03:05:04] Re-training INFO: Top-1/-5 acc: 27.76 /  9.35
[04/17 03:05:04] Re-training INFO: 

[04/17 03:06:15] Re-training INFO: iter: 250500/300480  CE: 1.8925  
[04/17 03:07:22] Re-training INFO: iter: 250625/300480  CE: 1.8133  
[04/17 03:08:29] Re-training INFO: iter: 250750/300480  CE: 1.9310  
[04/17 03:09:36] Re-training INFO: iter: 250875/300480  CE: 2.2720  
[04/17 03:10:42] Re-training INFO: iter: 251000/300480  CE: 1.8091  
[04/17 03:11:47] Re-training INFO: iter: 251125/300480  CE: 1.8362  
[04/17 03:12:53] Re-training INFO: iter: 251250/300480  CE: 1.9559  
[04/17 03:13:59] Re-training INFO: iter: 251375/300480  CE: 2.0973  
[04/17 03:15:05] Re-training INFO: iter: 251500/300480  CE: 2.1090  
[04/17 03:16:12] Re-training INFO: iter: 251625/300480  CE: 2.2443  
[04/17 03:16:24] Re-training INFO: --> epoch: 201/240  avg CE: 1.9776  lr: 0.043540171091275885  
[04/17 03:17:34] Re-training INFO: iter: 251750/300480  CE: 1.9611  
[04/17 03:18:40] Re-training INFO: iter: 251875/300480  CE: 2.1407  
[04/17 03:19:46] Re-training INFO: iter: 252000/300480  CE: 1.9460  
[04/17 03:20:53] Re-training INFO: iter: 252125/300480  CE: 2.1037  
[04/17 03:21:59] Re-training INFO: iter: 252250/300480  CE: 2.0574  
[04/17 03:23:05] Re-training INFO: iter: 252375/300480  CE: 1.8737  
[04/17 03:24:10] Re-training INFO: iter: 252500/300480  CE: 2.0655  
[04/17 03:25:18] Re-training INFO: iter: 252625/300480  CE: 1.9322  
[04/17 03:26:24] Re-training INFO: iter: 252750/300480  CE: 2.0101  
[04/17 03:27:31] Re-training INFO: iter: 252875/300480  CE: 1.8883  
[04/17 03:27:44] Re-training INFO: --> epoch: 202/240  avg CE: 1.9733  lr: 0.04223396595853761  
[04/17 03:28:53] Re-training INFO: iter: 253000/300480  CE: 2.0066  
[04/17 03:30:01] Re-training INFO: iter: 253125/300480  CE: 1.8676  
[04/17 03:31:08] Re-training INFO: iter: 253250/300480  CE: 2.1249  
[04/17 03:32:16] Re-training INFO: iter: 253375/300480  CE: 1.9376  
[04/17 03:33:22] Re-training INFO: iter: 253500/300480  CE: 1.9778  
[04/17 03:34:29] Re-training INFO: iter: 253625/300480  CE: 1.9994  
[04/17 03:35:36] Re-training INFO: iter: 253750/300480  CE: 1.9240  
[04/17 03:36:41] Re-training INFO: iter: 253875/300480  CE: 1.9901  
[04/17 03:37:48] Re-training INFO: iter: 254000/300480  CE: 1.8413  
[04/17 03:38:56] Re-training INFO: iter: 254125/300480  CE: 1.9816  
[04/17 03:39:11] Re-training INFO: --> epoch: 203/240  avg CE: 1.9728  lr: 0.04223396595853761  
[04/17 03:40:18] Re-training INFO: iter: 254250/300480  CE: 2.0223  
[04/17 03:41:24] Re-training INFO: iter: 254375/300480  CE: 1.7251  
[04/17 03:42:31] Re-training INFO: iter: 254500/300480  CE: 2.0120  
[04/17 03:43:38] Re-training INFO: iter: 254625/300480  CE: 2.0066  
[04/17 03:44:44] Re-training INFO: iter: 254750/300480  CE: 1.8679  
[04/17 03:45:51] Re-training INFO: iter: 254875/300480  CE: 2.2505  
[04/17 03:46:58] Re-training INFO: iter: 255000/300480  CE: 1.9077  
[04/17 03:48:06] Re-training INFO: iter: 255125/300480  CE: 1.8869  
[04/17 03:49:12] Re-training INFO: iter: 255250/300480  CE: 1.9663  
[04/17 03:50:18] Re-training INFO: iter: 255375/300480  CE: 1.9891  
[04/17 03:50:33] Re-training INFO: --> epoch: 204/240  avg CE: 1.9776  lr: 0.04096694697978148  
[04/17 03:51:39] Re-training INFO: iter: 255500/300480  CE: 1.7220  
[04/17 03:52:46] Re-training INFO: iter: 255625/300480  CE: 2.1525  
[04/17 03:53:54] Re-training INFO: iter: 255750/300480  CE: 1.9097  
[04/17 03:55:01] Re-training INFO: iter: 255875/300480  CE: 1.8861  
[04/17 03:56:08] Re-training INFO: iter: 256000/300480  CE: 2.0128  
[04/17 03:57:15] Re-training INFO: iter: 256125/300480  CE: 1.9901  
[04/17 03:58:22] Re-training INFO: iter: 256250/300480  CE: 2.0610  
[04/17 03:59:28] Re-training INFO: iter: 256375/300480  CE: 2.1527  
[04/17 04:00:34] Re-training INFO: iter: 256500/300480  CE: 1.8564  
[04/17 04:01:40] Re-training INFO: iter: 256625/300480  CE: 2.0775  
[04/17 04:01:56] Re-training INFO: --> epoch: 205/240  avg CE: 1.9650  lr: 0.04096694697978148  
[04/17 04:03:02] Re-training INFO: iter: 256750/300480  CE: 1.6822  
[04/17 04:04:09] Re-training INFO: iter: 256875/300480  CE: 2.0726  
[04/17 04:05:15] Re-training INFO: iter: 257000/300480  CE: 1.9787  
[04/17 04:06:23] Re-training INFO: iter: 257125/300480  CE: 1.9515  
[04/17 04:07:30] Re-training INFO: iter: 257250/300480  CE: 2.0694  
[04/17 04:08:37] Re-training INFO: iter: 257375/300480  CE: 2.0114  
[04/17 04:09:44] Re-training INFO: iter: 257500/300480  CE: 2.0578  
[04/17 04:10:50] Re-training INFO: iter: 257625/300480  CE: 1.8386  
[04/17 04:11:57] Re-training INFO: iter: 257750/300480  CE: 1.9337  
[04/17 04:13:04] Re-training INFO: iter: 257875/300480  CE: 2.0606  
[04/17 04:13:21] Re-training INFO: --> epoch: 206/240  avg CE: 1.9679  lr: 0.04096694697978148  
[04/17 04:14:25] Re-training INFO: iter: 258000/300480  CE: 1.8585  
[04/17 04:15:32] Re-training INFO: iter: 258125/300480  CE: 1.9383  
[04/17 04:16:37] Re-training INFO: iter: 258250/300480  CE: 1.8959  
[04/17 04:17:43] Re-training INFO: iter: 258375/300480  CE: 2.0090  
[04/17 04:18:48] Re-training INFO: iter: 258500/300480  CE: 2.0560  
[04/17 04:19:55] Re-training INFO: iter: 258625/300480  CE: 2.1637  
[04/17 04:21:01] Re-training INFO: iter: 258750/300480  CE: 1.9111  
[04/17 04:22:05] Re-training INFO: iter: 258875/300480  CE: 1.9751  
[04/17 04:23:11] Re-training INFO: iter: 259000/300480  CE: 1.9873  
[04/17 04:24:17] Re-training INFO: iter: 259125/300480  CE: 2.1064  
[04/17 04:24:35] Re-training INFO: --> epoch: 207/240  avg CE: 1.9658  lr: 0.039737938570388036  
[04/17 04:25:38] Re-training INFO: iter: 259250/300480  CE: 1.8768  
[04/17 04:26:44] Re-training INFO: iter: 259375/300480  CE: 1.9006  
[04/17 04:27:50] Re-training INFO: iter: 259500/300480  CE: 1.9340  
[04/17 04:28:57] Re-training INFO: iter: 259625/300480  CE: 2.0583  
[04/17 04:30:04] Re-training INFO: iter: 259750/300480  CE: 1.8988  
[04/17 04:31:12] Re-training INFO: iter: 259875/300480  CE: 1.7601  
[04/17 04:32:19] Re-training INFO: iter: 260000/300480  CE: 1.9863  
[04/17 04:33:27] Re-training INFO: iter: 260125/300480  CE: 2.3432  
[04/17 04:34:32] Re-training INFO: iter: 260250/300480  CE: 1.9963  
[04/17 04:35:39] Re-training INFO: iter: 260375/300480  CE: 1.9234  
[04/17 04:35:59] Re-training INFO: --> epoch: 208/240  avg CE: 1.9702  lr: 0.039737938570388036  
[04/17 04:37:02] Re-training INFO: iter: 260500/300480  CE: 2.0411  
[04/17 04:38:08] Re-training INFO: iter: 260625/300480  CE: 2.1900  
[04/17 04:39:13] Re-training INFO: iter: 260750/300480  CE: 1.7870  
[04/17 04:40:19] Re-training INFO: iter: 260875/300480  CE: 1.8950  
[04/17 04:41:25] Re-training INFO: iter: 261000/300480  CE: 2.1524  
[04/17 04:42:30] Re-training INFO: iter: 261125/300480  CE: 2.1065  
[04/17 04:43:36] Re-training INFO: iter: 261250/300480  CE: 1.9179  
[04/17 04:44:41] Re-training INFO: iter: 261375/300480  CE: 1.7978  
[04/17 04:45:46] Re-training INFO: iter: 261500/300480  CE: 2.1096  
[04/17 04:46:51] Re-training INFO: iter: 261625/300480  CE: 1.9280  
[04/17 04:47:11] Re-training INFO: --> epoch: 209/240  avg CE: 1.9658  lr: 0.0385458004132764  
[04/17 04:48:12] Re-training INFO: iter: 261750/300480  CE: 1.9465  
[04/17 04:49:17] Re-training INFO: iter: 261875/300480  CE: 2.1815  
[04/17 04:50:23] Re-training INFO: iter: 262000/300480  CE: 1.9868  
[04/17 04:51:28] Re-training INFO: iter: 262125/300480  CE: 1.8053  
[04/17 04:52:34] Re-training INFO: iter: 262250/300480  CE: 2.0230  
[04/17 04:53:40] Re-training INFO: iter: 262375/300480  CE: 2.0213  
[04/17 04:54:45] Re-training INFO: iter: 262500/300480  CE: 2.0706  
[04/17 04:55:49] Re-training INFO: iter: 262625/300480  CE: 1.8407  
[04/17 04:56:56] Re-training INFO: iter: 262750/300480  CE: 2.1851  
[04/17 04:58:02] Re-training INFO: iter: 262875/300480  CE: 1.9051  
[04/17 04:58:25] Re-training INFO: --> epoch: 210/240  avg CE: 1.9585  lr: 0.0385458004132764  
[04/17 04:59:24] Re-training INFO: iter: 263000/300480  CE: 1.9359  
[04/17 05:00:29] Re-training INFO: iter: 263125/300480  CE: 2.0410  
[04/17 05:01:34] Re-training INFO: iter: 263250/300480  CE: 2.0345  
[04/17 05:02:40] Re-training INFO: iter: 263375/300480  CE: 1.9654  
[04/17 05:03:45] Re-training INFO: iter: 263500/300480  CE: 2.0774  
[04/17 05:04:51] Re-training INFO: iter: 263625/300480  CE: 1.9095  
[04/17 05:05:57] Re-training INFO: iter: 263750/300480  CE: 1.9961  
[04/17 05:07:02] Re-training INFO: iter: 263875/300480  CE: 2.0705  
[04/17 05:08:10] Re-training INFO: iter: 264000/300480  CE: 2.0490  
[04/17 05:09:15] Re-training INFO: iter: 264125/300480  CE: 2.1533  
[04/17 05:09:38] Re-training INFO: --> epoch: 211/240  avg CE: 1.9632  lr: 0.0385458004132764  
[04/17 05:10:38] Re-training INFO: iter: 264250/300480  CE: 2.0854  
[04/17 05:11:45] Re-training INFO: iter: 264375/300480  CE: 1.9338  
[04/17 05:12:52] Re-training INFO: iter: 264500/300480  CE: 1.9044  
[04/17 05:13:58] Re-training INFO: iter: 264625/300480  CE: 1.9702  
[04/17 05:15:04] Re-training INFO: iter: 264750/300480  CE: 1.9188  
[04/17 05:16:10] Re-training INFO: iter: 264875/300480  CE: 1.9667  
[04/17 05:17:17] Re-training INFO: iter: 265000/300480  CE: 2.1916  
[04/17 05:18:25] Re-training INFO: iter: 265125/300480  CE: 1.9427  
[04/17 05:19:33] Re-training INFO: iter: 265250/300480  CE: 1.9087  
[04/17 05:20:38] Re-training INFO: iter: 265375/300480  CE: 1.6088  
[04/17 05:21:03] Re-training INFO: --> epoch: 212/240  avg CE: 1.9581  lr: 0.037389426400878105  
[04/17 05:22:01] Re-training INFO: iter: 265500/300480  CE: 1.9584  
[04/17 05:23:08] Re-training INFO: iter: 265625/300480  CE: 1.9388  
[04/17 05:24:14] Re-training INFO: iter: 265750/300480  CE: 2.0573  
[04/17 05:25:20] Re-training INFO: iter: 265875/300480  CE: 1.8140  
[04/17 05:26:26] Re-training INFO: iter: 266000/300480  CE: 2.0834  
[04/17 05:27:33] Re-training INFO: iter: 266125/300480  CE: 1.9901  
[04/17 05:28:40] Re-training INFO: iter: 266250/300480  CE: 1.9469  
[04/17 05:29:46] Re-training INFO: iter: 266375/300480  CE: 1.8905  
[04/17 05:30:52] Re-training INFO: iter: 266500/300480  CE: 1.9772  
[04/17 05:31:58] Re-training INFO: iter: 266625/300480  CE: 1.9977  
[04/17 05:32:23] Re-training INFO: --> epoch: 213/240  avg CE: 1.9607  lr: 0.037389426400878105  
[04/17 05:33:21] Re-training INFO: iter: 266750/300480  CE: 2.0426  
[04/17 05:34:26] Re-training INFO: iter: 266875/300480  CE: 1.9720  
[04/17 05:35:32] Re-training INFO: iter: 267000/300480  CE: 1.8225  
[04/17 05:36:38] Re-training INFO: iter: 267125/300480  CE: 1.8347  
[04/17 05:37:44] Re-training INFO: iter: 267250/300480  CE: 1.9149  
[04/17 05:38:50] Re-training INFO: iter: 267375/300480  CE: 2.0754  
[04/17 05:39:56] Re-training INFO: iter: 267500/300480  CE: 1.9246  
[04/17 05:41:02] Re-training INFO: iter: 267625/300480  CE: 1.8757  
[04/17 05:42:09] Re-training INFO: iter: 267750/300480  CE: 2.2373  
[04/17 05:43:15] Re-training INFO: iter: 267875/300480  CE: 2.1439  
[04/17 05:43:41] Re-training INFO: --> epoch: 214/240  avg CE: 1.9542  lr: 0.03626774360885175  
[04/17 05:44:37] Re-training INFO: iter: 268000/300480  CE: 1.9550  
[04/17 05:45:44] Re-training INFO: iter: 268125/300480  CE: 2.0179  
[04/17 05:46:51] Re-training INFO: iter: 268250/300480  CE: 1.9451  
[04/17 05:47:57] Re-training INFO: iter: 268375/300480  CE: 2.0383  
[04/17 05:49:04] Re-training INFO: iter: 268500/300480  CE: 1.9471  
[04/17 05:50:10] Re-training INFO: iter: 268625/300480  CE: 1.8385  
[04/17 05:51:17] Re-training INFO: iter: 268750/300480  CE: 1.9733  
[04/17 05:52:23] Re-training INFO: iter: 268875/300480  CE: 2.0775  
[04/17 05:53:29] Re-training INFO: iter: 269000/300480  CE: 1.9436  
[04/17 05:54:35] Re-training INFO: iter: 269125/300480  CE: 2.0137  
[04/17 05:55:03] Re-training INFO: --> epoch: 215/240  avg CE: 1.9560  lr: 0.03626774360885175  
[04/17 05:55:57] Re-training INFO: iter: 269250/300480  CE: 2.0437  
[04/17 05:57:04] Re-training INFO: iter: 269375/300480  CE: 1.9971  
[04/17 05:58:10] Re-training INFO: iter: 269500/300480  CE: 1.9746  
[04/17 05:59:15] Re-training INFO: iter: 269625/300480  CE: 1.9951  
[04/17 06:00:22] Re-training INFO: iter: 269750/300480  CE: 2.1516  
[04/17 06:01:27] Re-training INFO: iter: 269875/300480  CE: 1.8472  
[04/17 06:02:32] Re-training INFO: iter: 270000/300480  CE: 2.0967  
[04/17 06:03:38] Re-training INFO: iter: 270125/300480  CE: 1.9459  
[04/17 06:04:43] Re-training INFO: iter: 270250/300480  CE: 1.8106  
[04/17 06:05:49] Re-training INFO: iter: 270375/300480  CE: 2.2108  
[04/17 06:06:17] Re-training INFO: --> epoch: 216/240  avg CE: 1.9542  lr: 0.0351797113005862  
[04/17 06:07:11] Re-training INFO: iter: 270500/300480  CE: 2.2275  
[04/17 06:08:18] Re-training INFO: iter: 270625/300480  CE: 1.9336  
[04/17 06:09:24] Re-training INFO: iter: 270750/300480  CE: 2.0435  
[04/17 06:10:31] Re-training INFO: iter: 270875/300480  CE: 2.0658  
[04/17 06:11:36] Re-training INFO: iter: 271000/300480  CE: 1.8086  
[04/17 06:12:43] Re-training INFO: iter: 271125/300480  CE: 1.8404  
[04/17 06:13:49] Re-training INFO: iter: 271250/300480  CE: 1.9476  
[04/17 06:14:55] Re-training INFO: iter: 271375/300480  CE: 1.7313  
[04/17 06:16:01] Re-training INFO: iter: 271500/300480  CE: 1.8646  
[04/17 06:17:09] Re-training INFO: iter: 271625/300480  CE: 2.0230  
[04/17 06:17:39] Re-training INFO: --> epoch: 217/240  avg CE: 1.9535  lr: 0.0351797113005862  
[04/17 06:18:33] Re-training INFO: iter: 271750/300480  CE: 1.7242  
[04/17 06:19:39] Re-training INFO: iter: 271875/300480  CE: 1.7317  
[04/17 06:20:46] Re-training INFO: iter: 272000/300480  CE: 1.8311  
[04/17 06:21:51] Re-training INFO: iter: 272125/300480  CE: 1.8741  
[04/17 06:22:59] Re-training INFO: iter: 272250/300480  CE: 2.1826  
[04/17 06:24:05] Re-training INFO: iter: 272375/300480  CE: 1.7658  
[04/17 06:25:11] Re-training INFO: iter: 272500/300480  CE: 1.9177  
[04/17 06:26:16] Re-training INFO: iter: 272625/300480  CE: 2.0309  
[04/17 06:27:22] Re-training INFO: iter: 272750/300480  CE: 2.1004  
[04/17 06:28:28] Re-training INFO: iter: 272875/300480  CE: 1.7705  
[04/17 06:28:58] Re-training INFO: --> epoch: 218/240  avg CE: 1.9467  lr: 0.0351797113005862  
[04/17 06:29:50] Re-training INFO: iter: 273000/300480  CE: 1.9309  
[04/17 06:30:55] Re-training INFO: iter: 273125/300480  CE: 2.0357  
[04/17 06:32:01] Re-training INFO: iter: 273250/300480  CE: 1.9624  
[04/17 06:33:07] Re-training INFO: iter: 273375/300480  CE: 1.8773  
[04/17 06:34:12] Re-training INFO: iter: 273500/300480  CE: 2.1793  
[04/17 06:35:19] Re-training INFO: iter: 273625/300480  CE: 1.9499  
[04/17 06:36:25] Re-training INFO: iter: 273750/300480  CE: 1.9967  
[04/17 06:37:31] Re-training INFO: iter: 273875/300480  CE: 1.8164  
[04/17 06:38:36] Re-training INFO: iter: 274000/300480  CE: 2.0129  
[04/17 06:39:42] Re-training INFO: iter: 274125/300480  CE: 2.0965  
[04/17 06:40:14] Re-training INFO: --> epoch: 219/240  avg CE: 1.9446  lr: 0.03412431996156862  
[04/17 06:41:04] Re-training INFO: iter: 274250/300480  CE: 1.6789  
[04/17 06:42:11] Re-training INFO: iter: 274375/300480  CE: 1.9393  
[04/17 06:43:18] Re-training INFO: iter: 274500/300480  CE: 2.0382  
[04/17 06:44:24] Re-training INFO: iter: 274625/300480  CE: 1.9735  
[04/17 06:45:29] Re-training INFO: iter: 274750/300480  CE: 2.0900  
[04/17 06:46:37] Re-training INFO: iter: 274875/300480  CE: 2.0503  
[04/17 06:47:42] Re-training INFO: iter: 275000/300480  CE: 2.0011  
[04/17 06:48:49] Re-training INFO: iter: 275125/300480  CE: 1.9252  
[04/17 06:49:56] Re-training INFO: iter: 275250/300480  CE: 2.1025  
[04/17 06:51:02] Re-training INFO: iter: 275375/300480  CE: 1.8511  
[04/17 06:51:34] Re-training INFO: --> epoch: 220/240  avg CE: 1.9466  lr: 0.03412431996156862  
[04/17 06:52:09] Re-training INFO: # of Test Samples: 50000.0
[04/17 06:52:09] Re-training INFO: Top-1/-5 acc: 72.65 / 90.79
[04/17 06:52:09] Re-training INFO: Top-1/-5 acc: 27.35 /  9.21
[04/17 06:52:09] Re-training INFO: 

[04/17 06:53:00] Re-training INFO: iter: 275500/300480  CE: 2.0097  
[04/17 06:54:06] Re-training INFO: iter: 275625/300480  CE: 1.8595  
[04/17 06:55:11] Re-training INFO: iter: 275750/300480  CE: 1.9621  
[04/17 06:56:18] Re-training INFO: iter: 275875/300480  CE: 1.8578  
[04/17 06:57:23] Re-training INFO: iter: 276000/300480  CE: 1.9404  
[04/17 06:58:30] Re-training INFO: iter: 276125/300480  CE: 1.9155  
[04/17 06:59:36] Re-training INFO: iter: 276250/300480  CE: 1.9903  
[04/17 07:00:42] Re-training INFO: iter: 276375/300480  CE: 2.1442  
[04/17 07:01:47] Re-training INFO: iter: 276500/300480  CE: 1.7904  
[04/17 07:02:53] Re-training INFO: iter: 276625/300480  CE: 1.9658  
[04/17 07:03:27] Re-training INFO: --> epoch: 221/240  avg CE: 1.9466  lr: 0.03310059036272155  
[04/17 07:04:16] Re-training INFO: iter: 276750/300480  CE: 2.0081  
[04/17 07:05:23] Re-training INFO: iter: 276875/300480  CE: 1.9803  
[04/17 07:06:29] Re-training INFO: iter: 277000/300480  CE: 1.8959  
[04/17 07:07:35] Re-training INFO: iter: 277125/300480  CE: 2.0199  
[04/17 07:08:42] Re-training INFO: iter: 277250/300480  CE: 1.8260  
[04/17 07:09:48] Re-training INFO: iter: 277375/300480  CE: 1.7444  
[04/17 07:10:53] Re-training INFO: iter: 277500/300480  CE: 2.0431  
[04/17 07:11:59] Re-training INFO: iter: 277625/300480  CE: 2.2405  
[04/17 07:13:05] Re-training INFO: iter: 277750/300480  CE: 1.9657  
[04/17 07:14:11] Re-training INFO: iter: 277875/300480  CE: 1.8220  
[04/17 07:14:45] Re-training INFO: --> epoch: 222/240  avg CE: 1.9463  lr: 0.03310059036272155  
[04/17 07:15:33] Re-training INFO: iter: 278000/300480  CE: 1.9854  
[04/17 07:16:39] Re-training INFO: iter: 278125/300480  CE: 1.7860  
[04/17 07:17:46] Re-training INFO: iter: 278250/300480  CE: 1.9587  
[04/17 07:18:54] Re-training INFO: iter: 278375/300480  CE: 1.8843  
[04/17 07:19:59] Re-training INFO: iter: 278500/300480  CE: 2.0390  
[04/17 07:21:04] Re-training INFO: iter: 278625/300480  CE: 1.6876  
[04/17 07:22:09] Re-training INFO: iter: 278750/300480  CE: 1.9093  
[04/17 07:23:15] Re-training INFO: iter: 278875/300480  CE: 1.7280  
[04/17 07:24:21] Re-training INFO: iter: 279000/300480  CE: 1.9760  
[04/17 07:25:27] Re-training INFO: iter: 279125/300480  CE: 2.0149  
[04/17 07:26:02] Re-training INFO: --> epoch: 223/240  avg CE: 1.9386  lr: 0.03310059036272155  
[04/17 07:26:49] Re-training INFO: iter: 279250/300480  CE: 1.8857  
[04/17 07:27:55] Re-training INFO: iter: 279375/300480  CE: 1.9902  
[04/17 07:29:01] Re-training INFO: iter: 279500/300480  CE: 1.9313  
[04/17 07:30:07] Re-training INFO: iter: 279625/300480  CE: 2.1637  
[04/17 07:31:13] Re-training INFO: iter: 279750/300480  CE: 1.9159  
[04/17 07:32:19] Re-training INFO: iter: 279875/300480  CE: 1.8158  
[04/17 07:33:26] Re-training INFO: iter: 280000/300480  CE: 1.9265  
[04/17 07:34:32] Re-training INFO: iter: 280125/300480  CE: 1.9508  
[04/17 07:35:38] Re-training INFO: iter: 280250/300480  CE: 2.0726  
[04/17 07:36:46] Re-training INFO: iter: 280375/300480  CE: 1.7378  
[04/17 07:37:23] Re-training INFO: --> epoch: 224/240  avg CE: 1.9387  lr: 0.03210757265183991  
[04/17 07:38:08] Re-training INFO: iter: 280500/300480  CE: 1.8438  
[04/17 07:39:14] Re-training INFO: iter: 280625/300480  CE: 2.0161  
[04/17 07:40:21] Re-training INFO: iter: 280750/300480  CE: 1.9903  
[04/17 07:41:27] Re-training INFO: iter: 280875/300480  CE: 1.8140  
[04/17 07:42:34] Re-training INFO: iter: 281000/300480  CE: 1.9643  
[04/17 07:43:40] Re-training INFO: iter: 281125/300480  CE: 1.8187  
[04/17 07:44:47] Re-training INFO: iter: 281250/300480  CE: 1.9324  
[04/17 07:45:53] Re-training INFO: iter: 281375/300480  CE: 1.9389  
[04/17 07:46:59] Re-training INFO: iter: 281500/300480  CE: 1.7564  
[04/17 07:48:05] Re-training INFO: iter: 281625/300480  CE: 1.9282  
[04/17 07:48:43] Re-training INFO: --> epoch: 225/240  avg CE: 1.9432  lr: 0.03210757265183991  
[04/17 07:49:27] Re-training INFO: iter: 281750/300480  CE: 1.8472  
[04/17 07:50:33] Re-training INFO: iter: 281875/300480  CE: 1.9312  
[04/17 07:51:38] Re-training INFO: iter: 282000/300480  CE: 1.9654  
[04/17 07:52:45] Re-training INFO: iter: 282125/300480  CE: 2.0047  
[04/17 07:53:52] Re-training INFO: iter: 282250/300480  CE: 1.8935  
[04/17 07:54:58] Re-training INFO: iter: 282375/300480  CE: 2.0057  
[04/17 07:56:04] Re-training INFO: iter: 282500/300480  CE: 1.7123  
[04/17 07:57:09] Re-training INFO: iter: 282625/300480  CE: 1.9597  
[04/17 07:58:15] Re-training INFO: iter: 282750/300480  CE: 1.9594  
[04/17 07:59:21] Re-training INFO: iter: 282875/300480  CE: 1.9350  
[04/17 08:00:00] Re-training INFO: --> epoch: 226/240  avg CE: 1.9370  lr: 0.031144345472284708  
[04/17 08:00:42] Re-training INFO: iter: 283000/300480  CE: 1.9552  
[04/17 08:01:49] Re-training INFO: iter: 283125/300480  CE: 1.9039  
[04/17 08:02:56] Re-training INFO: iter: 283250/300480  CE: 1.9228  
[04/17 08:04:03] Re-training INFO: iter: 283375/300480  CE: 1.8554  
[04/17 08:05:10] Re-training INFO: iter: 283500/300480  CE: 1.9509  
[04/17 08:06:15] Re-training INFO: iter: 283625/300480  CE: 1.8367  
[04/17 08:07:21] Re-training INFO: iter: 283750/300480  CE: 1.9156  
[04/17 08:08:27] Re-training INFO: iter: 283875/300480  CE: 1.9944  
[04/17 08:09:33] Re-training INFO: iter: 284000/300480  CE: 2.0290  
[04/17 08:10:39] Re-training INFO: iter: 284125/300480  CE: 1.9092  
[04/17 08:11:18] Re-training INFO: --> epoch: 227/240  avg CE: 1.9412  lr: 0.031144345472284708  
[04/17 08:12:00] Re-training INFO: iter: 284250/300480  CE: 1.9085  
[04/17 08:13:06] Re-training INFO: iter: 284375/300480  CE: 2.1774  
[04/17 08:14:12] Re-training INFO: iter: 284500/300480  CE: 2.0738  
[04/17 08:15:18] Re-training INFO: iter: 284625/300480  CE: 1.9413  
[04/17 08:16:24] Re-training INFO: iter: 284750/300480  CE: 1.8195  
[04/17 08:17:30] Re-training INFO: iter: 284875/300480  CE: 1.9779  
[04/17 08:18:36] Re-training INFO: iter: 285000/300480  CE: 1.8919  
[04/17 08:19:42] Re-training INFO: iter: 285125/300480  CE: 1.8361  
[04/17 08:20:47] Re-training INFO: iter: 285250/300480  CE: 1.9295  
[04/17 08:21:53] Re-training INFO: iter: 285375/300480  CE: 1.9061  
[04/17 08:22:32] Re-training INFO: --> epoch: 228/240  avg CE: 1.9352  lr: 0.03021001510811617  
[04/17 08:23:13] Re-training INFO: iter: 285500/300480  CE: 1.9553  
[04/17 08:24:20] Re-training INFO: iter: 285625/300480  CE: 1.8560  
[04/17 08:25:26] Re-training INFO: iter: 285750/300480  CE: 1.9304  
[04/17 08:26:32] Re-training INFO: iter: 285875/300480  CE: 1.9571  
[04/17 08:27:39] Re-training INFO: iter: 286000/300480  CE: 2.0505  
[04/17 08:28:44] Re-training INFO: iter: 286125/300480  CE: 2.1547  
[04/17 08:29:51] Re-training INFO: iter: 286250/300480  CE: 1.9155  
[04/17 08:30:58] Re-training INFO: iter: 286375/300480  CE: 1.9287  
[04/17 08:32:04] Re-training INFO: iter: 286500/300480  CE: 1.7857  
[04/17 08:33:10] Re-training INFO: iter: 286625/300480  CE: 1.8231  
[04/17 08:33:52] Re-training INFO: --> epoch: 229/240  avg CE: 1.9341  lr: 0.03021001510811617  
[04/17 08:34:32] Re-training INFO: iter: 286750/300480  CE: 1.8763  
[04/17 08:35:38] Re-training INFO: iter: 286875/300480  CE: 2.2191  
[04/17 08:36:44] Re-training INFO: iter: 287000/300480  CE: 2.0688  
[04/17 08:37:51] Re-training INFO: iter: 287125/300480  CE: 1.9485  
[04/17 08:38:59] Re-training INFO: iter: 287250/300480  CE: 2.0541  
[04/17 08:40:04] Re-training INFO: iter: 287375/300480  CE: 2.0322  
[04/17 08:41:10] Re-training INFO: iter: 287500/300480  CE: 2.0831  
[04/17 08:42:17] Re-training INFO: iter: 287625/300480  CE: 2.0524  
[04/17 08:43:23] Re-training INFO: iter: 287750/300480  CE: 2.0149  
[04/17 08:44:28] Re-training INFO: iter: 287875/300480  CE: 1.8718  
[04/17 08:45:11] Re-training INFO: --> epoch: 230/240  avg CE: 1.9318  lr: 0.03021001510811617  
[04/17 08:45:50] Re-training INFO: iter: 288000/300480  CE: 1.9574  
[04/17 08:46:56] Re-training INFO: iter: 288125/300480  CE: 1.9604  
[04/17 08:48:03] Re-training INFO: iter: 288250/300480  CE: 2.0431  
[04/17 08:49:11] Re-training INFO: iter: 288375/300480  CE: 1.6522  
[04/17 08:50:17] Re-training INFO: iter: 288500/300480  CE: 1.7593  
[04/17 08:51:25] Re-training INFO: iter: 288625/300480  CE: 1.9876  
[04/17 08:52:32] Re-training INFO: iter: 288750/300480  CE: 1.8635  
[04/17 08:53:37] Re-training INFO: iter: 288875/300480  CE: 1.9640  
[04/17 08:54:43] Re-training INFO: iter: 289000/300480  CE: 1.8906  
[04/17 08:55:49] Re-training INFO: iter: 289125/300480  CE: 1.9991  
[04/17 08:56:35] Re-training INFO: --> epoch: 231/240  avg CE: 1.9307  lr: 0.029303714654872682  
[04/17 08:57:13] Re-training INFO: iter: 289250/300480  CE: 1.9769  
[04/17 08:58:19] Re-training INFO: iter: 289375/300480  CE: 1.8856  
[04/17 08:59:25] Re-training INFO: iter: 289500/300480  CE: 1.8776  
[04/17 09:00:32] Re-training INFO: iter: 289625/300480  CE: 2.1649  
[04/17 09:01:38] Re-training INFO: iter: 289750/300480  CE: 1.8519  
[04/17 09:02:44] Re-training INFO: iter: 289875/300480  CE: 1.8628  
[04/17 09:03:53] Re-training INFO: iter: 290000/300480  CE: 2.0483  
[04/17 09:05:00] Re-training INFO: iter: 290125/300480  CE: 1.8157  
[04/17 09:06:07] Re-training INFO: iter: 290250/300480  CE: 1.9638  
[04/17 09:07:14] Re-training INFO: iter: 290375/300480  CE: 1.9766  
[04/17 09:07:58] Re-training INFO: --> epoch: 232/240  avg CE: 1.9351  lr: 0.029303714654872682  
[04/17 09:08:35] Re-training INFO: iter: 290500/300480  CE: 1.9973  
[04/17 09:09:42] Re-training INFO: iter: 290625/300480  CE: 1.9355  
[04/17 09:10:49] Re-training INFO: iter: 290750/300480  CE: 1.9027  
[04/17 09:11:55] Re-training INFO: iter: 290875/300480  CE: 1.8950  
[04/17 09:13:01] Re-training INFO: iter: 291000/300480  CE: 1.8073  
[04/17 09:14:06] Re-training INFO: iter: 291125/300480  CE: 1.9505  
[04/17 09:15:12] Re-training INFO: iter: 291250/300480  CE: 1.9835  
[04/17 09:16:19] Re-training INFO: iter: 291375/300480  CE: 1.9115  
[04/17 09:17:24] Re-training INFO: iter: 291500/300480  CE: 1.9347  
[04/17 09:18:31] Re-training INFO: iter: 291625/300480  CE: 1.8033  
[04/17 09:19:17] Re-training INFO: --> epoch: 233/240  avg CE: 1.9293  lr: 0.028424603215226503  
[04/17 09:19:53] Re-training INFO: iter: 291750/300480  CE: 1.8738  
[04/17 09:20:59] Re-training INFO: iter: 291875/300480  CE: 1.8283  
[04/17 09:22:06] Re-training INFO: iter: 292000/300480  CE: 1.9821  
[04/17 09:23:13] Re-training INFO: iter: 292125/300480  CE: 1.9491  
[04/17 09:24:20] Re-training INFO: iter: 292250/300480  CE: 2.1102  
[04/17 09:25:27] Re-training INFO: iter: 292375/300480  CE: 1.9784  
[04/17 09:26:34] Re-training INFO: iter: 292500/300480  CE: 2.1155  
[04/17 09:27:39] Re-training INFO: iter: 292625/300480  CE: 2.0425  
[04/17 09:28:46] Re-training INFO: iter: 292750/300480  CE: 2.1216  
[04/17 09:29:52] Re-training INFO: iter: 292875/300480  CE: 1.8378  
[04/17 09:30:39] Re-training INFO: --> epoch: 234/240  avg CE: 1.9297  lr: 0.028424603215226503  
[04/17 09:31:15] Re-training INFO: iter: 293000/300480  CE: 2.0734  
[04/17 09:32:20] Re-training INFO: iter: 293125/300480  CE: 1.9700  
[04/17 09:33:27] Re-training INFO: iter: 293250/300480  CE: 1.9239  
[04/17 09:34:32] Re-training INFO: iter: 293375/300480  CE: 2.1110  
[04/17 09:35:39] Re-training INFO: iter: 293500/300480  CE: 1.9020  
[04/17 09:36:45] Re-training INFO: iter: 293625/300480  CE: 2.0438  
[04/17 09:37:51] Re-training INFO: iter: 293750/300480  CE: 1.8769  
[04/17 09:38:57] Re-training INFO: iter: 293875/300480  CE: 1.9400  
[04/17 09:40:01] Re-training INFO: iter: 294000/300480  CE: 2.0605  
[04/17 09:41:09] Re-training INFO: iter: 294125/300480  CE: 2.0789  
[04/17 09:41:58] Re-training INFO: --> epoch: 235/240  avg CE: 1.9319  lr: 0.028424603215226503  
[04/17 09:42:32] Re-training INFO: iter: 294250/300480  CE: 1.9133  
[04/17 09:43:37] Re-training INFO: iter: 294375/300480  CE: 1.8991  
[04/17 09:44:45] Re-training INFO: iter: 294500/300480  CE: 2.0144  
[04/17 09:45:51] Re-training INFO: iter: 294625/300480  CE: 1.9545  
[04/17 09:46:59] Re-training INFO: iter: 294750/300480  CE: 1.7359  
[04/17 09:48:05] Re-training INFO: iter: 294875/300480  CE: 1.8267  
[04/17 09:49:12] Re-training INFO: iter: 295000/300480  CE: 2.0835  
[04/17 09:50:20] Re-training INFO: iter: 295125/300480  CE: 1.8208  
[04/17 09:51:27] Re-training INFO: iter: 295250/300480  CE: 1.9321  
[04/17 09:52:35] Re-training INFO: iter: 295375/300480  CE: 1.8316  
[04/17 09:53:25] Re-training INFO: --> epoch: 236/240  avg CE: 1.9307  lr: 0.027571865118769703  
[04/17 09:53:58] Re-training INFO: iter: 295500/300480  CE: 1.7807  
[04/17 09:55:04] Re-training INFO: iter: 295625/300480  CE: 1.9551  
[04/17 09:56:10] Re-training INFO: iter: 295750/300480  CE: 2.0300  
[04/17 09:57:16] Re-training INFO: iter: 295875/300480  CE: 1.9389  
[04/17 09:58:22] Re-training INFO: iter: 296000/300480  CE: 2.1314  
[04/17 09:59:29] Re-training INFO: iter: 296125/300480  CE: 1.7937  
[04/17 10:00:35] Re-training INFO: iter: 296250/300480  CE: 2.0473  
[04/17 10:01:42] Re-training INFO: iter: 296375/300480  CE: 1.9242  
[04/17 10:02:47] Re-training INFO: iter: 296500/300480  CE: 1.8989  
[04/17 10:03:53] Re-training INFO: iter: 296625/300480  CE: 1.7833  
[04/17 10:04:43] Re-training INFO: --> epoch: 237/240  avg CE: 1.9258  lr: 0.027571865118769703  
[04/17 10:05:15] Re-training INFO: iter: 296750/300480  CE: 1.8262  
[04/17 10:06:21] Re-training INFO: iter: 296875/300480  CE: 1.8369  
[04/17 10:07:28] Re-training INFO: iter: 297000/300480  CE: 1.6513  
[04/17 10:08:35] Re-training INFO: iter: 297125/300480  CE: 1.8354  
[04/17 10:09:41] Re-training INFO: iter: 297250/300480  CE: 1.9649  
[04/17 10:10:49] Re-training INFO: iter: 297375/300480  CE: 1.9148  
[04/17 10:11:55] Re-training INFO: iter: 297500/300480  CE: 1.9544  
[04/17 10:13:02] Re-training INFO: iter: 297625/300480  CE: 2.0045  
[04/17 10:14:08] Re-training INFO: iter: 297750/300480  CE: 1.6936  
[04/17 10:15:14] Re-training INFO: iter: 297875/300480  CE: 1.8533  
[04/17 10:16:06] Re-training INFO: --> epoch: 238/240  avg CE: 1.9247  lr: 0.026744709165206614  
[04/17 10:16:37] Re-training INFO: iter: 298000/300480  CE: 2.0907  
[04/17 10:17:44] Re-training INFO: iter: 298125/300480  CE: 2.1498  
[04/17 10:18:51] Re-training INFO: iter: 298250/300480  CE: 1.9474  
[04/17 10:19:57] Re-training INFO: iter: 298375/300480  CE: 2.1571  
[04/17 10:21:03] Re-training INFO: iter: 298500/300480  CE: 1.9856  
[04/17 10:22:09] Re-training INFO: iter: 298625/300480  CE: 1.6659  
[04/17 10:23:16] Re-training INFO: iter: 298750/300480  CE: 2.0174  
[04/17 10:24:23] Re-training INFO: iter: 298875/300480  CE: 1.9234  
[04/17 10:25:30] Re-training INFO: iter: 299000/300480  CE: 1.8357  
[04/17 10:26:37] Re-training INFO: iter: 299125/300480  CE: 2.2244  
[04/17 10:27:29] Re-training INFO: --> epoch: 239/240  avg CE: 1.9234  lr: 0.026744709165206614  
[04/17 10:27:58] Re-training INFO: iter: 299250/300480  CE: 1.9018  
[04/17 10:29:05] Re-training INFO: iter: 299375/300480  CE: 1.8477  
[04/17 10:30:12] Re-training INFO: iter: 299500/300480  CE: 1.9483  
[04/17 10:31:17] Re-training INFO: iter: 299625/300480  CE: 1.8684  
[04/17 10:32:23] Re-training INFO: iter: 299750/300480  CE: 1.8624  
[04/17 10:33:30] Re-training INFO: iter: 299875/300480  CE: 1.8026  
[04/17 10:34:36] Re-training INFO: iter: 300000/300480  CE: 1.9264  
[04/17 10:35:42] Re-training INFO: iter: 300125/300480  CE: 1.8445  
[04/17 10:36:48] Re-training INFO: iter: 300250/300480  CE: 1.7505  
[04/17 10:37:54] Re-training INFO: iter: 300375/300480  CE: 1.9971  
[04/17 10:38:47] Re-training INFO: --> epoch: 240/240  avg CE: 1.9182  lr: 0.025942367890250412  
[04/17 10:39:22] Re-training INFO: # of Test Samples: 50000.0
[04/17 10:39:22] Re-training INFO: Top-1/-5 acc: 72.56 / 90.84
[04/17 10:39:22] Re-training INFO: Top-1/-5 acc: 27.44 /  9.16
[04/17 10:39:22] Re-training INFO: 

[04/17 10:39:57] Re-training INFO: # of Test Samples: 50000.0
[04/17 10:39:57] Re-training INFO: Top-1/-5 acc: 72.56 / 90.84
[04/17 10:39:57] Re-training INFO: Top-1/-5 acc: 27.44 /  9.16
[04/17 10:39:57] Re-training INFO: --> END Retrain-gss-mobile0-test-seed-0
[04/17 10:39:59] Re-training INFO: ELAPSED TIME: 158430.4(s) = 44(h) 00(m)
